{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1ec5c25",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": [],
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Aluminum-issue-check\" data-toc-modified-id=\"Aluminum-issue-check-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Aluminum issue check</a></span></li><li><span><a href=\"#Steel-issue-check\" data-toc-modified-id=\"Steel-issue-check-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Steel issue check</a></span></li><li><span><a href=\"#Initial-integration-run\" data-toc-modified-id=\"Initial-integration-run-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Initial integration run</a></span></li><li><span><a href=\"#Tuning-individual-materials,-historical\" data-toc-modified-id=\"Tuning-individual-materials,-historical-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Tuning individual materials, historical</a></span><ul class=\"toc-item\"><li><span><a href=\"#Demand\" data-toc-modified-id=\"Demand-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Demand</a></span></li><li><span><a href=\"#Mining\" data-toc-modified-id=\"Mining-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Mining</a></span></li><li><span><a href=\"#All\" data-toc-modified-id=\"All-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>All</a></span></li><li><span><a href=\"#For-debugging\" data-toc-modified-id=\"For-debugging-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>For debugging</a></span></li></ul></li><li><span><a href=\"#Interpreting-multi-objective-results\" data-toc-modified-id=\"Interpreting-multi-objective-results-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Interpreting multi-objective results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Aluminum\" data-toc-modified-id=\"Aluminum-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Aluminum</a></span></li><li><span><a href=\"#Steel\" data-toc-modified-id=\"Steel-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Steel</a></span></li><li><span><a href=\"#Gold\" data-toc-modified-id=\"Gold-6.3\"><span class=\"toc-item-num\">6.3&nbsp;&nbsp;</span>Gold</a></span></li></ul></li><li><span><a href=\"#Doing-all-commodities---tuning-and-interpreting\" data-toc-modified-id=\"Doing-all-commodities---tuning-and-interpreting-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Doing all commodities - tuning and interpreting</a></span><ul class=\"toc-item\"><li><span><a href=\"#Checking-on-retries\" data-toc-modified-id=\"Checking-on-retries-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Checking on retries</a></span></li><li><span><a href=\"#Parameter-distributions\" data-toc-modified-id=\"Parameter-distributions-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Parameter distributions</a></span></li><li><span><a href=\"#Adding-rmse_df-to-future-run\" data-toc-modified-id=\"Adding-rmse_df-to-future-run-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Adding rmse_df to future run</a></span></li><li><span><a href=\"#Checking-baselines\" data-toc-modified-id=\"Checking-baselines-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Checking baselines</a></span></li><li><span><a href=\"#Checking-train-vs-test\" data-toc-modified-id=\"Checking-train-vs-test-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Checking train vs test</a></span></li><li><span><a href=\"#Unconstrained-vs-constrained-tuning\" data-toc-modified-id=\"Unconstrained-vs-constrained-tuning-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Unconstrained vs constrained tuning</a></span></li><li><span><a href=\"#T-SNE-and-PCA\" data-toc-modified-id=\"T-SNE-and-PCA-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>T-SNE and PCA</a></span></li><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-7.8\"><span class=\"toc-item-num\">7.8&nbsp;&nbsp;</span>Feature importance</a></span></li><li><span><a href=\"#Trying-SHAP\" data-toc-modified-id=\"Trying-SHAP-7.9\"><span class=\"toc-item-num\">7.9&nbsp;&nbsp;</span>Trying SHAP</a></span><ul class=\"toc-item\"><li><span><a href=\"#For-defense\" data-toc-modified-id=\"For-defense-7.9.1\"><span class=\"toc-item-num\">7.9.1&nbsp;&nbsp;</span>For defense</a></span></li></ul></li><li><span><a href=\"#Integrated-model\" data-toc-modified-id=\"Integrated-model-7.10\"><span class=\"toc-item-num\">7.10&nbsp;&nbsp;</span>Integrated model</a></span></li><li><span><a href=\"#Determining-which-hyperparameters-to-use\" data-toc-modified-id=\"Determining-which-hyperparameters-to-use-7.11\"><span class=\"toc-item-num\">7.11&nbsp;&nbsp;</span>Determining which hyperparameters to use</a></span></li><li><span><a href=\"#Checking-for-supply-elasticity-to-price\" data-toc-modified-id=\"Checking-for-supply-elasticity-to-price-7.12\"><span class=\"toc-item-num\">7.12&nbsp;&nbsp;</span>Checking for supply elasticity to price</a></span></li></ul></li><li><span><a href=\"#Running-scrap-demand-scenarios\" data-toc-modified-id=\"Running-scrap-demand-scenarios-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Running scrap demand scenarios</a></span></li><li><span><a href=\"#Interpreting-scrap-demand-scenario-results\" data-toc-modified-id=\"Interpreting-scrap-demand-scenario-results-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Interpreting scrap demand scenario results</a></span><ul class=\"toc-item\"><li><span><a href=\"#Fruity-plots\" data-toc-modified-id=\"Fruity-plots-9.1\"><span class=\"toc-item-num\">9.1&nbsp;&nbsp;</span>Fruity plots</a></span></li><li><span><a href=\"#Fruity-feature-importance\" data-toc-modified-id=\"Fruity-feature-importance-9.2\"><span class=\"toc-item-num\">9.2&nbsp;&nbsp;</span>Fruity feature importance</a></span></li><li><span><a href=\"#Trying-for-hyperparam-distribution\" data-toc-modified-id=\"Trying-for-hyperparam-distribution-9.3\"><span class=\"toc-item-num\">9.3&nbsp;&nbsp;</span>Trying for hyperparam distribution</a></span></li></ul></li><li><span><a href=\"#Big-sensitivity\" data-toc-modified-id=\"Big-sensitivity-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Big sensitivity</a></span></li><li><span><a href=\"#Paper-figures---main\" data-toc-modified-id=\"Paper-figures---main-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Paper figures - main</a></span></li><li><span><a href=\"#Paper-figures---SI\" data-toc-modified-id=\"Paper-figures---SI-12\"><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Paper figures - SI</a></span></li><li><span><a href=\"#Trying-to-get-USGS-data---just-ask-for-it\" data-toc-modified-id=\"Trying-to-get-USGS-data---just-ask-for-it-13\"><span class=\"toc-item-num\">13&nbsp;&nbsp;</span>Trying to get USGS data - just ask for it</a></span></li><li><span><a href=\"#Extracting-Dahl-Data\" data-toc-modified-id=\"Extracting-Dahl-Data-14\"><span class=\"toc-item-num\">14&nbsp;&nbsp;</span>Extracting Dahl Data</a></span></li><li><span><a href=\"#Criticality---future\" data-toc-modified-id=\"Criticality---future-15\"><span class=\"toc-item-num\">15&nbsp;&nbsp;</span>Criticality - future</a></span></li><li><span><a href=\"#Byproduct-trial\" data-toc-modified-id=\"Byproduct-trial-16\"><span class=\"toc-item-num\">16&nbsp;&nbsp;</span>Byproduct trial</a></span></li><li><span><a href=\"#ISIE\" data-toc-modified-id=\"ISIE-17\"><span class=\"toc-item-num\">17&nbsp;&nbsp;</span>ISIE</a></span><ul class=\"toc-item\"><li><span><a href=\"#Round-1,-scrap-supply-only-shocks,-no-collection-response\" data-toc-modified-id=\"Round-1,-scrap-supply-only-shocks,-no-collection-response-17.1\"><span class=\"toc-item-num\">17.1&nbsp;&nbsp;</span>Round 1, scrap supply only shocks, no collection response</a></span></li><li><span><a href=\"#SHAP\" data-toc-modified-id=\"SHAP-17.2\"><span class=\"toc-item-num\">17.2&nbsp;&nbsp;</span>SHAP</a></span></li></ul></li><li><span><a href=\"#Checking-copper\" data-toc-modified-id=\"Checking-copper-18\"><span class=\"toc-item-num\">18&nbsp;&nbsp;</span>Checking copper</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4c95d",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80aeab3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T21:53:29.610487Z",
     "start_time": "2022-07-26T21:53:23.658194Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from modules.integration import Integration\n",
    "from modules.useful_functions import *\n",
    "from modules.integration_functions import *\n",
    "from scipy import stats\n",
    "from random import seed, sample, shuffle\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from modules.Individual import *\n",
    "from modules.Many import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import shap\n",
    "import fasttreeshap as shap # works exactly the same for the TreeExplainer function, should be faster\n",
    "from itertools import permutations,combinations\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950c6619",
   "metadata": {
    "code_folding": [
     40,
     64,
     82,
     95,
     107,
     123,
     140,
     163,
     178
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# make sure that for parameters like incentive_mine_cost_change_per_year, we capture the increases \n",
    "# from the prior years if we change this variable midway through a simulation\n",
    "filename='generalization/Scenario setup.xlsx'\n",
    "big = get_scenario_dataframe(filename)\n",
    "update = big.loc['1']\n",
    "\n",
    "class thing:\n",
    "    def __init__(self):\n",
    "        self.initial_value=0\n",
    "        self.hyperparam = pd.DataFrame(columns=['Value'])\n",
    "        self.hyperparam.loc['collection_rate_price_response','Value'] = True\n",
    "        self.hyperparam.loc['direct_melt_price_response','Value'] = True\n",
    "        self.scenario_update_df = update.copy()\n",
    "        \n",
    "    def run(self):\n",
    "        for i in np.arange(1990,2040):\n",
    "            self.i = i\n",
    "            self.update_hyperparam_scenario_input()\n",
    "    \n",
    "    def update_hyperparam_scenario_input(self):\n",
    "        \"\"\"\n",
    "        Run within any year to update hyperparameters from the scenario input excel file\n",
    "        \"\"\"\n",
    "        if self.i==2002:\n",
    "            return None\n",
    "        intersect = np.intersect1d(self.i, self.scenario_update_df.index.get_level_values(1).unique())\n",
    "        if len(intersect)>0:\n",
    "            update_this_year = self.scenario_update_df.loc[idx[:,self.i]]\n",
    "            if update_this_year.index.duplicated().any():\n",
    "                ind = update_this_year.loc[update_this_year.index.duplicated()].index\n",
    "                update_this_year = update_this_year.loc[~update_this_year.index.duplicated()]\n",
    "                warnings.warn(f'\\nMultiple entries for the same year: {list(ind)}, {self.i}  |  Only the first row will be implemented.')\n",
    "            for v in update_this_year.index.get_level_values(0).unique():\n",
    "                value = update_this_year.loc[v] if update_this_year.index.nlevels==1\\\n",
    "                            else update_this_year.loc[v].loc[self.i]\n",
    "                self.hyperparam.loc[v] = value\n",
    "                print(self.i, v, value)\n",
    "\n",
    "def scenario_update_scrap_handling(self, update):\n",
    "    self.scenario_type = ''\n",
    "    self.collection_rate_price_response = self.hyperparam['Value']['collection_rate_price_response']\n",
    "    self.direct_melt_price_response = self.hyperparam['Value']['direct_melt_price_response']\n",
    "    self.collection_rate_duration = 0\n",
    "    self.collection_rate_pct_change_tot = 0\n",
    "    self.collection_rate_pct_change_inc = 0\n",
    "    self.direct_melt_duration = 0\n",
    "    self.direct_melt_pct_change_tot = 0\n",
    "    self.direct_melt_pct_change_inc = 0\n",
    "    self.secondary_refined_duration = 0\n",
    "    self.secondary_refined_pct_change_tot = 0\n",
    "    self.secondary_refined_pct_change_inc = 0\n",
    "    self.secondary_refined_alt = False\n",
    "    self.direct_melt_alt = False\n",
    "    \n",
    "    update = check_year_consistency(self, update)\n",
    "    update = check_duration_variables(self,update)\n",
    "    update = check_scrap_demand_overlap(update)\n",
    "    update = update_from_scrap_demand(update)\n",
    "    update = update_percent_values(self, update)\n",
    "    self.scenario_update_df = update.copy()\n",
    "    update_scenario_type(self)\n",
    "\n",
    "scrap_parameters = ['scenario_type',\n",
    "                                'collection_rate_price_response',\n",
    "                                'direct_melt_price_response',\n",
    "                                'secondary_refined_price_response',\n",
    "                                'collection_rate_duration',\n",
    "                                'collection_rate_pct_change_tot',\n",
    "                                'collection_rate_pct_change_inc',\n",
    "                                'scrap_demand_duration',\n",
    "                                'scrap_demand_pct_change_tot',\n",
    "                                'scrap_demand_pct_change_inc',\n",
    "                                'direct_melt_duration',\n",
    "                                'direct_melt_pct_change_tot',\n",
    "                                'direct_melt_pct_change_inc',\n",
    "                                'secondary_refined_duration',\n",
    "                                'secondary_refined_pct_change_tot',\n",
    "                                'secondary_refined_pct_change_inc',\n",
    "                                ]\n",
    "\n",
    "def check_year_consistency(self, update):\n",
    "    update_ind = update.index.get_level_values(0).unique()\n",
    "    intersect = np.intersect1d(update_ind, scrap_parameters)\n",
    "    scrap_param_in_update = update.loc[intersect].index.get_level_values(0)\n",
    "    if len(scrap_param_in_update.unique())!=len(scrap_param_in_update):\n",
    "        warnings.warn(f'\\nCan only accept one row for each Scrap scenario in {filename}; other parameter categories are ok to have duplicates for different years. Only the first row will be implemented.')\n",
    "    years = update.loc[intersect].index.get_level_values(1).unique()\n",
    "    if np.any(years!=years[0]):\n",
    "        warnings.warn('''\\nUsing 2019 for integ.scrap_shock_year since there are discrepancies on scrap year\n",
    "coming from the scenario input file:\\n'''+filename+\n",
    "'''\\nTo use a different year, ensure all values are matching in the Year column.\n",
    "**This includes any values in the Year column left blank; please ensure that no \n",
    "Scrap scenario variables have blank Year columns if you want to change the year of the\n",
    "scrap supply/demand change**''')\n",
    "        self.scrap_shock_year=2019\n",
    "        update_ph = update.loc[intersect]\n",
    "        update_ph.index = pd.MultiIndex.from_tuples([(k[0],scrap_shock_year) for k in update_ph.index])\n",
    "        update = pd.concat([\n",
    "            update.drop(intersect,level=0),\n",
    "            update_ph\n",
    "        ])\n",
    "    else:\n",
    "        self.scrap_shock_year=int(years[0])\n",
    "    return update\n",
    "\n",
    "def check_duration_variables(self,update):\n",
    "    \"\"\"\n",
    "    making sure the _duration variables are set correctly\n",
    "    \"\"\"\n",
    "    update_ind = update.index.get_level_values(0).unique()\n",
    "    intersect = np.intersect1d(update_ind, scrap_parameters)\n",
    "    for q in ['collection_rate','scrap_demand']:\n",
    "        if np.any([q in i for i in intersect])\\\n",
    "            and q+'_duration' not in intersect:\n",
    "            warnings.warn('\\n'+q+'_duration not set, using default value 1')\n",
    "            update = pd.concat([\n",
    "                update,\n",
    "                pd.Series(1, pd.MultiIndex.from_tuples([(q+'_duration',self.scrap_shock_year)])),\n",
    "        ])\n",
    "    return update\n",
    "    \n",
    "def check_scrap_demand_overlap(update):\n",
    "    \"\"\"\n",
    "    checking that we don't have overlap between scrap_demand, direct_melt, and secondary_refined variables\n",
    "    \"\"\"\n",
    "    update_ind = update.index.get_level_values(0).unique()\n",
    "    intersect = np.intersect1d(update_ind, scrap_parameters)\n",
    "    if np.any(['direct_melt' in k or 'secondary_refined' in k for k in intersect]) and\\\n",
    "        np.any(['scrap_demand' in k for k in intersect]):\n",
    "        warnings.warn('''\\nIncluding variables starting with direct_melt or secondary_refined\n",
    "        alongside variables starting with scrap_demand means that the scrap_demand variable will\n",
    "        supercede the others. If you want to specify the direct_melt and secondary_refined behavior \n",
    "        separately, set those variables and leave scrap_demand variables blank.\n",
    "        ''')\n",
    "        for k in [k for k in intersect if 'direct_melt' in k or 'secondary_refined' in k]:\n",
    "            update = update.drop(k,level=0)\n",
    "    return update\n",
    "\n",
    "def update_from_scrap_demand(update):\n",
    "    \"\"\"\n",
    "    updating the secondary refining and direct melt variables from scrap demand input\n",
    "    \"\"\"\n",
    "    update_ind = update.index.get_level_values(0).unique()\n",
    "    intersect = np.intersect1d(update_ind, scrap_parameters)\n",
    "\n",
    "    for k in [i for i in intersect if 'scrap_demand' in i]:\n",
    "        value = update.loc[k].iloc[0]\n",
    "        if 'duration' in k:\n",
    "            refine_val, direct_val = value, value\n",
    "        else:\n",
    "            refine_val = value*0.4\n",
    "            direct_val = value*0.6\n",
    "        refine_str = 'secondary_refined'+k.split('scrap_demand')[1]\n",
    "        direct_str = 'direct_melt'+k.split('scrap_demand')[1]\n",
    "        update = pd.concat([\n",
    "            update,\n",
    "            pd.Series(refine_val, pd.MultiIndex.from_tuples([(refine_str,self.scrap_shock_year)])),\n",
    "            pd.Series(direct_val, pd.MultiIndex.from_tuples([(direct_str,self.scrap_shock_year)]))\n",
    "        ])\n",
    "    return update\n",
    "    \n",
    "def update_percent_values(self, update):\n",
    "    \"\"\"\n",
    "    updating percentage values\n",
    "    \"\"\"\n",
    "    update_ind = update.index.get_level_values(0).unique()\n",
    "    intersect = np.intersect1d(update_ind, scrap_parameters)\n",
    "\n",
    "    for k in intersect:\n",
    "        value = update.loc[k].iloc[0]\n",
    "        if '_pct_' in k: \n",
    "            update.loc[k] = update.loc[[k]]/100 + 1\n",
    "            value = value/100 +1\n",
    "        setattr(self,k,value)\n",
    "    return update\n",
    "\n",
    "def update_scenario_type(self):\n",
    "    update = self.scenario_update_df.copy()\n",
    "    update_ind = update.index.get_level_values(0).unique()\n",
    "    intersect = np.intersect1d(update_ind, scrap_parameters)\n",
    "    initial_scenario_type=self.scenario_type\n",
    "    if np.any(['collection' in j for j in intersect]) and\\\n",
    "        np.any(['scrap_demand' in j for j in intersect]):\n",
    "        self.scenario_type = 'both-alt'\n",
    "    elif np.any(['collection' in j for j in intersect]):\n",
    "        self.scenario_type = 'scrap supply'\n",
    "    elif np.any(['scrap_demand' in j for j in intersect]):\n",
    "        self.scenario_type = 'scrap demand-alt'\n",
    "    if 'scenario_type' in intersect and initial_scenario_type!=self.scenario_type:\n",
    "        warnings.warn(\"\"\"\n",
    "        Giving a scenario_type input means there can be discrepancy between the\n",
    "        other variables given and the scenario_type; ensure this variable is one of\n",
    "        `scrap supply`, `scrap demand`, `scrap demand-alt`, `both`, or `both-alt`.\n",
    "        \n",
    "        \"\"\")\n",
    "\n",
    "self = thing()\n",
    "scenario_update_scrap_handling(self, update)\n",
    "self.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8924f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c807e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T18:03:03.827954Z",
     "start_time": "2022-06-09T18:03:03.819975Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "​\n",
    "class IterTimer():\n",
    "    '''Timer for iterations. Usage:\n",
    "    \n",
    "    n_iters = 50\n",
    "    timer = IterTimer(n_iters=n_iters, ...)\n",
    "    \n",
    "    for args in arguments:\n",
    "        timer.start_iter()\n",
    "        your_function(args)\n",
    "        t_end, mean_iter = timer.end_iter()\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_iters, default_print=True, window=0, log_times=True, log_folder=\"F:/Code/misc/IterTimer/timer_data\"):\n",
    "        '''\n",
    "        args:\n",
    "            n_iters: (int) Total number of iterations.\n",
    "            default_print: (bool) Whether to let IterTimer print at the end of the iteration. WARNING: uses in-place priting that might interfere with other printing (default: True).\n",
    "            window: (int) Number of iterations to consider when making the average, -10 is the last 10 iterations, 0 is all (default: 0).\n",
    "            log_times: (bool) Whether to save the iteration times in an .npy file, useful to make the ETA prediciton code better (default: True).\n",
    "            log_folder: (str) Folder location of where to save iteration times, using an absolute path is recommended (default: F:/Code/misc/IterTimer/timer_data).\n",
    "        '''\n",
    "        \n",
    "        self.iter_times = []\n",
    "        self.n_iters = n_iters\n",
    "        self.default_print = default_print\n",
    "        self.window = window\n",
    "        self.current_iter = 0\n",
    "        self.log_times = log_times\n",
    "        \n",
    "        if self.log_times:\n",
    "            #saving the iter times just to get some data about what I tend to measure to find best method\n",
    "            import uuid\n",
    "            self.save_file = f\"{log_folder}/{uuid.uuid4()}.{self.n_iters}.npy\"\n",
    "    \n",
    "    def start_iter(self):\n",
    "        '''Start measuring time.'''\n",
    "        self.start = time.perf_counter()\n",
    "    \n",
    "    def end_iter(self, default_print=False):\n",
    "        '''Finish measuring time and calculate relevant metrics. Will print if defaul_print=True.\n",
    "        \n",
    "        args:\n",
    "            default_print: (bool) Whether to let IterTimer print at the end of the iteration, overwrites Class default_print (default: False).\n",
    "            \n",
    "        returns:\n",
    "            t_end: (float) Absolute ETA in seconds.\n",
    "            mean_iter: (float) Mean iteration time.\n",
    "        '''\n",
    "        \n",
    "        #keep track of time and calculate ETA based on mean iteration time\n",
    "        self.iter_times.append(time.perf_counter()-self.start)\n",
    "        self.mean_iter = np.array(self.iter_times[self.window:]).mean() #only take the last few iterations to calculate mean to account for changing conditions\n",
    "        self.t_end = time.time() + self.mean_iter*(self.n_iters-self.current_iter-1)\n",
    "        \n",
    "        #if default_print call the function to print\n",
    "        if self.default_print or default_print: self.print_default()\n",
    "        \n",
    "        #incremenent the number of iterations\n",
    "        self.current_iter += 1\n",
    "        \n",
    "        #saving the iter times just to get some data about what I tend to measure to find best method\n",
    "        if self.log_times: np.save(self.save_file, self.iter_times)\n",
    "        \n",
    "        return self.t_end, self.mean_iter\n",
    "    \n",
    "    def print_default(self):\n",
    "        '''Default printing routine.'''\n",
    "        if self.current_iter+1 == self.n_iters:\n",
    "            print(f\"Iteration {self.current_iter+1}/{self.n_iters}, ETA: {self.get_time(time.localtime(self.t_end))} (average time per iteration: {self.mean_iter:.3f}s)\")\n",
    "        else:\n",
    "            print(f\"Iteration {self.current_iter+1}/{self.n_iters}, ETA: {self.get_time(time.localtime(self.t_end))} (average time per iteration: {self.mean_iter:.3f}s)\", end=\"\\r\")\n",
    "            \n",
    "    def get_time(self, time):\n",
    "        '''Converts a time.struct_time into HH:mm:ss.'''\n",
    "        return f\"{time.tm_hour:02}:{time.tm_min:02}:{time.tm_sec:02}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4258a090-6756-4d2d-b574-af0a820af039",
   "metadata": {},
   "source": [
    "# Aluminum issue check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61294405-8a09-4324-8e39-d9a1c1035c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity_inputs.loc[[i for i in commodity_inputs.index if 'primary_prod' in i]]\n",
    "# integ.mining.ml['Production (kt)'].unstack()\n",
    "# integ.mining.all_inc_mines\n",
    "max(stats.lognorm.rvs(1.5,loc=0,scale=5.53036,size=100))\n",
    "integ.mining.ml['Cash flow expect ($M)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6b21db-5b95-4c8d-b144-914075106a9c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "commodity_inputs = pd.read_pickle('hyperparams_error')['Value'].dropna()\n",
    "commodity_inputs.loc['incentive_require_tune_years'] = 10\n",
    "commodity_inputs.loc['presimulate_n_years'] = 10\n",
    "commodity_inputs.loc['end_calibrate_years'] = 10\n",
    "commodity_inputs.loc['start_calibrate_years'] = 5\n",
    "commodity_inputs.loc['close_price_method'] = 'probabilistic'\n",
    "# commodity_inputs.loc['incentive_opening_probability'] = 0.15\n",
    "# commodity_inputs.loc['refinery_capacity_growth_lag'] = 0\n",
    "integ = Integration(simulation_time=np.arange(2001,2020),verbosity=-1)\n",
    "for i in commodity_inputs.index:\n",
    "    if i != 'simulation_time':\n",
    "        integ.hyperparam.loc[i,'Value'] = commodity_inputs[i]\n",
    "integ.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f719f69-39d5-45f4-93db-ecce0be6344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = create_result_df(integ)['Global'][0]\n",
    "results = df.copy()\n",
    "variables = ['Total','Conc','Ref.','Scrap','Spread','TCRC','Refined','CU','SR','Direct','Mean total','mine grade','Conc. SD','Ref. SD','Scrap SD']\n",
    "fig,ax=easy_subplots(variables)\n",
    "for var,a in zip(variables,ax):\n",
    "    parameters = [i for i in results.columns if var in i and ('SD' not in i or 'SD' in var)]\n",
    "    res = results[parameters].loc[2001:]\n",
    "    param_str = ', '.join(parameters) if len(', '.join(parameters))<30 else ',\\n'.join(parameters)\n",
    "    if res.shape[1]>0:\n",
    "        res.plot(ax=a,title=param_str)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac5f202-1253-42f1-933e-ae6877810e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "(results['Ref. demand']/results['Ref. supply']).loc[2020:]\n",
    "results['Ref. demand'].where(results['Ref. demand']>400).fillna(400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43967c91",
   "metadata": {},
   "source": [
    "# Steel issue check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aa4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_run=['Steel']\n",
    "mod = Many()\n",
    "mod.run_all_integration(n_runs=200, n_params=3, n_jobs=3,\n",
    "                        tuned_rmse_df_out_append='_mcpe0',\n",
    "                        train_time=np.arange(2001,2020),\n",
    "                        simulation_time=np.arange(2001,2041),\n",
    "                        normalize_objectives=True,\n",
    "                        constrain_previously_tuned=False,\n",
    "                        commodities=to_run,\n",
    "                        force_integration_historical_price=False,\n",
    "                        filename_modifier='_mcpe0',\n",
    "                        n_parallel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9348c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "integ = Integration(data_folder='generalization/data',simulation_time=np.arange(2001,2041),\n",
    "                    commodity='Steel',#, input_hyperparam=mod.s.mod.hyperparam,\n",
    "                   price_to_use='log', historical_price_rolling_window=5)\n",
    "integ.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfec4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "file = open('self.pkl','rb')\n",
    "full = pickle.load(file)\n",
    "file.close()\n",
    "file = open('self_mining.pkl','rb')\n",
    "mine = pickle.load(file)\n",
    "file.close()\n",
    "mine.resources_contained_series\n",
    "# mine.inc.primary_price_series.loc[2000:]\n",
    "# mine.concentrate_supply_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ab04ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full.primary_commodity_price.loc[2019:]\n",
    "# full.tcrc.loc[2019:]\n",
    "# full.concentrate_supply\n",
    "full.demand\n",
    "mine.demand_series.loc[2018:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4afc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pd.DataFrame(worst)\n",
    "p['Value']=2\n",
    "p.loc['verbosity','Value'] = pd.Series([pd.read_pickle('the worst.pkl')['primary_sxew_fraction_series']],['verbosity'])\n",
    "worst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56827a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "worst = pd.read_pickle('the worst.pkl').dropna(how='all')\n",
    "integ = Integration(data_folder='generalization/data',simulation_time=np.arange(2001,2041),\n",
    "                    commodity='Cu', input_hyperparam=worst,\n",
    "                   price_to_use='log', historical_price_rolling_window=5)\n",
    "integ.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796ba3f",
   "metadata": {},
   "source": [
    "# Initial integration run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f715ac8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T23:08:31.038698Z",
     "start_time": "2022-07-20T23:08:30.437104Z"
    }
   },
   "outputs": [],
   "source": [
    "new_param = pd.read_excel('generalization/data/case study data.xlsx',index_col=0)\n",
    "al_param = new_param['Al']\n",
    "al_param.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1aadee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T23:09:23.968771Z",
     "start_time": "2022-07-20T23:08:32.728410Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "integ = Integration(simulation_time=np.arange(2019,2041),verbosity=4,scenario_name='sd-alt_pr_1yr_20%tot_0%inc',data_folder='generalization/data',commodity='Al',price_to_use='log')\n",
    "for i in al_param.index:\n",
    "    integ.hyperparam.loc[i,'Value'] = al_param[i]\n",
    "integ.hyperparam.loc['incentive_require_tune_years','Value'] = 10\n",
    "integ.hyperparam.loc['presimulate_n_years','Value'] = 10\n",
    "integ.hyperparam.loc['pri CU price elas','Value']=0.5\n",
    "integ.hyperparam.loc['sec CU price elas','Value']=0.5\n",
    "integ.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baadc72-91c9-407b-94d9-7f216882ebb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(integ.direct_melt_demand-integ.additional_direct_melt).loc[2018:]\n",
    "integ.additional_direct_melt['Global']/integ.total_demand['Global'].loc[2018:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3ff0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T22:45:15.189109Z",
     "start_time": "2022-07-20T22:45:14.857095Z"
    }
   },
   "outputs": [],
   "source": [
    "integ.mining.hyperparam\n",
    "integ.demand.hyperparam\n",
    "integ.refine.hyperparam\n",
    "ml = integ.mining.ml.copy()\n",
    "new = ml.loc[ml['Opening']>2019]\n",
    "old = ml.loc[ml['Opening']<=2019]\n",
    "new['Head grade (%)'].unstack(0).mean().plot(label='New')\n",
    "old['Head grade (%)'].unstack(0).mean().plot(label='Old')\n",
    "plt.legend()\n",
    "integ.simulation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3b7a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T23:09:54.665728Z",
     "start_time": "2022-07-20T23:09:52.126707Z"
    }
   },
   "outputs": [],
   "source": [
    "reg_results = create_result_df(integ)\n",
    "results = reg_results['Global'][0]\n",
    "variables = ['Total','Conc','Ref.','Scrap','Spread','TCRC','Refined','CU','SR','Direct','Mean total','mine grade']\n",
    "fig,ax=easy_subplots(variables)\n",
    "for var,a in zip(variables,ax):\n",
    "    results[[i for i in results.columns if var in i]].plot(ax=a)\n",
    "    a.set(xlim=(2019,a.get_xlim()[1]))\n",
    "plt.figure()\n",
    "plt.plot(results['Total demand'],label='Total demand',color='k')\n",
    "plt.stackplot(results.index, results[['Pri. ref. prod.','Sec. ref. prod.','Direct melt']].T, labels=['Pri. ref.','Sec. ref.','Direct melt']);\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9db48",
   "metadata": {},
   "source": [
    "# Tuning individual materials, historical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61a4476",
   "metadata": {},
   "source": [
    "## Demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeacbe2",
   "metadata": {},
   "source": [
    "Initial run to tune the demand/intensity-related parameters, where historical price is used to approach historical demand. The best parameters found here are used in later runs. This approach seems to require far fewer runs, 25-50 seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7838ddec-6ca5-405e-815f-740f09a6b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if 'shist1' in globals().keys():\n",
    "    file = open('pickleyshist','wb')\n",
    "    pickle.dump(shist1,file)\n",
    "    file.close()\n",
    "else:\n",
    "    file = open('pickleyshist','rb')\n",
    "    shist1 = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92db457f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T23:16:38.986824Z",
     "start_time": "2022-07-28T23:16:33.896504Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from integration import Integration\n",
    "from useful_functions import *\n",
    "from integration_functions import *\n",
    "from scipy import stats\n",
    "from random import seed, sample, shuffle\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from ax.service.utils.report_utils import exp_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31d752-e877-4eb5-9511-f274bf50df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(shist1.mod.commodity_price_series.index.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d81012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T23:22:07.591135Z",
     "start_time": "2022-07-28T23:16:44.167301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "material='Steel'\n",
    "element_commodity_map = {'Al':'Aluminum','Au':'Gold','Cu':'Copper','Steel':'Steel','Co':'Cobalt','REEs':'REEs','W':'Tungsten','Sn':'Tin','Ta':'Tantalum','Ni':'Nickel','Ag':'Silver','Zn':'Zinc','Pb':'Lead','Mo':'Molybdenum','Pt':'Platinum','Te':'Telllurium','Li':'Lithium'}\n",
    "filename='data/'+element_commodity_map[material].lower()+'_run_hist_DELETE.pkl'\n",
    "shist1 = Sensitivity(filename,changing_base_parameters_series=material,notes='Monte Carlo aluminum run',\n",
    "                simulation_time=np.arange(2001,2020),OVERWRITE=True,use_alternative_gold_volumes=True, \n",
    "                    historical_price_rolling_window=5,verbosity=0)\n",
    "shist1.historical_sim_check_demand(50,demand_or_mining='demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643c8f68-fc97-4f47-8fd8-51aaf5ad28a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "ready_commodities = ['Steel']#,'Al','Au','Sn','Cu','Ni','Ag','Zn','Pb']\n",
    "element_commodity_map = {'Al':'Aluminum','Au':'Gold','Cu':'Copper','Steel':'Steel','Co':'Cobalt','REEs':'REEs','W':'Tungsten','Sn':'Tin','Ta':'Tantalum','Ni':'Nickel','Ag':'Silver','Zn':'Zinc','Pb':'Lead','Mo':'Molybdenum','Pt':'Platinum','Te':'Telllurium','Li':'Lithium'}\n",
    "time_init = datetime.now()\n",
    "for material in ready_commodities:\n",
    "    start_t = datetime.now()\n",
    "    filename='data/'+element_commodity_map[material].lower()+'_run_hist.pkl'\n",
    "    shist1 = Sensitivity(filename,changing_base_parameters_series=material,notes='Monte Carlo aluminum run',\n",
    "                    simulation_time=np.arange(2001,2020),OVERWRITE=True,use_alternative_gold_volumes=True, \n",
    "                        historical_price_rolling_window=5,verbosity=0)\n",
    "    shist1.historical_sim_check_demand(50,demand_or_mining='demand')\n",
    "    print(f'elapsed time: {str(datetime.now()-start_t)}')\n",
    "\n",
    "print(f'total time for all scenarios: {str(datetime.now()-time_init)}')          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b008330b-4313-41e5-b505-4d16db5b59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "shist1.check_hist_demand_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c170a5e-d07b-46b0-b2ea-729b659b02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update silver to use the gold volume drivers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743cbd7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T23:14:45.157189Z",
     "start_time": "2022-07-28T23:14:44.261377Z"
    }
   },
   "outputs": [],
   "source": [
    "ph = shist1.mod.volumes.groupby(level=1,axis=1).sum()\n",
    "# ph = pd.read_excel('generalization/data/Demand prediction data-copper.xlsx', sheet_name='All sectors', header=[0,1], index_col=0).sort_index().sort_index(axis=1).stack(0).unstack().groupby(level=1,axis=1).sum()\n",
    "ph1 = pd.DataFrame()\n",
    "fig,ax = easy_subplots(ph.columns)\n",
    "for i,a in zip(ph.columns,ax):\n",
    "    ph[i].loc[2001:2019].plot(ax=a,title=i)\n",
    "    \n",
    "    ph2 = ph[i]/ph[i].loc[2001]\n",
    "    ph2*=shist1.mod.hyperparam.loc['sector_dist_'+i.lower(),'Value']\n",
    "    ph1 = pd.concat([ph1,ph2],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c4bf96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T19:18:12.531501Z",
     "start_time": "2022-07-28T19:18:12.284079Z"
    }
   },
   "outputs": [],
   "source": [
    "# ph['Industrial'].rolling(3).mean().loc[2001:2019].plot()\n",
    "# ph1.loc[2001:2019].plot()\n",
    "ph1.sum(axis=1).loc[2001:2019].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799db140",
   "metadata": {},
   "source": [
    "The below can be used to check whether it saved the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca556f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T15:55:29.879276Z",
     "start_time": "2022-07-07T15:55:29.848891Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_pickle('data/updated_commodity_inputs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0911ac13-fb3f-438b-8168-d2218c8f81ce",
   "metadata": {},
   "source": [
    "## Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60540e-d07a-4029-b613-4d38e2046ee4",
   "metadata": {},
   "source": [
    "Initial run to tune the demand/intensity-related parameters, where historical price is used to approach historical demand. The best parameters found here are used in later runs. This approach seems to require far fewer runs, 25-50 seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b212afc-1eb9-4cd1-8f81-b0a9c43cf67d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T23:16:38.986824Z",
     "start_time": "2022-07-28T23:16:33.896504Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from integration import Integration\n",
    "from useful_functions import *\n",
    "from integration_functions import *\n",
    "from scipy import stats\n",
    "from random import seed, sample, shuffle\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from ax.service.utils.report_utils import exp_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e325a01-1dcc-49dc-887f-de1ee3744f07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T23:22:07.591135Z",
     "start_time": "2022-07-28T23:16:44.167301Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "material='aluminum'\n",
    "filename='data/'+material+'_run_hist_DELETE.pkl'\n",
    "col_map = {'aluminum':'Al','steel':'Steel','gold':'Au'}\n",
    "shist = Sensitivity(filename,changing_base_parameters_series=col_map[material],notes='Monte Carlo aluminum run',\n",
    "                simulation_time=np.arange(2001,2020),OVERWRITE=True,use_alternative_gold_volumes=True, \n",
    "                    historical_price_rolling_window=5,verbosity=0,\n",
    "                   incentive_opening_probability_fraction_zero=0)\n",
    "shist.historical_sim_check_demand(5,\n",
    "                                  demand_or_mining='mining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e5ab2-1be6-48f8-9518-db0f862271dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4e564a-3469-4a0c-acf5-80cd3bbddc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "if 'shist' in globals().keys():\n",
    "    file = open('pickleyshist_mining','wb')\n",
    "    pickle.dump(shist,file)\n",
    "    file.close()\n",
    "else:\n",
    "    file = open('pickleyshist_mining','rb')\n",
    "    shist = pickle.load(file)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e519b-009c-471a-a93d-22cc02447367",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-28T23:14:03.350144Z",
     "start_time": "2022-07-28T23:14:00.559834Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "shist.check_hist_demand_convergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b11f9b-a149-4a07-816a-d4ccf2b940a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9979771a-573e-4fe9-81e4-da5528ae9d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f372a5e-caf7-41c4-9a04-ca5c13b10b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "shist.mod.inc.\n",
    "mines['NPV ($M)'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a2faf4-c7dd-4a6e-b9ea-6d42773246b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('data/updated_commodity_inputs.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e064c7",
   "metadata": {},
   "source": [
    "## All"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468e61f",
   "metadata": {},
   "source": [
    "Full sensitivity run to attempt getting the best parameter responses/elasticities. Have gotten ok results with 200 runs (usually ~5 hours), and trying with 400 to see if there is reasonable improvement (at least 24 hours due to longer load and save times for the larger pickle file, or perhaps due to longer prediction times for the Bayesian optimization package, and maybe for checking whether the scenario already exists but I thought I allowed that to be skipped). Should plot best scenario vs number of runs and see if there is an ideal number of runs. The setup below allows for anywhere from 1-3, maybe 4 columns (from the historical data) to serve as objectives within the Bayesian optimization, minimizing root mean squared error (RMSE) between the historical and simulated variables, typically total demand, primary commodity price, and primary supply (mine production)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bdc789",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T15:08:40.334470Z",
     "start_time": "2022-07-29T15:08:35.095965Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from integration import Integration\n",
    "from useful_functions import *\n",
    "from integration_functions import *\n",
    "from scipy import stats\n",
    "from random import seed, sample, shuffle\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from ax.service.utils.report_utils import exp_to_df\n",
    "from Individual import Individual\n",
    "from Many import Many\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import shap\n",
    "import fasttreeshap as shap # works exactly the same for the TreeExplainer function, should be faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38fb084",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2022-08-01T14:25:16.083Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    # timer=IterTimer()\n",
    "    # 7 used original_demand in lines 456-460, 8 used original_production\n",
    "    warnings.simplefilter('error')\n",
    "    material='aluminum'\n",
    "    for n in np.arange(3,4):\n",
    "        thing = '_'+str(n)+'p' if n>1 else ''\n",
    "        filename='data/'+material+'_run_hist_newdemand_test1'+thing+'.pkl'\n",
    "        col_map = {'aluminum':'Al','steel':'Steel','gold':'Au'}\n",
    "        print('--'*15+filename+'-'*15)\n",
    "        s = Sensitivity(filename,changing_base_parameters_series=col_map[material],notes='Monte Carlo aluminum run',\n",
    "                        additional_base_parameters=pd.Series(0,['refinery_capacity_growth_lag']),\n",
    "                        simulation_time=np.arange(2001,2020), include_sd_objectives=False,\n",
    "                        OVERWRITE=True,verbosity=1,historical_price_rolling_window=5,\n",
    "                        constrain_previously_tuned=True)\n",
    "        s.run_historical_monte_carlo(n_scenarios=200,bayesian_tune=True,n_params=n,\n",
    "                                    sensitivity_parameters=['elas','refinery_capacity_fraction_increase_mining','incentive_opening_probability','reserves_ratio_price_lag'])\n",
    "        # add 'response','growth' to sensitivity_parameters input to allow demand parameters to change again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f62dd6-3f26-490f-acf5-6805497d6469",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = Many()\n",
    "# mod.run_all_integration(200,commodities = [i for i in mod.ready_commodities if i not in ['Steel','Al']])\n",
    "mod.run_all_integration(200, normalize_objectives=True, constrain_previously_tuned=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82417c58",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T15:32:53.195786Z",
     "start_time": "2022-07-29T15:32:53.175786Z"
    }
   },
   "outputs": [],
   "source": [
    "indiv = Individual('Au',3,filename='data/gold_run_hist_newdemand_test7_3p.pkl',\n",
    "                   rmse_not_mae=True,weight_price=1,dpi=50,price_rolling=5)\n",
    "# indiv.results['Total demand'].dropna().droplevel(0).sort_index().loc[2001:].plot()\n",
    "# indiv.historical_data['Total demand'].plot()\n",
    "# indiv.hyperparam.loc[[i for i in indiv.hyperparam.index if 'response' in i or 'growth' in i]]\n",
    "# s.historical_data['Primary commodity price'].plot()\n",
    "# s.updated_commodity_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f609fd7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T18:47:24.205798Z",
     "start_time": "2022-07-26T18:47:19.192109Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://ax.dev/tutorials/multiobjective_optimization.html#Set-Objective-Thresholds-to-focus-candidate-generation-in-a-region-of-interest\n",
    "import pandas as pd\n",
    "from ax import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from ax.metrics.noisy_function import NoisyFunctionMetric\n",
    "from ax.service.utils.report_utils import exp_to_df\n",
    "from ax.runners.synthetic import SyntheticRunner\n",
    "from ax.plot.pareto_utils import compute_posterior_pareto_frontier\n",
    "from ax.plot.pareto_frontier import plot_pareto_frontier\n",
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "init_notebook_plotting()\n",
    "\n",
    "# Factory methods for creating multi-objective optimization modesl.\n",
    "from ax.modelbridge.factory import get_MOO_EHVI, get_MOO_PAREGO\n",
    "\n",
    "# Analysis utilities, including a method to evaluate hypervolumes\n",
    "from ax.modelbridge.modelbridge_utils import observed_hypervolume\n",
    "\n",
    "class practice():\n",
    "    def __init__(self):\n",
    "        self.rs = 0\n",
    "\n",
    "    def run(self):\n",
    "        parameter_names = ['x1','x2']\n",
    "        objective_names = ['a','b']\n",
    "        self.parameter_names = parameter_names\n",
    "        parameters = [RangeParameter(name=x, lower=0, upper=1, parameter_type=ParameterType.FLOAT) for x in parameter_names]\n",
    "\n",
    "        search_space = SearchSpace(\n",
    "            parameters=parameters,\n",
    "        )\n",
    "        self.data = pd.DataFrame(np.nan, np.append(parameter_names,objective_names), [])\n",
    "\n",
    "        def run_model1(x):\n",
    "            x = pd.Series(x,self.parameter_names)\n",
    "            first_check = np.all([(x[i] == self.data.loc[i]).any() for i in x.index])\n",
    "            if first_check:\n",
    "                second_check_idx = [(x[i] == self.data.loc[i]).idxmax() for i in x.index]\n",
    "                second_check = np.all(second_check_idx[0]==second_check_idx)\n",
    "            else: second_check=False\n",
    "            check = first_check and second_check\n",
    "            if check:\n",
    "                result = self.data.loc[objective_names,second_check_idx[0]].values\n",
    "#                 print('read result',x.values,result)\n",
    "            else:\n",
    "                result = x['x2']+x['x1'],x['x1']**x['x2']\n",
    "                self.rs += 1\n",
    "#                 print('create result',x.values,result)\n",
    "                x.loc['a'] = result[0]\n",
    "                x.loc['b'] = result[1]\n",
    "                self.data.loc[:,self.data.shape[1]] = x\n",
    "            return result\n",
    "\n",
    "        class MetricA(NoisyFunctionMetric):\n",
    "            def f(self, x: np.ndarray) -> float:\n",
    "\n",
    "                return float(run_model1(x)[0])\n",
    "\n",
    "        class MetricB(NoisyFunctionMetric):\n",
    "            def f(self, x: np.ndarray) -> float:\n",
    "                return float(run_model1(x)[1])\n",
    "\n",
    "        metric_a = MetricA(\"a\", parameter_names, noise_sd=0.0, lower_is_better=False)\n",
    "        metric_b = MetricB(\"b\", parameter_names, noise_sd=0.0, lower_is_better=True)\n",
    "\n",
    "        mo = MultiObjective(\n",
    "            objectives=[Objective(metric=metric_a), Objective(metric=metric_b)],\n",
    "        )\n",
    "        \n",
    "        thresholds = [1,1]\n",
    "\n",
    "        objective_thresholds = [\n",
    "            ObjectiveThreshold(metric=metric, bound=val, relative=False)\n",
    "            for metric, val in zip(mo.metrics, thresholds)\n",
    "        ]\n",
    "\n",
    "        optimization_config = MultiObjectiveOptimizationConfig(\n",
    "            objective=mo,\n",
    "            objective_thresholds=objective_thresholds,\n",
    "        )\n",
    "\n",
    "\n",
    "        #################\n",
    "\n",
    "        # Reasonable defaults for number of quasi-random initialization points and for subsequent model-generated trials.\n",
    "        N_INIT = 6\n",
    "        N_BATCH = 25\n",
    "\n",
    "        def build_experiment():\n",
    "            experiment = Experiment(\n",
    "                name=\"pareto_experiment\",\n",
    "                search_space=search_space,\n",
    "                optimization_config=optimization_config,\n",
    "                runner=SyntheticRunner(),\n",
    "            )\n",
    "            return experiment\n",
    "\n",
    "\n",
    "        ## Initialize with Sobol samples\n",
    "\n",
    "        def initialize_experiment(experiment):\n",
    "            sobol = Models.SOBOL(search_space=experiment.search_space, seed=1234)\n",
    "\n",
    "            for _ in range(N_INIT):\n",
    "                trial = experiment.new_trial(sobol.gen(1))\n",
    "                display(exp_to_df(experiment))\n",
    "                display(experiment.arms_by_name[str(_)+'_0'].parameters)\n",
    "                self.experiment = experiment\n",
    "                self.trial = trial\n",
    "                trial.run()\n",
    "\n",
    "            return experiment.fetch_data()\n",
    "\n",
    "        ###################### Using SOBOL for main run #############################\n",
    "        print('SOBOL')\n",
    "\n",
    "        sobol_experiment = build_experiment()\n",
    "        sobol_data = initialize_experiment(sobol_experiment)\n",
    "\n",
    "        sobol_model = Models.SOBOL(\n",
    "            experiment=sobol_experiment, \n",
    "            data=sobol_data,\n",
    "        )\n",
    "        sobol_hv_list = []\n",
    "        for i in range(N_BATCH):\n",
    "            generator_run = sobol_model.gen(1)\n",
    "            trial = sobol_experiment.new_trial(generator_run=generator_run)\n",
    "            trial.run()\n",
    "            exp_df = exp_to_df(sobol_experiment)\n",
    "            outcomes = np.array(exp_df[['a', 'b']], dtype=np.double)\n",
    "            # Fit a GP-based model in order to calculate hypervolume.\n",
    "            # We will not use this model to generate new points.\n",
    "            dummy_model = get_MOO_EHVI(\n",
    "                experiment=sobol_experiment, \n",
    "                data=sobol_experiment.fetch_data(),\n",
    "            )\n",
    "            try:\n",
    "                hv = observed_hypervolume(modelbridge=dummy_model)\n",
    "            except:\n",
    "                hv = 0\n",
    "                print(\"\\tFailed to compute hv\")\n",
    "            sobol_hv_list.append(hv)\n",
    "            print(f\"\\tIteration: {i}, HV: {hv}\")\n",
    "\n",
    "        sobol_outcomes = np.array(exp_to_df(sobol_experiment)[['a', 'b']], dtype=np.double)\n",
    "        self.sobol_outcomes = exp_to_df(sobol_experiment)\n",
    "        \n",
    "        ###################### Using qNEHVI for main run #############################\n",
    "        # Noisy Expected Hypervolume Improvement. This is our current recommended algorithm for multi-objective optimization.\n",
    "        print('Noisy Expected Hypervolume Improvement')\n",
    "        ehvi_experiment = build_experiment()\n",
    "        ehvi_data = initialize_experiment(ehvi_experiment)\n",
    "\n",
    "        ehvi_hv_list = []\n",
    "        ehvi_model = None\n",
    "        for i in range(N_BATCH):   \n",
    "            ehvi_model = get_MOO_EHVI(\n",
    "                experiment=ehvi_experiment, \n",
    "                data=ehvi_data,\n",
    "            )\n",
    "            generator_run = ehvi_model.gen(1)\n",
    "            trial = ehvi_experiment.new_trial(generator_run=generator_run)\n",
    "            trial.run()\n",
    "            ehvi_data = Data.from_multiple_data([ehvi_data, trial.fetch_data()])\n",
    "\n",
    "            exp_df = exp_to_df(ehvi_experiment)\n",
    "            outcomes = np.array(exp_df[['a', 'b']], dtype=np.double)\n",
    "            try:\n",
    "                hv = observed_hypervolume(modelbridge=ehvi_model)\n",
    "            except:\n",
    "                hv = 0\n",
    "                print(\"\\tFailed to compute hv\")\n",
    "            ehvi_hv_list.append(hv)\n",
    "            print(f\"\\tIteration: {i}, HV: {hv}\")\n",
    "\n",
    "        ehvi_outcomes = np.array(exp_to_df(ehvi_experiment)[['a', 'b']], dtype=np.double)\n",
    "        self.ehvi_outcomes = exp_to_df(ehvi_experiment)\n",
    "        \n",
    "        # Plotting\n",
    "        frontier = compute_posterior_pareto_frontier(\n",
    "            experiment=ehvi_experiment,\n",
    "            data=ehvi_experiment.fetch_data(),\n",
    "            primary_objective=metric_b,\n",
    "            secondary_objective=metric_a,\n",
    "            absolute_metrics=[\"a\", \"b\"],\n",
    "            num_points=20,\n",
    "        )\n",
    "\n",
    "        render(plot_pareto_frontier(frontier, CI_level=0.90)) \n",
    "m = practice()\n",
    "m.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e96400f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T18:46:38.143189Z",
     "start_time": "2022-07-26T18:46:38.127567Z"
    }
   },
   "outputs": [],
   "source": [
    "# m.data\n",
    "# m.sobol_outcomes\n",
    "m.experiment.arms_by_name#['0_0'].parameters\n",
    "# exp_to_df(m.experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6040ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T14:48:16.869161Z",
     "start_time": "2022-07-21T14:43:03.822534Z"
    }
   },
   "outputs": [],
   "source": [
    "from ax.utils.notebook.plotting import render, init_notebook_plotting\n",
    "from ax.utils.measurement.synthetic_functions import hartmann6\n",
    "from ax.plot.pareto_utils import compute_posterior_pareto_frontier\n",
    "from ax.plot.pareto_frontier import plot_pareto_frontier\n",
    "init_notebook_plotting()\n",
    "# render(s.ax_client.get_contour_plot(metric_name='Primary supply'))\n",
    "objectives = s.ax_client.experiment.optimization_config.objective.objectives\n",
    "frontier = compute_posterior_pareto_frontier(\n",
    "    experiment=s.ax_client.experiment,\n",
    "    data=s.ax_client.experiment.fetch_data(),\n",
    "    primary_objective=objectives[1].metric,\n",
    "    secondary_objective=objectives[0].metric,\n",
    "    absolute_metrics=['Primary commodity price', 'Primary supply', 'Total demand'],\n",
    "    num_points=20,\n",
    ")\n",
    "render(plot_pareto_frontier(frontier, CI_level=0.90)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c69187",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T16:31:19.833814Z",
     "start_time": "2022-07-07T16:31:19.733511Z"
    }
   },
   "outputs": [],
   "source": [
    "s.mod.refine.hyperparam\n",
    "# s.mod.refine.ref_paramhttp://localhost:8888/notebooks/Dropbox%20(MIT)/John%20MIT/Research/generalizationOutside/Integrate%20generalization-Copy2.ipynb#\n",
    "pd.read_excel('generalization/data/case study data.xlsx',index_col=0)['Au'].head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5189e",
   "metadata": {},
   "source": [
    "Checking for an individual scenario what the pickle file looks like. Displaying the full big_df below will error because the dataframe exceeds the capabilities of jupyter's printing functions. The dataframe is reassembled into more useful forms in the section below, Interpreting multi-objective results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f73205",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T20:09:00.400367Z",
     "start_time": "2022-07-05T20:09:00.268816Z"
    }
   },
   "outputs": [],
   "source": [
    "big_df = pd.read_pickle('data/aluminum_run_hist5_3p.pkl')\n",
    "big_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b406e7-0c03-4366-a6a8-eca4697fa500",
   "metadata": {},
   "source": [
    "## For debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd1668-11b1-4031-9815-49870930e38b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "idx = pd.IndexSlice\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from mining_class import miningModel\n",
    "%load_ext pyinstrument\n",
    "from copy import deepcopy\n",
    "from integration import Integration\n",
    "\n",
    "# def do_a_run2():\n",
    "m=miningModel(verbosity=10,byproduct=True)\n",
    "m.hyperparam['simulate_opening']=False\n",
    "for i in m.simulation_time:\n",
    "    print('-'*40,i,'-'*40)\n",
    "    m.i = i\n",
    "    m.run()\n",
    "df2 = m.ml.generate_df()\n",
    "    # return m,df2\n",
    "# m,df2=do_a_run2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9512d450-99b4-4486-b44c-d44056db0dfb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drop(self, ind, to_na=True):\n",
    "    for i in self.columns:\n",
    "        if len(np.shape(getattr(self, i))) > 0:\n",
    "            if to_na:\n",
    "                ph = getattr(self, i)\n",
    "                try:\n",
    "                    if i != 'index':\n",
    "                        ph[ind] = np.nan\n",
    "                except ValueError:\n",
    "                    ph = ph.astype(float)\n",
    "                    ph[[i in ind for i in self.index]] = np.nan\n",
    "                setattr(self, i, ph)\n",
    "            else:\n",
    "                setattr(self, i, np.delete(getattr(self, i), np.where([i in ind for i in self.index])))\n",
    "\n",
    "m.ml.loc[2021].drop([5,0])#[[i in m.ml.loc[2020].index for i in m.ml.loc[2021].index]]\n",
    "[i for i in m.ml.loc[2021].columns if getattr(m.ml.loc[2021],i).dtype==bool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f281fcb0-078e-4c76-b458-9aaca7c04561",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([True,False])\n",
    "x[1]=np.nan\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb1dce7-8435-4021-ae05-f08e3485bdcd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "idx = pd.IndexSlice\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from mining_class import miningModel\n",
    "%load_ext pyinstrument\n",
    "from copy import deepcopy\n",
    "from integration import Integration\n",
    "\n",
    "def do_a_run():\n",
    "# if True:\n",
    "    input_file = pd.read_excel('generalization/data/case study data.xlsx',index_col=0)\n",
    "    commodity_inputs = input_file['Steel'].dropna()\n",
    "    commodity_inputs.loc['incentive_require_tune_years'] = 20\n",
    "    commodity_inputs.loc['presimulate_n_years'] = 20\n",
    "    commodity_inputs.loc['end_calibrate_years'] = 20\n",
    "    commodity_inputs.loc['start_calibrate_years'] = 5\n",
    "    commodity_inputs.loc['close_price_method'] = 'max'\n",
    "    integ = Integration(simulation_time=np.arange(2001,2019),byproduct=False,verbosity=0)\n",
    "    for i in commodity_inputs.index:\n",
    "        integ.hyperparam.loc[i,'Value'] = commodity_inputs[i]\n",
    "\n",
    "    integ.run()\n",
    "    df = integ.mining.ml_yr.generate_df()\n",
    "    df2 = integ.mining.ml.generate_df()\n",
    "    return integ,df,df2\n",
    "m,df,df2 = do_a_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4fa7cd-c8ad-4b01-9ec9-a5d96a876876",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2['production_kt'].unstack().sum(axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3d325d-9b24-4932-b14c-5300d9f01387",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%pyinstrument\n",
    "m,df,df2 = do_a_run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b42426-4a24-4ca2-b2b1-9c297f824b1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%pyinstrument\n",
    "do_a_run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b519445-f309-46ef-8209-41cad7377aa9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('generalization')\n",
    "from Many import Many\n",
    "mod = Many()\n",
    "# # mod.run_all_integration(200,commodities = [i for i in mod.ready_commodities if i not in ['Steel','Al']])\n",
    "mod.run_all_integration(200, normalize_objectives=True, constrain_previously_tuned=True, commodities=['Cu','Ni','Ag','Zn','Pb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eed4ca-2d80-4adb-86e1-e9bcae888dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cb = pd.read_pickle('data/aluminum_run_hist_all_3p.pkl').loc['hyperparam']\n",
    "cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228c9cec-1c7e-4070-92bf-4727ba40cea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "material='Al'\n",
    "n=3\n",
    "thing = '_'+str(n)+'p' if n>1 else ''\n",
    "mat = 'aluminum'\n",
    "pkl_folder='data'\n",
    "filename=f'{pkl_folder}/{mat}_run_DEL_all{thing}.pkl'\n",
    "print('--'*15+filename+'-'*15)\n",
    "self.s = Sensitivity(pkl_filename=filename, data_folder=self.data_folder,changing_base_parameters_series=cb,notes=f'Monte Carlo {material} run',\n",
    "                additional_base_parameters=pd.Series(1,['refinery_capacity_growth_lag']),\n",
    "                simulation_time=np.arange(2001,2020), include_sd_objectives=False,\n",
    "                OVERWRITE=True,verbosity=verbosity,historical_price_rolling_window=5,\n",
    "                constrain_previously_tuned=constrain_previously_tuned, normalize_objectives=normalize_objectives,\n",
    "                save_mining_info=save_mining_info, trim_result_df=trim_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f4683-6c17-491f-8f65-745ef115f2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7ecf2477",
   "metadata": {},
   "source": [
    "# Interpreting multi-objective results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfa9af7-f8c2-449d-be8f-8de8cbe4a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from integration import Integration\n",
    "from useful_functions import *\n",
    "from integration_functions import *\n",
    "from scipy import stats\n",
    "from random import seed, sample, shuffle\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from ax.service.utils.report_utils import exp_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fce0917-b51d-4c1c-8f8b-66039b52f08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv.hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0f733-3057-4621-8638-fed8a47d3baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66c8886-e9cc-46d2-b497-590eabed3db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71f5a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T14:09:45.407953Z",
     "start_time": "2022-08-01T14:09:45.155724Z"
    },
    "code_folding": [
     3,
     87,
     310
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from integration import Integration\n",
    "from random import seed, sample, shuffle\n",
    "from demand_class import demandModel\n",
    "from mining_class import miningModel\n",
    "import os\n",
    "\n",
    "from useful_functions import easy_subplots, do_a_regress\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from itertools import combinations\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def get_unit(simulated, historical, param):\n",
    "    \"\"\"\n",
    "    returns dictionary of simulated, historical, and unit.\n",
    "    Unit is simply the unit, supply your own parentheses, etc.\n",
    "    - e.g. USD/t, fraction, Mt, t, kt\n",
    "    \"\"\"\n",
    "    simulated, historical = simulated.copy(), historical.copy()\n",
    "    if np.any([i in param.lower() for i in ['price','tcrc','spread']]):\n",
    "        unit = 'USD/t'\n",
    "    elif 'CU' in param or 'SR' in param:\n",
    "        unit = 'fraction'\n",
    "    else:\n",
    "        min_simulated = abs(simulated).min() if len(simulated.shape)<=1 else abs(simulated).min().min()\n",
    "        max_simulated = abs(simulated).max() if len(simulated.shape)<=1 else abs(simulated).max().max()\n",
    "        mean_simulated = abs(simulated).mean() if len(simulated.shape)<=1 else abs(simulated).mean().mean()\n",
    "        min_historical = abs(historical).min() if len(historical.shape)<=1 else abs(historical).min().min()\n",
    "        max_historical = abs(historical).max() if len(historical.shape)<=1 else abs(historical).max().max()\n",
    "        mean_historical = abs(historical).mean() if len(historical.shape)<=1 else abs(historical).mean().mean()\n",
    "        if np.mean([mean_historical,mean_simulated])>1000:\n",
    "            historical /= 1000\n",
    "            simulated /= 1000\n",
    "            unit = 'Mt'\n",
    "        elif np.mean([mean_historical,mean_simulated])<1:\n",
    "            historical *= 1000\n",
    "            simulated *= 1000\n",
    "            unit = 't'\n",
    "        else:\n",
    "            unit = 'kt'\n",
    "    return {'simulated':simulated, 'historical':historical, 'unit':unit}\n",
    "\n",
    "def is_pareto_efficient_simple(costs):\n",
    "    \"\"\"\n",
    "    Find the pareto-efficient points\n",
    "    :param costs: An (n_points, n_costs) array\n",
    "    :return: A (n_points, ) boolean array, indicating whether each point is Pareto efficient\n",
    "\n",
    "    from Peter at https://stackoverflow.com/questions/32791911/fast-calculation-of-pareto-front-in-python\n",
    "    \"\"\"\n",
    "    is_efficient = np.ones(costs.shape[0], dtype = bool)\n",
    "    for i, c in enumerate(costs):\n",
    "        if is_efficient[i]:\n",
    "            is_efficient[is_efficient] = np.any(costs[is_efficient]<c, axis=1)  # Keep any point with a lower cost\n",
    "            is_efficient[i] = True  # And keep self\n",
    "    return is_efficient\n",
    "\n",
    "class Individual():\n",
    "    '''\n",
    "    Use this class to interpret Sensitivity runs. Input variables to the\n",
    "    class initialization are listed here, methods and resulting variables\n",
    "    for interpretation are listed farther down:\n",
    "    - material: str, Al, Steel, etc., in the format of the case study data.xlsx file column/sheet names. Not necessary if giving the filename input. The combination of material and n_params can be used to get specific files, but they have to match the aluminum_run_hist_3p.pkl format and be in the same folder as this file. Easier to just give the filename.\n",
    "    - n_params: int, can be used with the material input to get the file. If filename is given, can be used to only plot a smaller number of columns (where n_params is any integer <= the number of parameters used in run_historical_monte_carlo()). This can be particularly useful when calling the find_pareto function, so you can see the tradeoff between just the two parameters.\n",
    "    - filename: str, the pickle path/filename produced by your Sensitivity() run.\n",
    "    - historical_data_file_path: str, the path/filename to access the case study data excel file\n",
    "    - rmse_not_mae: bool, True means root mean squared error is used to calculate the error and select the best-performing scenarios, while False means the mean absolute error is used.\n",
    "    - drop_no_price_elasticitiy: bool, True means that the price elasticity to SD must be nonzero and all scenarios where price elasticity to SD (primary_commodity_price_elas_sd in hyperparam) are dropped. False means we leave them in.\n",
    "    - weight_price: float, allows scaling of the normalized Primary commodity price RMSE/MAE such that NORM SUM and NORM SUM OBJ ONLY rows will consider price error more or less heavily. Is in exponential form.\n",
    "    - dpi: int, dots per inch controls the resolution of the figures displayed.\n",
    "\n",
    "    -------------------------------------------\n",
    "\n",
    "    After initialization, if the instance of Individual() is called indiv,\n",
    "    use the following variable names and methods/functions:\n",
    "    - if using a file after running Sensitivity().historical_sim_check_demand():\n",
    "        - var indiv.historical_data: dataframe with the historical data loaded from case study data excel sheet\n",
    "        - var indiv.simulated_demand: dataframe with simulated demand for each scenario run\n",
    "        - var indiv.price: dataframe with prices for each run (should all be identical here)\n",
    "        - func indiv.plot_demand_results(): plots the demand results, with historical and best-fit scenarios using thicker lines and all others semi-transparent thin lines\n",
    "    - if using a file after running Sensitivity().run_historical_monte_carlo()\n",
    "      or any other where the model used is the Integration() model rather than\n",
    "      demandModel():\n",
    "        - var indiv.results: multi-indexed dataframe with all scenarios and their outputs over time\n",
    "        - var indiv.hyperparam: dataframe with all scenarios and their hyperparameters, as well as their RMSE values for each objective (still called RMSE even when MAE is used). Gets updated when the normalize_rmses() function is called to include NORM SUM and NORM SUM OBJ ONLY rows. The NORM SUM row uses the objectives from the Bayesian optimization as well as the scrap, refined, and concentrate supply-demand imbalances. The NORM SUM OBJ ONLY row does not include the SD imblanaces, just the objectives.\n",
    "        - var indiv.historical_data: dataframe with historical data loaded from case study data excel sheet\n",
    "        - func indiv.plot_best_all(): plots the scenario results for each objective given, with historical and the best-case scenario given thicker outlines. Also does a regression of the best and historical to give statistical measure of fit\n",
    "        - func indiv.find_pareto(): adds the is_pareto row to the indiv.hyperparam dataframe, which gives True for all scenarios on the pareto front. Can plot the pareto front for each objective\n",
    "        - func indiv.normalize_rmses(): adds the NORM SUM and NORM SUM OBJ ONLY rows to the indiv.hyperparam dataframe\n",
    "        - func indiv.plot_results(): produces many different plots you can use to try and understand the model outputs. Plots the best overall scenario over time (using NORM SUM or NORM SUM OBJ ONLY) for each objective to allow comparison, plots a heatmap of the hyperparameter values for the best n scenarios, plots the hyperparameter distributions, plots the hyperparameter values vs the error value\n",
    "    '''\n",
    "\n",
    "    def __init__(self,material='Al',n_params=3,filename='',historical_data_path=None,rmse_not_mae=True, drop_no_price_elasticity=True, weight_price=1, dpi=50, price_rolling=1):\n",
    "        self.material = material\n",
    "        self.price_rolling = price_rolling\n",
    "        self.n_params = n_params\n",
    "        self.filename = filename\n",
    "        element_commodity_map = {'Al':'Aluminum','Au':'Gold','Cu':'Copper','Steel':'Steel','Co':'Cobalt','REEs':'REEs','W':'Tungsten','Sn':'Tin','Ta':'Tantalum','Ni':'Nickel','Ag':'Silver','Zn':'Zinc','Pb':'Lead','Mo':'Molybdenum','Pt':'Platinum','Te':'Telllurium','Li':'Lithium'}\n",
    "        commodity_element_map = dict(zip(element_commodity_map.values(),element_commodity_map.keys()))\n",
    "        if filename!='':\n",
    "            if '/' in filename: filename=filename.split('/')[1]\n",
    "            self.material = commodity_element_map[filename.split('_')[0].capitalize()]\n",
    "        self.rmse_not_mae = rmse_not_mae\n",
    "        self.drop_no_price_elasticity = drop_no_price_elasticity\n",
    "        self.weight_price = weight_price\n",
    "        self.dpi = dpi\n",
    "        self.historical_data_path = 'generalization/data/' if historical_data_path==None else historical_data_path\n",
    "        self.historical_data_file_path = self.historical_data_path+'case study data.xlsx'\n",
    "        self.price_adjustment_results_file_path = self.historical_data_path+'price adjustment results.xlsx'\n",
    "\n",
    "        self.get_results()\n",
    "\n",
    "    def get_results(self):\n",
    "        '''\n",
    "        Handles the FileNotFound error for pickle files so the possible files available get printed out, while calling get_results_hyperparam_history for Integration() based models and getting the right variables for demandModel() based models.\n",
    "        '''\n",
    "        try:\n",
    "            if '_DEM' in self.filename or '_mining' in self.filename:\n",
    "                self.demand_flag = '_DEM' in self.filename\n",
    "                self.big_df = pd.read_pickle(self.filename)\n",
    "                big_df = self.big_df.copy()\n",
    "                self.simulated_demand = pd.concat([big_df.loc['results'][i]['Total demand'] for i in big_df.columns],keys=big_df.columns,axis=1).loc[2001:2019]\n",
    "                self.historical_data = pd.read_excel(self.historical_data_file_path,sheet_name=self.material,index_col=0).loc[self.simulated_demand.index].astype(float)\n",
    "                self.historical_data.index = self.historical_data.index.astype(int)\n",
    "                self.historical_data_column_list = ['Total demand','Primary commodity price','Primary supply','Scrap demand','Total production','Primary demand']\n",
    "                self.historical_data_column_list = [j for j in self.historical_data_column_list if j in self.historical_data.columns]\n",
    "\n",
    "                notes = self.big_df[0]['notes']\n",
    "                if 'price rolling: ' in notes:\n",
    "                    self.price_rolling = notes.split('price rolling: ')[1].split(',')[0]\n",
    "                    try: self.price_rolling = int(self.price_rolling)\n",
    "                    except: self.price_rolling = int(self.price_rolling.split()[0])\n",
    "                if 'price version: ' in notes:\n",
    "                    price_to_use = notes.split('price version: ')[1].split(',')[0]\n",
    "                    commodity = self.filename if '/' not in self.filename else self.filename.split('/')[1]\n",
    "                    commodity = commodity.split('_')[0].capitalize()\n",
    "                    if price_to_use!='case study data':\n",
    "                        price_update_file = pd.read_excel(self.price_adjustment_results_file_path,index_col=0)\n",
    "                        price_map = {'log':'log('+commodity+')',  'diff':'∆'+commodity,  'original':commodity+' original'}\n",
    "                        historical_price = price_update_file[price_map[price_to_use]].astype(float)\n",
    "                        historical_price.name = 'Primary commodity price'\n",
    "                        if 'Primary commodity price' in self.historical_data.columns:\n",
    "                            self.historical_data = pd.concat([self.historical_data.drop('Primary commodity price',axis=1),historical_price],axis=1).sort_index().dropna(how='all')\n",
    "                        else:\n",
    "                            self.historical_data = pd.concat([self.historical_data,historical_price],axis=1).sort_index().dropna(how='all')\n",
    "                if 'Primary commodity price' in self.historical_data.columns:\n",
    "                    self.historical_data.loc[:,'Primary commodity price'] = self.historical_data.loc[:,'Primary commodity price'].rolling(self.price_rolling,min_periods=1,center=True).mean()\n",
    "                self.price =  pd.concat([big_df.loc['results'][i]['Primary commodity price'] for i in big_df.columns],keys=big_df.columns,axis=1).loc[2001:2019]\n",
    "                self.hyperparam =  pd.concat([big_df.loc['hyperparam'][i] for i in big_df.columns],keys=big_df.columns,axis=1)\n",
    "                if 'Primary supply' in big_df.loc['results'][big_df.columns[0]].columns:\n",
    "                    self.mine_supply = pd.concat([big_df.loc['results'][i]['Primary supply'] for i in big_df.columns],keys=big_df.columns,axis=1).loc[2001:2019]\n",
    "                if 'rmse_df' in big_df.index:\n",
    "                    self.rmse_df = big_df.loc['rmse_df'].iloc[-1][0]\n",
    "                    self.rmse_df.index = pd.MultiIndex.from_tuples(self.rmse_df.index)\n",
    "                    self.rmse_df = self.rmse_df.unstack(0)\n",
    "            else:\n",
    "                self.get_results_hyperparam_history()\n",
    "        except Exception as e:\n",
    "            print('Files within current path:')\n",
    "            self.check_available_files()\n",
    "            for i in self.filename.split('/')[:-1]:\n",
    "                print('Files within folder ['+i+'/] specified:')\n",
    "                self.check_available_files(i)\n",
    "            raise e\n",
    "\n",
    "    def check_available_files(self,path=None):\n",
    "        '''\n",
    "        displays the available files for the path given, or for the current directory if no path given.\n",
    "        '''\n",
    "        display(os.listdir(path))\n",
    "\n",
    "    def get_results_hyperparam_history(self):\n",
    "        '''\n",
    "        gets all the relevant parameters for Integration() based models, and calculates the RMSE/MAE between scenario results and historical values, in both cases saving them as RMSE+objective name in the self.hyperparam dataframe\n",
    "        '''\n",
    "\n",
    "        material = self.material\n",
    "        n_params = self.n_params\n",
    "        filename = self.filename\n",
    "        historical_data_file_path = self.historical_data_file_path\n",
    "        material_map={'Al':'aluminum','Steel':'steel'}\n",
    "        if n_params==1 and filename=='':\n",
    "            filename = material_map[material]+'_run_hist.pkl'\n",
    "            if not os.path.exists(filename):\n",
    "                filename = material_map[material]+'_run_hist_1p.pkl'\n",
    "        elif n_params>1 and filename=='':\n",
    "            filename = material_map[material]+'_run_hist_'+str(n_params)+'p.pkl'\n",
    "        if type(filename)==str:\n",
    "            big_df = pd.read_pickle(filename)\n",
    "        else:\n",
    "            big_df = pd.concat([pd.read_pickle(i) for i in filename],axis=1).T.reset_index().T\n",
    "        ind = big_df.loc['results'].dropna().index\n",
    "        results = pd.concat([big_df.loc['results',i]['Global'][0] for i in ind],keys=ind)\n",
    "        hyperparameters = pd.concat([big_df.loc['hyperparam'].dropna().loc[i].loc[:,'Value'] for i in ind],keys=ind,axis=1)\n",
    "        if self.drop_no_price_elasticity:\n",
    "            cols = (hyperparameters.loc['primary_commodity_price_elas_sd']!=0)\n",
    "            cols = cols[cols].index\n",
    "            results = results.loc[idx[cols,:],:].copy()\n",
    "            hyperparameters = hyperparameters[cols].copy()\n",
    "\n",
    "        historical_data = pd.read_excel(historical_data_file_path,sheet_name=material,index_col=0).loc[2001:].astype(float)\n",
    "        historical_data.index = historical_data.index.astype(int)\n",
    "        self.historical_data_column_list = ['Total demand','Primary commodity price','Primary supply','Scrap demand','Total production','Primary demand']\n",
    "        self.historical_data_column_list = [j for j in self.historical_data_column_list if j in historical_data.columns]\n",
    "\n",
    "        notes = big_df[0]['notes']\n",
    "        if 'price rolling: ' in notes:\n",
    "            self.price_rolling = notes.split('price rolling: ')[1].split(',')[0]\n",
    "            try: self.price_rolling = int(self.price_rolling)\n",
    "            except: self.price_rolling = int(self.price_rolling.split()[0])\n",
    "        if 'price version: ' in notes:\n",
    "            price_to_use = notes.split('price version: ')[1].split(',')[0]\n",
    "            commodity = self.filename if '/' not in self.filename else self.filename.split('/')[1]\n",
    "            commodity = commodity.split('_')[0].capitalize()\n",
    "            if price_to_use!='case study data':\n",
    "                price_update_file = pd.read_excel(self.price_adjustment_results_file_path,index_col=0)\n",
    "                price_map = {'log':'log('+commodity+')',  'diff':'∆'+commodity,  'original':commodity+' original'}\n",
    "                historical_price = price_update_file[price_map[price_to_use]].astype(float)\n",
    "                historical_price.name = 'Primary commodity price'\n",
    "                if 'Primary commodity price' in historical_data.columns:\n",
    "                    historical_data = pd.concat([historical_data.drop('Primary commodity price',axis=1),historical_price],axis=1).sort_index().dropna(how='all')\n",
    "                else:\n",
    "                    historical_data = pd.concat([historical_data,historical_price],axis=1).sort_index().dropna(how='all')\n",
    "\n",
    "        if 'Primary commodity price' in historical_data.columns:\n",
    "            historical_data.loc[:,'Primary commodity price'] = historical_data.loc[:,'Primary commodity price'].rolling(self.price_rolling,min_periods=1,center=True).mean()\n",
    "\n",
    "        self.objective_params = self.historical_data_column_list[:n_params]\n",
    "        self.objective_results_map = {'Total demand':'Total demand','Primary commodity price':'Refined price',\n",
    "                                 'Primary demand':'Conc. demand','Primary supply':'Mine production',\n",
    "                                'Conc. SD':'Conc. SD','Scrap SD':'Scrap SD','Ref. SD':'Ref. SD'}\n",
    "        for i in ['Conc.','Ref.','Scrap']:\n",
    "            results.loc[:,i+' SD'] = results[i+' supply']-results[i+' demand']\n",
    "        self.sd_ind = [i for i in results.columns if 'SD' in i]\n",
    "        if 'Conc. SD' not in self.objective_params:\n",
    "            self.all_params = np.append(self.objective_params,self.sd_ind)\n",
    "        for obj in self.all_params:\n",
    "            simulated = results[self.objective_results_map[obj]].unstack(0).loc[2001:2019].astype(float)\n",
    "            if self.rmse_not_mae:\n",
    "                if 'SD' in obj:\n",
    "                    rmses = (simulated**2).sum().div(simulated.shape[0])**0.5\n",
    "                else:\n",
    "                    rmses = (simulated.apply(lambda x: x-historical_data[obj])**2).sum().div(simulated.shape[0])**0.5\n",
    "            else:\n",
    "                if 'SD' in obj:\n",
    "                    rmses = abs(simulated).sum()\n",
    "                else:\n",
    "                    rmses = abs(simulated.apply(lambda x: x-historical_data[obj])).sum()\n",
    "            hyperparameters.loc['RMSE '+obj] = rmses\n",
    "\n",
    "        self.big_df = big_df.copy()\n",
    "\n",
    "        if 'rmse_df' in big_df.index:\n",
    "            self.rmse_df = big_df.loc['rmse_df'].iloc[-1][0]\n",
    "            self.rmse_df.index = pd.MultiIndex.from_tuples(self.rmse_df.index)\n",
    "            self.rmse_df = self.rmse_df.unstack(0)\n",
    "\n",
    "        if 'mine_data' in big_df.index and type(big_df.loc['mine_data'].iloc[1])==pd.core.frame.DataFrame:\n",
    "            self.mine_data = pd.concat([big_df[i]['mine_data'] for i in big_df.columns],keys=big_df.columns)\n",
    "        else:\n",
    "            self.mine_data = pd.DataFrame(0,index=results.index,columns=pd.MultiIndex.from_product([['Mine data'],np.arange(0,10)])).stack()\n",
    "        self.mine_data.index.set_names(['scenario','year','mine_id'],inplace=True)\n",
    "\n",
    "        self.results, self.hyperparam, self.historical_data = results.astype(float), hyperparameters, historical_data.astype(float)\n",
    "\n",
    "    def plot_best_all(self):\n",
    "        '''\n",
    "        plots the scenario results for each objective given, with historical and the best-case scenario given thicker outlines. Also does a regression of the best and historical to give statistical measure of fit\n",
    "        '''\n",
    "        if not hasattr(self,'results'):\n",
    "            self.get_results()\n",
    "\n",
    "        hyperparam = self.hyperparam.copy()\n",
    "        results = self.results.copy()\n",
    "        historical_data = self.historical_data.copy().loc[2001:2019]\n",
    "\n",
    "        for obj in self.objective_params:\n",
    "            simulated = results[self.objective_results_map[obj]].unstack(0).loc[2001:2019]\n",
    "            best = simulated[(simulated.astype(float).apply(lambda x: x-historical_data[obj])**2).sum().astype(float).idxmin()]\n",
    "\n",
    "            fig,ax = easy_subplots(3,dpi=self.dpi)\n",
    "            for i,a in enumerate(ax[:2]):\n",
    "                if i==0:\n",
    "                    simulated.plot(linewidth=1,alpha=0.5,legend=False,ax=a)\n",
    "                historical_data[obj].plot(ax=a,label='Historical')\n",
    "                best.plot(ax=a,label='Best simulated')\n",
    "                if i==1:\n",
    "                    a.legend()\n",
    "                a.set(title=obj+' over time',xlabel='Year',ylabel=obj)\n",
    "\n",
    "            do_a_regress(best.astype(float),historical_data[obj].astype(float),ax=ax[2],xlabel='Simulated',ylabel='Historical')\n",
    "            ax[-1].set(title='Historical regressed on simulated')\n",
    "            plt.suptitle(obj+', varying demand parameters (historical sensitivity result)',fontweight='bold')\n",
    "            fig.tight_layout()\n",
    "\n",
    "    def find_pareto(self, plot=False, log=True, plot_non_pareto=True):\n",
    "        '''\n",
    "        adds the is_pareto row to the indiv.hyperparam dataframe, which gives True for all scenarios on the pareto front. Can plot the pareto front for each objective\n",
    "\n",
    "        Inputs:\n",
    "        - plot: bool, whether to plot the pareto result\n",
    "        - log: bool, whether to use log scaling while plotting the pareto result\n",
    "        - plot_non_pareto: bool, whether to include scenarios not on the pareto front when plotting\n",
    "        '''\n",
    "        if not hasattr(self,'results'):\n",
    "            self.get_results()\n",
    "        combos = list(combinations(self.objective_params,2))\n",
    "        cost_df = self.rmse_df.loc[[i for i in self.rmse_df.index if 'RMSE' in i]].T\n",
    "        cost_array = cost_df.values\n",
    "        cost_df.loc[:,'is_pareto'] = is_pareto_efficient_simple(cost_array)\n",
    "        yes = cost_df.loc[cost_df.is_pareto].astype(float)\n",
    "        no = cost_df.loc[cost_df.is_pareto==False].astype(float)\n",
    "\n",
    "        if plot:\n",
    "            fig,ax = easy_subplots(combos, dpi=self.dpi)\n",
    "            for c,a in zip(combos,ax):\n",
    "                if plot_non_pareto:\n",
    "                    no.plot.scatter(x='RMSE '+c[0],y='RMSE '+c[1],loglog=log,ax=a,color='tab:blue')\n",
    "                yes.plot.scatter(x='RMSE '+c[0],y='RMSE '+c[1],loglog=log,ax=a,color='tab:orange')\n",
    "            fig.tight_layout()\n",
    "        self.pareto_ind = yes\n",
    "\n",
    "    def normalize_rmses(self):\n",
    "        '''\n",
    "        adds the NORM SUM and NORM SUM OBJ ONLY rows to the indiv.hyperparam dataframe\n",
    "        '''\n",
    "        for i in [i for i in self.hyperparam.index if 'RMSE' in i]:\n",
    "            self.hyperparam.loc['NORM '+i] = self.hyperparam.loc[i]/self.hyperparam.loc[i].replace(0,1e-6).min()\n",
    "        if 'NORM RMSE Primary commodity price' in self.hyperparam.index:\n",
    "            self.hyperparam.loc['NORM RMSE Primary commodity price'] = self.hyperparam.loc['NORM RMSE Primary commodity price']**self.weight_price\n",
    "        normed = [i for i in self.hyperparam.index if 'NORM' in i]\n",
    "        normed_obj = [i for i in self.hyperparam.index if 'NORM' in i and np.any([j in i for j in self.objective_params])]\n",
    "        self.hyperparam.loc['NORM SUM'] = self.hyperparam.loc[normed].sum()\n",
    "        self.hyperparam.loc['NORM SUM OBJ ONLY'] = self.hyperparam.loc[normed_obj].sum()\n",
    "\n",
    "        # repeating above for rmse_df\n",
    "        for i in [i for i in self.rmse_df.index if 'RMSE' in i]:\n",
    "            self.rmse_df.loc['NORM '+i] = np.log(self.rmse_df.loc[i].astype(float))/np.log(self.rmse_df.loc[i].astype(float)).replace(0,1e-6).min()\n",
    "            self.rmse_df.loc['LOG '+i] = np.log(self.rmse_df.loc[i].astype(float))\n",
    "        if 'NORM Primary commodity price RMSE' in self.rmse_df.index:\n",
    "            self.rmse_df.loc['NORM Primary commodity price RMSE'] = self.rmse_df.loc['NORM Primary commodity price RMSE']**self.weight_price\n",
    "            self.rmse_df.loc['LOG Primary commodity price RMSE'] = self.rmse_df.loc['LOG Primary commodity price RMSE']**self.weight_price\n",
    "        normed = [i for i in self.rmse_df.index if 'NORM' in i]\n",
    "        logged = [i for i in self.rmse_df.index if 'LOG' in i]\n",
    "        normed_obj = [i for i in self.rmse_df.index if 'NORM' in i and np.any([j in i for j in self.objective_params])]\n",
    "        self.rmse_df.loc['NORM SUM'] = self.rmse_df.loc[normed].sum()\n",
    "        self.rmse_df.loc['LOG SUM'] = np.log(np.exp(self.rmse_df.loc[logged]).sum())\n",
    "        self.rmse_df.loc['NORM SUM OBJ ONLY'] = self.rmse_df.loc[normed_obj].sum()\n",
    "\n",
    "    def plot_results(self, plot_over_time=True, n_best=0, include_sd=False, nth_best=1,\n",
    "                     plot_sd_over_time=True,\n",
    "                     plot_best_indiv_over_time=True,\n",
    "                    plot_hyperparam_heatmap=True,\n",
    "                    plot_hyperparam_distributions=True, n_per_plot=3,\n",
    "                    plot_hyperparam_vs_error=True, flip_yx=False,\n",
    "                    plot_best_params=True, plot_supply_demand_stack=True, best_ind=-1):\n",
    "        '''\n",
    "        produces many different plots you can use to try and understand the model outputs. More info is given with each model input bool description below.\n",
    "\n",
    "        Inputs:\n",
    "        plot_over_time: bool, True plots the best overall scenario over time (using NORM SUM or NORM SUM OBJ ONLY) for each objective to allow comparison\n",
    "        n_best: int, the number of scenarios to include in plot_over_time or in plot_hyperparam_distributions\n",
    "        include_sd: bool, True means we use the NORM SUM row to evaluate the best scenario, while False means we use NORM SUM OBJ ONLY\n",
    "        plot_hyperparam_heatmap: bool, True plots a heatmap of the hyperparameter values for the best n scenarios\n",
    "        plot_hyperparam_distributions: bool, True plots the hyperparameter distributions\n",
    "        n_per_plot: int, for use with plot_hyperparam_distributions. Determines how many hyperparameter values are put in each plot, since it can be hard to tell what is going on when there are too many lines in a figure\n",
    "        plot_hyperparam_vs_error: bool, plots the hyperparameter values vs the error value, separate plot for each hyperparameter. Use this to try and see if there are ranges for the best hyperparameter values.\n",
    "        flip_yx: bool, False means plot hyperparam value vs error, while True means plot error vs hyperparam value\n",
    "        '''\n",
    "        fig_list = []\n",
    "        self.normalize_rmses()\n",
    "        norm_sum = 'NORM SUM' if include_sd else 'NORM SUM OBJ ONLY'\n",
    "        if plot_over_time:\n",
    "            fig,ax = easy_subplots(self.all_params[:3],dpi=self.dpi)\n",
    "            for i,a in zip(self.all_params[:3], ax):\n",
    "                results = self.results.copy()[self.objective_results_map[i]].loc[idx[:,2001:]]\n",
    "                if 'SD' not in i:\n",
    "                    historical_data = self.historical_data.copy()[i]\n",
    "                else:\n",
    "                    historical_data = pd.Series(results.min(),[0])\n",
    "\n",
    "                n_best = n_best if n_best>1 else 2\n",
    "                # simulated = self.hyperparam.loc[norm_sum].astype(float).sort_values().head(n_best).index\n",
    "                if nth_best<0:\n",
    "                    simulated = self.rmse_df.loc['score'].astype(float).sort_values().head(n_best).index\n",
    "                else:\n",
    "                    simulated = self.rmse_df.loc['LOG SUM'].astype(float).sort_values().head(n_best).index\n",
    "                results = results.loc[idx[simulated,:]]\n",
    "\n",
    "                diction = get_unit(results, historical_data, i)\n",
    "                results, historical_data, unit = [diction[i] for i in ['simulated','historical','unit']]\n",
    "                # best = self.hyperparam.loc[norm_sum].astype(float).sort_values().index[nth_best-1]\n",
    "                if nth_best<0:\n",
    "                    best_ind = self.rmse_df.loc['score'].astype(float).sort_values().index[-nth_best-1]\n",
    "                else:\n",
    "                    best_ind = self.rmse_df.loc['LOG SUM'].astype(float).sort_values().index[nth_best-1]\n",
    "\n",
    "                best = results.loc[best_ind]\n",
    "                if 'SD' not in i:\n",
    "                    hist_line = a.plot(historical_data,label='Historical',color='k',linewidth=6)\n",
    "                    regr = sm.GLS(historical_data.loc[2001:2019],sm.add_constant(best.loc[2001:2019])).fit(cov_type='HC3')\n",
    "                    reg_res = r'$R^2$'+': {:.3f}'.format(regr.rsquared)\n",
    "                else: reg_res=''\n",
    "                sim_line = a.plot(best,label='Simulated',color='tab:blue',linewidth=6)\n",
    "                reg_point = plt.plot(best,color='w',zorder=0,alpha=0)\n",
    "                if n_best>1:\n",
    "                    a.plot(results.unstack(0),linewidth=1,alpha=0.5,zorder=0)\n",
    "                mins = min(historical_data.min(),best.min())*0.95\n",
    "                maxs = max(historical_data.max(),best.max())*1.1\n",
    "\n",
    "                a.set(title='Best '+i,ylabel=i+' ('+unit+')',xlabel='Year',ylim=(mins,maxs))\n",
    "                a.legend([hist_line[0],sim_line[0],reg_point[0]],['Historical','Simulated',reg_res])\n",
    "                self.sim = results\n",
    "                self.hist = historical_data\n",
    "            fig.tight_layout()\n",
    "            fig_list += [fig]\n",
    "            print(f'best scenario number is: {best_ind}')\n",
    "\n",
    "        if plot_sd_over_time:\n",
    "            fig,ax = easy_subplots(self.all_params[3:],dpi=self.dpi)\n",
    "            for i,a in zip(self.all_params[3:], ax):\n",
    "                results = self.results.copy()[self.objective_results_map[i]].loc[idx[:,2001:]]\n",
    "                if 'SD' not in i:\n",
    "                    historical_data = self.historical_data.copy()[i]\n",
    "                else:\n",
    "                    historical_data = pd.Series(results.min(),[0])\n",
    "\n",
    "                n_best = n_best if n_best>1 else 2\n",
    "                # simulated = self.hyperparam.loc[norm_sum].astype(float).sort_values().head(n_best).index\n",
    "                if nth_best<0:\n",
    "                    simulated = self.rmse_df.loc['score'].astype(float).sort_values().head(n_best).index\n",
    "                else:\n",
    "                    simulated = self.rmse_df.loc['LOG SUM'].astype(float).sort_values().head(n_best).index\n",
    "                results = results.loc[idx[simulated,:]]\n",
    "\n",
    "                results, historical_data, unit = self.get_unit(results, historical_data, i)\n",
    "                # best = self.hyperparam.loc[norm_sum].astype(float).sort_values().index[nth_best-1]\n",
    "                if nth_best<0:\n",
    "                    best_ind = self.rmse_df.loc['score'].astype(float).sort_values().index[-nth_best-1]\n",
    "                else:\n",
    "                    best_ind = self.rmse_df.loc['LOG SUM'].astype(float).sort_values().index[nth_best-1]\n",
    "\n",
    "                best = results.loc[best_ind]\n",
    "                sim_line = a.plot(best,label='Simulated',color='tab:blue')\n",
    "                if 'SD' not in i:\n",
    "                    hist_line = a.plot(historical_data,label='Historical',color='k')\n",
    "                    regr = sm.GLS(historical_data.loc[2001:2019],best.loc[2001:2019]).fit(cov_type='HC3')\n",
    "                    reg_res = r'$R^2$'+': {:.3f}'.format(regr.rsquared)\n",
    "                else: reg_res=''\n",
    "                reg_point = plt.scatter([historical_data.index[0]],[historical_data.iloc[0]],color='w',zorder=0,alpha=0)\n",
    "                if n_best>1:\n",
    "                    a.plot(results.unstack(0),linewidth=1,alpha=0.5,zorder=0)\n",
    "                a.set(title='Best '+i,ylabel=i+' ('+unit+')',xlabel='Year')\n",
    "                a.legend([hist_line[0],sim_line[0],reg_point],['Historical','Simulated',reg_res])\n",
    "\n",
    "            fig.tight_layout()\n",
    "            fig_list += [fig]\n",
    "\n",
    "        cols = self.hyperparam.loc[norm_sum].sort_values().head(n_best).index\n",
    "        ind = [i for i in self.hyperparam.index if type(self.hyperparam.loc[i].iloc[0]) in [float,int]]\n",
    "        ind = [i for i in ind if 'RMSE' not in i and 'NORM' not in i]\n",
    "        ind = self.hyperparam.loc[ind].loc[(self.hyperparam.loc[ind].std(axis=1)>1e-3)].index\n",
    "        self.hyperparams_changing = ind\n",
    "        best_hyperparam = self.hyperparam.copy().loc[ind,cols].astype(float)\n",
    "        if 'close_years_back' in best_hyperparam.index:\n",
    "            best_hyperparam.loc['close_years_back'] /= 10\n",
    "\n",
    "        if plot_best_indiv_over_time:\n",
    "            fig,ax = easy_subplots(self.objective_params,dpi=self.dpi)\n",
    "            for i,a in zip(self.objective_params, ax):\n",
    "                results = self.results.copy()[self.objective_results_map[i]].loc[idx[:,2001:]]\n",
    "                if 'SD' not in i:\n",
    "                    historical_data = self.historical_data.copy()[i]\n",
    "                else:\n",
    "                    historical_data = pd.Series(results.min(),[0])\n",
    "\n",
    "                n_best = n_best if n_best>1 else 2\n",
    "                simulated = self.hyperparam.loc['RMSE '+i].astype(float).sort_values().head(n_best).index\n",
    "                results = results.loc[idx[simulated,:]]\n",
    "\n",
    "                results, historical_data, unit = self.get_unit(results, historical_data, i)\n",
    "                best_ind = self.hyperparam.loc['RMSE '+i].astype(float).idxmin()\n",
    "                best = results.loc[best_ind]\n",
    "                best.plot(ax=a,label='Simulated',color='blue')\n",
    "                if 'SD' not in i:\n",
    "                    historical_data.plot(ax=a,label='Historical',color='k')\n",
    "                if n_best>1:\n",
    "                    results.unstack(0).plot(ax=a,linewidth=1,alpha=0.5,zorder=0,legend=False)\n",
    "                a.set(title=i+f' ({best_ind})',ylabel=i+' ('+unit+')',xlabel='Year')\n",
    "                a.legend(['Simulated','Historical'])\n",
    "            fig_list += [fig]\n",
    "\n",
    "        if plot_hyperparam_heatmap:\n",
    "            fig = plt.figure(figsize=(1.2*n_best,10),dpi=self.dpi)\n",
    "            heat = best_hyperparam.copy()\n",
    "            while (heat.max(axis=1)>1).any():\n",
    "                lrg=heat.max(axis=1)>1\n",
    "                lrg=lrg[lrg].index\n",
    "                heat.loc[lrg,:] /= 10\n",
    "                heat.rename(dict(zip(lrg,[i+'*' for i in lrg])),inplace=True)\n",
    "            sns.heatmap(heat,yticklabels=True,annot=True)\n",
    "            fig_list += [fig]\n",
    "\n",
    "        if plot_hyperparam_distributions:\n",
    "            breaks = np.arange(0,int(np.ceil(len(ind)/n_per_plot)))\n",
    "            fig,ax = easy_subplots(breaks,dpi=self.dpi)\n",
    "            for a,b in zip(ax,breaks):\n",
    "                best_hyperparam.iloc[b*n_per_plot:(b+1)*n_per_plot].T.plot(kind='kde',bw_method=0.1,ax=a)\n",
    "                a.legend(fontsize=16)\n",
    "            fig_list += [fig]\n",
    "\n",
    "        if plot_hyperparam_vs_error:\n",
    "            fig,ax=easy_subplots(self.hyperparams_changing,dpi=self.dpi)\n",
    "            for a,i in zip(ax,self.hyperparams_changing):\n",
    "                if flip_yx:\n",
    "                    v = self.hyperparam.sort_values(by=i,axis=1).T.reset_index(drop=True).T\n",
    "                else:\n",
    "                    v = self.hyperparam.sort_values(by=norm_sum,axis=1).T.reset_index(drop=True).T\n",
    "                x = v.copy().loc[i]\n",
    "                y = v.copy().loc[norm_sum]\n",
    "                y = y.where(y<y.quantile(0.7)).dropna()\n",
    "\n",
    "                if flip_yx:\n",
    "                    a.plot(x[y.index],y)\n",
    "                    a.set(title=i,xlabel='Hyperparameter value',ylabel='Normalized error')\n",
    "                else:\n",
    "                    a.plot(y,x[y.index])\n",
    "                    a.set(title=i,ylabel='Hyperparameter value',xlabel='Normalized error')\n",
    "\n",
    "            fig.tight_layout()\n",
    "            fig_list += [fig]\n",
    "\n",
    "        if plot_best_params:\n",
    "            fig = self.plot_best_scenario_sd(include_sd=include_sd, plot_supply_demand_stack=plot_supply_demand_stack, best=best_ind)\n",
    "            fig_list += [fig]\n",
    "        return fig_list\n",
    "\n",
    "    def plot_demand_results(self, mining=False, n_best=25):\n",
    "        '''\n",
    "        plots the demand results, with historical and best-fit scenarios using thicker lines and all others semi-transparent thin lines\n",
    "        '''\n",
    "        if self.demand_flag:\n",
    "            historical_demand = self.historical_data['Total demand']\n",
    "            simulated_demand = self.simulated_demand.copy()\n",
    "        else:\n",
    "            historical_demand = self.historical_data['Primary supply']\n",
    "            simulated_demand = self.mine_supply.copy()\n",
    "        simulated_demand, historical_demand, unit = self.get_unit(simulated_demand, historical_demand, 'Demand')\n",
    "        rmse = (simulated_demand.subtract(historical_demand,axis=0)**2).sum()**0.5\n",
    "        best = simulated_demand[rmse.idxmin()]\n",
    "        best_n = simulated_demand.loc[:,rmse.sort_values().tail(n_best).index]\n",
    "        fig,ax = easy_subplots(3,dpi=self.dpi)\n",
    "        best_n.plot(ax=ax[0],linewidth=1,alpha=0.5,legend=False)\n",
    "        best.plot(ax=ax[0],linewidth=4,color='blue')\n",
    "        historical_demand.plot(ax=ax[0],linewidth=4,color='k').grid(axis='x')\n",
    "        material = self.filename.split('_')[0]\n",
    "        material = material if '/' not in material else material.split('/')[1]\n",
    "\n",
    "        if self.demand_flag:\n",
    "            ax[0].set(title='Historical '+material+' demand',xlabel='Years',ylabel=f'{material} demand ({unit})'.capitalize())\n",
    "        else:\n",
    "            ax[0].set(title='Historical '+material+' mine production',xlabel='Years',ylabel=f'{material} production ({unit})'.capitalize())\n",
    "        custom_lines = [Line2D([0], [0], color='k', lw=4),\n",
    "                        Line2D([0], [0], color='blue', lw=4)]\n",
    "        ax[0].legend(custom_lines,['Historical','Simulated'],loc='upper left')\n",
    "        best.name = 'Simulated'\n",
    "        best.plot(ax=ax[1],linewidth=4,color='blue',label='Simulated')\n",
    "        historical_demand.loc[best.index].plot(ax=ax[1],linewidth=4,color='k',label='Historical').grid(axis='x')\n",
    "        ax[1].legend()\n",
    "        if self.demand_flag:\n",
    "            ax[1].set(title=f'Historical {material} demand',xlabel='Years',ylabel=f'{material} demand ({unit})'.capitalize())\n",
    "        else:\n",
    "            ax[1].set(title=f'Historical {material} mine production',xlabel='Years',ylabel=f'{material} production ({unit})'.capitalize())\n",
    "\n",
    "        do_a_regress(best.astype(float),historical_demand.loc[best.index].astype(float),ax=ax[2])\n",
    "\n",
    "    def get_unit(self, simulated_demand, historical_demand, param):\n",
    "        if 'price' in param.lower():\n",
    "            unit = 'USD/t'\n",
    "        else:\n",
    "            min_simulated = abs(simulated_demand).min() if len(simulated_demand.shape)<=1 else abs(simulated_demand).min().min()\n",
    "            max_simulated = abs(simulated_demand).max() if len(simulated_demand.shape)<=1 else abs(simulated_demand).max().max()\n",
    "            mean_simulated = abs(simulated_demand).mean() if len(simulated_demand.shape)<=1 else abs(simulated_demand).mean().mean()\n",
    "            if np.mean([historical_demand.mean(),mean_simulated])>1000:\n",
    "                historical_demand /= 1000\n",
    "                simulated_demand /= 1000\n",
    "                unit = 'Mt'\n",
    "            elif np.mean([historical_demand.mean(),mean_simulated])<1:\n",
    "                historical_demand *= 1000\n",
    "                simulated_demand *= 1000\n",
    "                unit = 't'\n",
    "            else:\n",
    "                unit = 'kt'\n",
    "        return simulated_demand, historical_demand, unit\n",
    "\n",
    "    def get_unit_df(self, res):\n",
    "        maxx, minn, mean = abs(res).max().max(), abs(res).min().min(), abs(res).mean().mean()\n",
    "        if np.any([i in res.columns[0].lower() for i in ['price','cost','tcrc','spread']]):\n",
    "            unit = ' (USD/t)'\n",
    "        elif 'CU' in res.columns[0] or 'SR' in res.columns[0]:\n",
    "            unit = ''\n",
    "        elif 'grade' in res.columns[0]:\n",
    "            unit = ' (%)'\n",
    "        elif mean>1000:\n",
    "            res/=1000\n",
    "            unit=' (Mt)'\n",
    "        elif mean<1:\n",
    "            res*=1000\n",
    "            unit=' (t)'\n",
    "        else:\n",
    "            unit=' (kt)'\n",
    "        return res, unit\n",
    "\n",
    "    def plot_best_scenario_sd(self, include_sd=False, plot_supply_demand_stack=True, best=-1):\n",
    "        self.normalize_rmses()\n",
    "        norm_sum = 'NORM SUM' if include_sd else 'NORM SUM OBJ ONLY'\n",
    "        results = self.results.loc[best].copy().dropna(how='all')\n",
    "        variables = ['Total','Conc','Ref.','Scrap','Spread','TCRC','Refined','CU','SR','Direct','Mean total','mine grade','Conc. SD','Ref. SD','Scrap SD']\n",
    "        fig,ax=easy_subplots(variables)\n",
    "        for var,a in zip(variables,ax):\n",
    "            parameters = [i for i in results.columns if var in i and ('SD' not in i or 'SD' in var)]\n",
    "            res = results[parameters].loc[2001:]\n",
    "            res, unit = self.get_unit_df(res)\n",
    "            param_str = ', '.join(parameters) if len(', '.join(parameters))<30 else ',\\n'.join(parameters)\n",
    "            res.plot(ax=a,title=param_str+unit)\n",
    "        fig.tight_layout()\n",
    "\n",
    "\n",
    "        if plot_supply_demand_stack:\n",
    "            fig1 = plt.figure()\n",
    "            res = results[['Pri. ref. prod.','Sec. ref. prod.','Direct melt','Total demand']].loc[2001:]\n",
    "            res = res.replace(0,np.nan)\n",
    "            res, unit = self.get_unit_df(res)\n",
    "            res = res.fillna(0)\n",
    "            plt.plot(res['Total demand'],label='Total demand',color='k')\n",
    "            plt.stackplot(res.index, res[['Pri. ref. prod.','Sec. ref. prod.','Direct melt']].T, labels=['Pri. ref.','Sec. ref.','Direct melt']);\n",
    "            plt.legend(framealpha=0.5,frameon=True);\n",
    "            plt.title('Total supply-demand imbalance'+unit)\n",
    "        return fig, fig1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9468c2",
   "metadata": {},
   "source": [
    "## Aluminum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1313ee2e",
   "metadata": {},
   "source": [
    "Demand check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bd00ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T16:50:28.528756Z",
     "start_time": "2022-07-07T16:50:28.177061Z"
    }
   },
   "outputs": [],
   "source": [
    "indiv = Individual(filename='data/Historical tuning/silver_run_hist_mcpe0_mining.pkl',rmse_not_mae=False,dpi=50)\n",
    "indiv.plot_demand_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4a4d0-2c28-497c-b3ab-d1f7769a99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv.big_df.loc['rmse_df'].iloc[-1]\n",
    "indiv.rmse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5fdf27",
   "metadata": {},
   "source": [
    "Bigger check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c90a50-8fd5-4f04-a399-ce99dd665b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indiv.historical_data['Primary commodity price'].plot()\n",
    "pd.read_excel('generalization/data/price adjustment results.xlsx',index_col=0)['log(Aluminum)'].sort_index().rolling(5,center=True,min_periods=1).mean().plot()\n",
    "s.mod.hyperparam.loc['primary_commodity_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b320ab-be2e-494b-82ef-1f9bbb369426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c67bad-5c3b-4b18-9afe-33a545d4bb20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194a764c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T00:49:20.596420Z",
     "start_time": "2022-07-26T00:49:17.371742Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_price_list = [1,2,100,0.1,-10]\n",
    "weight_price_list = [1]\n",
    "ready_commodities = ['Steel','Al','Au','Sn','Cu','Ni','Ag','Zn','Pb']\n",
    "ready_commodities = ['Sn']\n",
    "element_commodity_map = {'Steel':'Steel','Al':'Aluminum','Au':'Gold','Cu':'Copper','Steel':'Steel','Co':'Cobalt','REEs':'REEs','W':'Tungsten','Sn':'Tin','Ta':'Tantalum','Ni':'Nickel','Ag':'Silver','Zn':'Zinc','Pb':'Lead','Mo':'Molybdenum','Pt':'Platinum','Te':'Telllurium','Li':'Lithium'}\n",
    "commodity_element_map = dict(zip(element_commodity_map.values(),element_commodity_map.keys()))\n",
    "for el in ready_commodities:\n",
    "    commodity = element_commodity_map[el]\n",
    "    for weight_price in weight_price_list:\n",
    "        try:\n",
    "            indiv = Individual(el,3,filename=f'data/{commodity.lower()}_run_hist_all_3p.pkl',\n",
    "                               rmse_not_mae=True,weight_price=weight_price,dpi=50,price_rolling=5)\n",
    "            fig_list = indiv.plot_results(plot_over_time=True, include_sd=False, nth_best=1,\n",
    "                               plot_sd_over_time=False,\n",
    "                               plot_best_indiv_over_time=False,\n",
    "                               plot_hyperparam_heatmap=False, n_best=20,\n",
    "                               plot_hyperparam_distributions=False, n_per_plot=4,\n",
    "                               plot_hyperparam_vs_error=False,flip_yx=False,\n",
    "                               plot_best_params=False,\n",
    "                               plot_supply_demand_stack=False,\n",
    "                               )\n",
    "            fig_list[0].suptitle(f'{commodity}, {weight_price}',weight='bold',y=1.02,x=0.515)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a46c99-c9db-4c67-9073-52a87325f16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = Individual('Al',3,filename='data/aluminum_run_hist_newdemand_test8_3p.pkl',\n",
    "                   rmse_not_mae=True,weight_price=1,dpi=50,price_rolling=5)\n",
    "# indiv.plot_best_all()\n",
    "# indiv.find_pareto(plot=True,log=True,plot_non_pareto=False)\n",
    "indiv.plot_results(plot_over_time=True, include_sd=False,\n",
    "                   plot_best_indiv_over_time=True,\n",
    "                   plot_hyperparam_heatmap=False, n_best=20,\n",
    "                   plot_hyperparam_distributions=False, n_per_plot=4,\n",
    "                   plot_hyperparam_vs_error=False,flip_yx=False,\n",
    "                   plot_best_params=True,\n",
    "                   plot_supply_demand_stack=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d277ca-3fa2-438b-8c9d-58e6c80aaddf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_df = pd.read_pickle(f'data/tuned_rmse_df_out.pkl')\n",
    "rmse_df.loc['aluminum'].sort_values(by='score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d00cd-3935-43a5-bfe2-986590164327",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_df.loc['aluminum'].sort_values(by='score',axis=1).loc[rmse_df.index.get_level_values(1).unique().isin(indiv.hyperparam.index)].sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9256f8a6-cad4-4c10-83e7-081752df14d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "indiv = Individual('Al',3,filename='data/aluminum_run_scenario_set_alt_act0.pkl',\n",
    "                   rmse_not_mae=True,weight_price=1,dpi=50,price_rolling=5)\n",
    "indiv.hyperparam.loc[indiv.hyperparam.index.isin(rmse_df.index.get_level_values(1).unique())].sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa0d87-25fe-4b70-a109-d6178443935c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T00:49:20.596420Z",
     "start_time": "2022-07-26T00:49:17.371742Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "indiv = Individual('Au',3,filename='data/Historical tuning/gold_run_hist_all_mcpe0_limit.pkl',\n",
    "                   rmse_not_mae=True,weight_price=1,dpi=50,price_rolling=5)\n",
    "# indiv.plot_best_all()\n",
    "# indiv.find_pareto(plot=True,log=True,plot_non_pareto=False)\n",
    "indiv.plot_results(plot_over_time=True, include_sd=False,\n",
    "                   plot_best_indiv_over_time=True,\n",
    "                   plot_hyperparam_heatmap=False, n_best=20,\n",
    "                   plot_hyperparam_distributions=False, n_per_plot=4,\n",
    "                   plot_hyperparam_vs_error=False,flip_yx=False,\n",
    "                   plot_best_params=True,\n",
    "                   plot_supply_demand_stack=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39acfe65",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T18:27:18.876410Z",
     "start_time": "2022-07-06T18:27:18.729474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "indiv = Individual('Al',3,filename='data/aluminum_run_hist_newdemand_test7_3p.pkl',\n",
    "                   rmse_not_mae=True,weight_price=1,dpi=50,price_rolling=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdf9a5-e077-42f4-b238-b64805acf069",
   "metadata": {},
   "outputs": [],
   "source": [
    "intit = indiv.hyperparam.loc[['Total production, Global','initial_demand'],0]\n",
    "og_hyperparam = pd.read_excel('generalization/data/case study data.xlsx',index_col=0)['Al']\n",
    "indiv.historical_data['Primary supply'][2001]*og_hyperparam['Total production, Global']/og_hyperparam['primary_production'],intit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad5cc9-1e69-443c-9fa6-18a173f67ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "intit = indiv.hyperparam.loc[['Total production, Global','initial_demand'],0]\n",
    "intit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac572164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-06T13:00:41.333695Z",
     "start_time": "2022-07-06T13:00:39.793318Z"
    }
   },
   "outputs": [],
   "source": [
    "indiv.hyperparam.tail(20)\n",
    "cols = indiv.hyperparam.loc['RMSE Primary commodity price'].sort_values().head(10).index\n",
    "indiv.hyperparam.loc[['RMSE Primary commodity price','RMSE Total demand','RMSE Primary supply'],cols]\n",
    "indiv.results.loc[idx[cols,:],['Total demand','Conc. supply','Refined price']]\n",
    "fig,ax = easy_subplots(indiv.objective_params)\n",
    "for i,a in zip(indiv.objective_params,ax):\n",
    "    j = indiv.objective_results_map[i]\n",
    "    indiv.results.loc[idx[cols,:],j].unstack(0).loc[2001:].plot(ax=a)\n",
    "    indiv.historical_data[i].loc[2001:].plot(ax=a,color='k')\n",
    "    a.set(title=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b2f5d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T21:38:34.349529Z",
     "start_time": "2022-07-20T21:38:32.460851Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd2f401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-20T21:58:03.462864Z",
     "start_time": "2022-07-20T21:58:03.299991Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    (indiv.results.loc[87]['Refined price'].dropna().pct_change()+1)**indiv.hyperparam[87]['tcrc_elas_price'],\n",
    "    (indiv.results.loc[87]['Conc. supply']/indiv.results.loc[87]['Conc. demand']).dropna()**indiv.hyperparam[87]['tcrc_elas_sd']\n",
    "],axis=1).loc[2001:2019].product(axis=1).plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8521529-6bdb-465e-b3fc-f1813c5fbb97",
   "metadata": {},
   "source": [
    "## Steel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d70e2c-1366-40f0-9d82-59a5eabbd2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = Individual('Steel',3,filename='data/steel_run_hist_all_3p.pkl',\n",
    "                   rmse_not_mae=True,weight_price=1,dpi=50,price_rolling=5)\n",
    "# indiv.plot_best_all()\n",
    "# indiv.find_pareto(plot=True,log=True,plot_non_pareto=False)\n",
    "indiv.plot_results(plot_over_time=True, include_sd=False,\n",
    "                   plot_best_indiv_over_time=True,\n",
    "                   plot_hyperparam_heatmap=True, n_best=20,\n",
    "                   plot_hyperparam_distributions=True, n_per_plot=4,\n",
    "                   plot_hyperparam_vs_error=True,flip_yx=False,\n",
    "                   plot_best_params=True,\n",
    "                   plot_supply_demand_stack=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d58ce",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c5ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T14:10:24.561432Z",
     "start_time": "2022-08-01T14:09:52.865288Z"
    }
   },
   "outputs": [],
   "source": [
    "# indiv = Individual('Au',3,filename='data/gold_run_hist_newdemand_nosd_3p.pkl',\n",
    "#                    rmse_not_mae=True,weight_price=1,dpi=50)\n",
    "indiv = Individual('Au',3,filename='data/gold_run_hist_newdemand_test7_3p.pkl',\n",
    "                   rmse_not_mae=True,weight_price=1,dpi=50,price_rolling=5)\n",
    "# indiv.plot_best_all()\n",
    "# indiv.find_pareto(plot=True,log=True,plot_non_pareto=False)\n",
    "indiv.plot_results(plot_over_time=True, include_sd=False,\n",
    "                   plot_best_indiv_over_time=True,\n",
    "                   plot_hyperparam_heatmap=False, n_best=20,\n",
    "                   plot_hyperparam_distributions=False, n_per_plot=4,\n",
    "                   plot_hyperparam_vs_error=False,flip_yx=False,\n",
    "                   plot_best_params=True,\n",
    "                   plot_supply_demand_stack=True,\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5a6857-61c2-497a-b973-25439cf52f41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6fd1d7-8d2b-45b3-a9a7-334ed9f2c7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv.hyperparam.loc[[i for i in indiv.hyperparam.index if 'elas' in i]][23]\n",
    "# indiv.hyperparam.loc['primary_production',0],indiv.historical_data['Primary supply'][2001],\\\n",
    "# indiv.results[['Primary supply','Mine production','Conc. supply','Conc. demand']].loc[0].loc[2001]\n",
    "# # indiv.results.columns\n",
    "# indiv.results.loc[0].loc[2001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6058cb39",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-01T14:11:29.844780Z",
     "start_time": "2022-08-01T14:11:23.880193Z"
    }
   },
   "outputs": [],
   "source": [
    "indiv.results['Mine production'].dropna().unstack(0).loc[2001]\n",
    "# indiv.hyperparam.loc['primary_production']\n",
    "indiv.hyperparam.loc[[i for i in indiv.hyperparam.index if 'RMSE' in i]]\n",
    "indiv.plot_best_scenario_sd(best=27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b356912-dc6c-4150-83ee-776931a17051",
   "metadata": {},
   "outputs": [],
   "source": [
    "shist1.mod.new_demand.groupby(level=1,axis=1).sum().loc[2001:].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b3a4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:00:58.468681Z",
     "start_time": "2022-07-21T18:00:58.107648Z"
    }
   },
   "outputs": [],
   "source": [
    "rmses = indiv.hyperparam.loc['RMSE Total demand':'RMSE Primary supply'].index\n",
    "fig,ax=easy_subplots(rmses)\n",
    "for i,a in zip(rmses,ax):\n",
    "    indiv.hyperparam.loc[i].plot(ax=a,title=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5e84f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-21T18:52:12.897777Z",
     "start_time": "2022-07-21T18:52:10.225220Z"
    }
   },
   "outputs": [],
   "source": [
    "results = indiv.results.loc[175].copy()\n",
    "variables = ['Total','Conc','Ref.','Scrap','Spread','TCRC','Refined','CU','SR','Direct','Mean total','mine grade']\n",
    "fig,ax=easy_subplots(variables)\n",
    "for var,a in zip(variables,ax):\n",
    "    results[[i for i in results.columns if var in i]].plot(ax=a)\n",
    "    a.set(xlim=(2001,a.get_xlim()[1]))\n",
    "plt.figure()\n",
    "plt.plot(results['Total demand'],label='Total demand',color='k')\n",
    "plt.stackplot(results.index, results[['Pri. ref. prod.','Sec. ref. prod.','Direct melt']].T, labels=['Pri. ref.','Sec. ref.','Direct melt']);\n",
    "plt.xlim(2001,plt.xlim()[1])\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37077bcc-c939-4ee6-b639-0d82e85c8d51",
   "metadata": {},
   "source": [
    "# Doing all commodities - tuning and interpreting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3939ccc4-85d9-4227-bbf6-231664d4d019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "idx = pd.IndexSlice\n",
    "from matplotlib import pyplot as plt\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from modules.integration import Integration\n",
    "from modules.useful_functions import *\n",
    "from modules.integration_functions import *\n",
    "from scipy import stats\n",
    "from random import seed, sample, shuffle\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import seaborn as sns\n",
    "from modules.Individual import *\n",
    "from modules.Many import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from matplotlib.lines import Line2D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# import shap\n",
    "import fasttreeshap as shap # works exactly the same for the TreeExplainer function, should be faster\n",
    "from itertools import permutations,combinations\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7da834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "many_sg = Many()\n",
    "many_sg.load_data('2023-01-17 15_28_13_0_split_grades')\n",
    "\n",
    "many_omt = Many()\n",
    "many_omt.load_data('2023-01-20 11_36_19_0_run_hist_all_one_more_time')\n",
    "\n",
    "many_15 = Many()\n",
    "many_15.load_data('2023-01-17 15_29_01_3_split_2015')\n",
    "\n",
    "many_16 = Many()\n",
    "many_16.load_data('2023-01-17 15_29_04_4_split_2016')\n",
    "\n",
    "many_17 = Many()\n",
    "many_17.load_data('2023-01-17 15_28_21_4_split_2017')\n",
    "\n",
    "many_b = Many()\n",
    "many_b.load_data('2023-01-18 09_59_28_1_baselines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_b.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac296c4a-4392-49d6-8824-e5d047f37794",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "many_f=Many()\n",
    "many_f.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_act0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d89dfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for mat in ['aluminum','copper','zinc','lead','nickel','gold','silver','tin']:\n",
    "    for n in [0,1]:\n",
    "        filename=f'data/Simulation/{mat}_run_scenario_baselines{n}.pkl'\n",
    "        if os.path.exists(filename):\n",
    "            x = pd.read_pickle(filename)\n",
    "            x.loc['version',:]='1.0'\n",
    "            x.loc['version']\n",
    "            x.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ff93e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many_b=Many()\n",
    "# many_b.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_baselines',\n",
    "#                                 commodities=['Al','Cu','Au','Ag','Zn','Ni','Sn','Pb'])\n",
    "\n",
    "many_b.load_future_scenario_runs(verbosity=2, scenario_name_base='_baselines',\n",
    "                                commodities=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d4ccf-98a8-403a-8db1-233958f7d6ca",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many_b\n",
    "# big_df = pd.read_pickle('data/Simulation/copper_run_scenario_baselines0.pkl')\n",
    "# big_df.loc['mine_data']\n",
    "# many_mcpe0.integ.indiv_copper.mine_data\n",
    "# results_sorted.loc['copper'].loc[idx[:25,:],'Mine production'].unstack(0).plot()\n",
    "\n",
    "\n",
    "plot_best_scenario_sd(many_mcpe0.integ,'gold',best=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809de5cf-e880-4c39-aedb-33acf2c2d548",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# many_norm=Many()\n",
    "# many_norm.get_multiple(mining=True, demand=True, integ=True, filename_modifier='_norm', \n",
    "#                        noninteg_modifier='_constrain')\n",
    "# # many_norm.integ.rmse_df.to_pickle('data/tuned_rmse_df_out.pkl')\n",
    "\n",
    "# many_test=Many()\n",
    "# many_test.get_multiple(mining=True, demand=True, integ=True, filename_modifier='_norm_test',\n",
    "                       # noninteg_modifier='_constrain')\n",
    "# many_test.integ.rmse_df.to_pickle('data/tuned_rmse_df_out_test.pkl')\n",
    "\n",
    "# many_test16=Many()\n",
    "# many_test16.get_multiple(mining=True, demand=True, integ=True, filename_modifier='_norm_test2015',\n",
    "#                        noninteg_modifier='_constrain')\n",
    "# many_test.integ.rmse_df.to_pickle('data/tuned_rmse_df_out_test.pkl')\n",
    "\n",
    "# many_noprice=Many()\n",
    "# many_noprice.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_norm_noprice',\n",
    "#                        noninteg_modifier='_constrain')\n",
    "\n",
    "# many_unconstrain=Many()\n",
    "# many_unconstrain.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_norm_unconstrain',\n",
    "                       # noninteg_modifier='_constrain')\n",
    "\n",
    "# many_mcpe0=Many()\n",
    "# many_mcpe0.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_mcpe0',\n",
    "#                        noninteg_modifier='_constrain_mcpe0')\n",
    "\n",
    "# many_rerun=Many()\n",
    "# many_rerun.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_rerun',\n",
    "#                         tuned_rmse_df_out_append='_mcpe0',\n",
    "#                        noninteg_modifier='_constrain_mcpe0',commodities=['Cu'])\n",
    "\n",
    "# many=Many()\n",
    "# many.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_rerun_overhead',\n",
    "#                         tuned_rmse_df_out_append='_rerun_overhead',\n",
    "#                        noninteg_modifier='_constrain_mcpe0',commodities=None)\n",
    "\n",
    "# many_sg=Many()\n",
    "# many_sg.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_split_grades',\n",
    "#                         tuned_rmse_df_out_append='_split_grades',\n",
    "#                        noninteg_modifier='_constrain_mcpe0',commodities=None)\n",
    "\n",
    "many_15=Many()\n",
    "many_15.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_split_2015',\n",
    "                        tuned_rmse_df_out_append='_split_2015',\n",
    "                       noninteg_modifier='_constrain_mcpe0',commodities=None)\n",
    "# many_16=Many()\n",
    "# many_16.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_split_2016',\n",
    "#                         tuned_rmse_df_out_append='_split_2016',\n",
    "#                        noninteg_modifier='_constrain_mcpe0',commodities=None)\n",
    "# many_17=Many()\n",
    "# many_17.get_multiple(mining=False, demand=False, integ=True, filename_modifier='_split_2017',\n",
    "#                         tuned_rmse_df_out_append='_split_2017',\n",
    "#                        noninteg_modifier='_constrain_mcpe0',commodities=None)\n",
    "\n",
    "# many_histprice=Many()\n",
    "# many_histprice.get_multiple(mining=True, demand=True, integ=True, filename_modifier='_mcpe0_histprice',\n",
    "#                        noninteg_modifier='_constrain_mcpe0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a3e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dcb444-5fd8-4b0e-bebe-cfa14ddcdae6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "many_noprice.pkl_folder='data/Historical tuning'\n",
    "many_noprice.integ.plot_all_integration(dpi=60, filename_modifier='_norm_noprice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e7f16-3a33-45d1-b58a-adb96596f69f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "many = many_histprice\n",
    "many = many_noprice\n",
    "many = many_mcpe0\n",
    "commodity = 'aluminum'\n",
    "for element in many.ready_commodities:\n",
    "    commodity = many.element_commodity_map[element].lower()\n",
    "    print(commodity)\n",
    "    best = many.integ.rmse_df.loc[commodity].loc['score'].sort_values().head(25).index\n",
    "    fig,ax = plot_given_columns(many.integ, commodity, columns=best, end_year=2040)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d2989-30c8-41b5-9be1-231748956c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_mcpe0.integ.tuned_rmse_df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c413f97-a7a4-404b-8561-849989375792",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "many_histprice.mining.rmse_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32267c-fd5b-4eb8-be07-95ae4c350fe8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_norm_noprice_tune(many_norm, many_noprice, commodity, n_best=25, alpha=0.4, plot=True, label1='Model 1', label2='Model 2'):\n",
    "    norm_rmse = many_norm.integ.tuned_rmse_df_out.copy().loc[commodity].T\n",
    "    noprice_rmse = many_noprice.integ.tuned_rmse_df_out.copy().loc[commodity].T\n",
    "    \n",
    "    norm_rmse_best = norm_rmse.sort_values(by='score',axis=1).T.head(n_best).index\n",
    "    noprice_rmse_best = noprice_rmse.sort_values(by='score',axis=1).T.head(n_best).index\n",
    "    norm_rmse = norm_rmse.loc[:,norm_rmse_best]\n",
    "    noprice_rmse = noprice_rmse.loc[:,noprice_rmse_best]\n",
    "\n",
    "    demand1 = many_norm.demand.rmse_df.copy().loc[commodity]\n",
    "    demand2 = many_noprice.demand.rmse_df.copy().loc[commodity]\n",
    "    mining1 = many_norm.mining.rmse_df.copy().loc[commodity]\n",
    "    mining2 = many_noprice.mining.rmse_df.copy().loc[commodity]\n",
    "    demand1,demand2 = [i.loc[:,i.loc['RMSE'].sort_values().head(n_best).index] for i in [demand1,demand2]]\n",
    "    mining1,mining2 = [i.loc[:,i.loc['score'].sort_values().head(n_best).index] for i in [mining1,mining2]]\n",
    "    \n",
    "    r = [r for r in noprice_rmse.index if np.any([j in r for j in \n",
    "                                ['R2','RMSE','region_specific_price_response','score','Region specific']])]\n",
    "    noprice_rmse.drop(r,inplace=True)\n",
    "    r = [r for r in norm_rmse.index if np.any([j in r for j in \n",
    "                                ['R2','RMSE','region_specific_price_response','score','Region specific']])]\n",
    "    norm_rmse.drop(r,inplace=True)\n",
    "    inds = np.union1d(norm_rmse.index,noprice_rmse.index)\n",
    "    if plot:\n",
    "        fig,ax = easy_subplots(inds)\n",
    "    else:\n",
    "        fig,ax = 0,norm_rmse.index\n",
    "    n_stat_sig = 0\n",
    "    for k,a in zip(inds,ax):\n",
    "        if plot:\n",
    "            if k in norm_rmse.index:\n",
    "                a.hist(norm_rmse.loc[k], alpha=alpha, label=label1)\n",
    "            if k in noprice_rmse.index:\n",
    "                a.hist(noprice_rmse.loc[k,noprice_rmse_best], alpha=alpha, label=label2)\n",
    "        if (k in noprice_rmse.index and k in norm_rmse.index) and (\n",
    "            len(norm_rmse.loc[k,norm_rmse_best].value_counts())>2 and \n",
    "            len(noprice_rmse.loc[k,noprice_rmse_best].value_counts())>2):\n",
    "                pval = stats.kruskal(norm_rmse.loc[k,norm_rmse_best],noprice_rmse.loc[k,noprice_rmse_best])[1]\n",
    "        else: pval=1\n",
    "        if pval<0.05: n_stat_sig+=1\n",
    "        pval = round(pval,2)\n",
    "        if plot:\n",
    "            a.set(title=f'{k}\\ndifference pval: {pval}')\n",
    "            a.legend()\n",
    "    if plot: fig.tight_layout()\n",
    "    percentage_stat_sig = round(n_stat_sig/len(norm_rmse.index)*100,1)\n",
    "    print(f'{commodity.capitalize()}: {n_stat_sig}/{len(norm_rmse.index)} ({percentage_stat_sig}%) are significantly different')\n",
    "    if plot:\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "commodity='aluminum'\n",
    "compare_norm_noprice_tune(many_mcpe0, many_histprice, commodity, n_best=25, alpha=0.4, plot=True)\n",
    "\n",
    "# for commodity in many_norm.integ.rmse_df.index.get_level_values(0).unique():\n",
    "#     compare_norm_noprice_tune(many_norm, many_noprice, commodity, n_best=25, alpha=0.4, plot=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bb74f-263e-4dc2-9da4-30f3b3197ee0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "many = many_histprice\n",
    "commodity = 'lead'\n",
    "\n",
    "# def add_primary_commodity_price_elas_sd(self):\n",
    "    # if 'limit' in self.filename_modifier:\n",
    "self = many.integ.indiv_aluminum\n",
    "def get_regression_results(price, sd):\n",
    "    price=price.copy()\n",
    "    sd = sd.copy()\n",
    "    sd.name = 'SD'\n",
    "    m = sm.GLS(np.log(price), sm.add_constant(np.log(sd))).fit(cov_type='HC3')\n",
    "    return pd.Series([m.params['SD'], round(m.pvalues['SD'],6), m.rsquared],['coefficient','pvalue','rsquared'])\n",
    "\n",
    "\n",
    "regress_results = pd.DataFrame()\n",
    "res = self.results.copy()\n",
    "sd = (res['Ref. supply']/res['Ref. demand']).unstack(0).shift(1).dropna()\n",
    "price = res['Refined price'].unstack(0).loc[sd.index]\n",
    "regress_results = sd.apply(lambda x: get_regression_results(price[0],x),axis=0)\n",
    "if 0 in regress_results.columns:\n",
    "    regress_results = regress_results.loc[:,1:]\n",
    "\n",
    "rename_dict={'coefficient':'primary_commodity_price_elas_sd',\n",
    "             'pvalue':'primary_price_pval_score',\n",
    "             'rsquared':'primary_price_R2_score'}\n",
    "rmse_df = self.rmse_df.copy()\n",
    "for k in rename_dict.values():\n",
    "    if k in self.rmse_df.index:\n",
    "        self.rmse_df.drop(k,inplace=True)\n",
    "rmse_df = pd.concat([\n",
    "    rmse_df,\n",
    "    regress_results.rename(rename_dict)]).sort_index().T.sort_index().T\n",
    "# tuned_rmse_df_out = pd.concat([\n",
    "#     self.tuned_rmse_df_out,\n",
    "#     regress_results.loc[idx[:,:],1:].rename(rename_dict,level=1).stack().unstack(1)],axis=1).sort_index().T.sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417680ef",
   "metadata": {},
   "source": [
    "## Checking on retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070ffc95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commodities = ['Cu','Ni','Pb','Zn','Au','Ag','Sn','Al','Steel']\n",
    "# commodities = ['Au']\n",
    "many=Many()\n",
    "many.get_variables('all',filename_modifier='_rerun_overhead',tuned_rmse_df_out_append='_rerun_overhead',\n",
    "                   commodities=commodities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179abab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.results_sorted['Mean mine grade'].unstack().loc['gold'].loc[0:10].loc[:,2001:].T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba689bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.results['Mine production'].loc['copper'].loc[173].loc[:2019].plot()\n",
    "fig,ax=plt.subplots()\n",
    "many.historical_data.loc['copper'][['Primary commodity price','Original primary commodity price']].plot(ax=ax)\n",
    "many.results['Refined price'].loc['copper'].loc[173].plot(ax=ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f911071",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.historical_data.loc['copper'][['Primary commodity price','Original primary commodity price']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14caca1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfe140",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "commodities=['Cu','Ni','Pb','Zn','Au','Ag','Sn','Al','Steel']\n",
    "# commodities=['Au']\n",
    "commodities=[many.element_commodity_map[i].lower() for i in commodities]\n",
    "for comm in commodities:\n",
    "    best = many.rmse_df.loc[comm].sort_values(by='score',axis=1).iloc[:,:25].columns\n",
    "#     best = many.rmse_df.loc[comm].sort_values(by='Primary supply train RMSE',axis=1).iloc[:,:25].columns\n",
    "#     best = many.rmse_df.loc[comm].sort_values(by='Primary commodity price train RMSE',axis=1).iloc[:,:25].columns\n",
    "    fig,ax=plot_given_columns(many,comm,columns=best)\n",
    "    fig.tight_layout()\n",
    "    fig.set_dpi(55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58253b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.rmse_df_sorted.loc['copper'].loc['mine_cost_change_per_year',:25].plot.hist()\n",
    "print('Mean mine cost change per year:',\n",
    "      many.rmse_df_sorted.loc['copper'].loc['mine_cost_change_per_year',:25].mean())\n",
    "stats.ttest_1samp(many.rmse_df_sorted.loc['copper'].loc['mine_cost_change_per_year',:25],popmean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ef25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.results_sorted.columns#[['Number of mines opening','Number of mines closing']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e5987",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in [10,15,20,25]:\n",
    "    x = many.results_sorted['TCRC'].loc['lead'].loc[idx[:n,2001:]]\n",
    "    print(n,(x>x.median()*10).sum()/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7123ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab54490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 25\n",
    "def plot_future(many, commodity='copper', var='Mean total cash margin', n=50, \n",
    "                color=mpl.color_sequences['Dark2'][0], dpi=50, end_year=2040):\n",
    "    fig, ax = plt.subplots()\n",
    "    data = many.results_sorted[var].loc[commodity.lower()].loc[idx[:n,:end_year]]\n",
    "    if 'cost' not in var and 'margin' not in var and 'grade' not in var:\n",
    "        _ = get_unit(data,data,var)\n",
    "        data = _['simulated']\n",
    "        label= _['unit']\n",
    "    elif 'grade' in var:\n",
    "        label = '%'\n",
    "    else:\n",
    "        label = 'USD/t'\n",
    "    data = data.reset_index()\\\n",
    "        .rename(columns={'level_1':'Year'})\n",
    "    sns.lineplot(data=data, x='Year',y=var, color=color, ax=ax)\n",
    "    title=var.replace('Spread','Scrap spread').replace('Refined price','Cathode price')\\\n",
    "              .replace('Mean ','').replace('mine grade','ore grade')\n",
    "    if title!='TCRC': title=title.capitalize()\n",
    "    ax.set(ylabel=var+f' ({label})', title=title)\n",
    "    fig.set_dpi(dpi)\n",
    "    return fig, ax\n",
    "\n",
    "comm = commodities[8]\n",
    "# comm='gold'\n",
    "print(comm)\n",
    "for i,color in zip(['Mean total cash margin', 'Mean total minesite cost', 'Mean mine grade',\n",
    "          'Refined price','TCRC','Spread','Mine production','Total demand',\n",
    "         ],np.tile(mpl.color_sequences['Dark2'],(2,1))):        \n",
    "    fig, ax = plot_future(many_sg.integ, var=i, color=color, dpi=50, n=20, commodity=comm)\n",
    "\n",
    "# many.results.columns\n",
    "# many.hyperparam_sorted\n",
    "# many.results_sorted['Total demand'].loc['copper'].loc[0]\n",
    "\n",
    "# fig,ax=plt.subplots()\n",
    "# data = many.results_sorted[['Refined price','TCRC','Spread']].loc['copper'].loc[idx[:n,2001:],:]\\\n",
    "#     .stack().reset_index().rename(\n",
    "#     columns={'level_1':'Year','level_2':'Variable',0:'Value'}).dropna()\n",
    "# dataa = data.loc[data['Variable']=='Refined price']\n",
    "# datab = data.loc[data['Variable']!='Refined price']\n",
    "# sns.lineplot(data=dataa, x='Year',y='Value', hue='Variable', ax=ax)\n",
    "# tw = ax.twinx()\n",
    "\n",
    "# sns.lineplot(data=datab, x='Year',y='Value', hue='Variable', ax=tw)\n",
    "# twinx2(ax, tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5ea41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dpi=400\n",
    "# ['Sec. ref. prod.','Pri. ref. prod.','Total demand','Mine production', 'Mean mine grade', \n",
    "#  'Direct melt', 'Scrap supply', 'Conc. demand', 'SX-EW supply', 'TCRC', 'Spread', 'Refined price',\n",
    "#  'Mean total minesite cost', 'Mean total cash margin',\n",
    "# ]\n",
    "fig, axes = easy_subplots(3,width_scale=0.9)\n",
    "# Scrap supply and demand\n",
    "ax=axes[2]\n",
    "var = ['Sec. ref. prod.','Direct melt']\n",
    "data = many.results_sorted.loc['copper'].loc[idx[:n,:],var].dropna().groupby(level=1).mean().div(1e3)\n",
    "var = ['Scrap supply']\n",
    "line = many.results_sorted.loc['copper'].loc[idx[:n,:],var].dropna().groupby(level=1).mean().div(1e3)\n",
    "# fig,ax=plt.subplots(figsize=(5,5.5))\n",
    "ax.stackplot(data.index, data.T, labels=data.columns, colors=['#e6550d','#fd8d3c'])\n",
    "ax.plot(line,color='k',label='Scrap supply')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(reversed(handles), reversed(labels),loc='upper left')\n",
    "ax.set(ylabel='Scrap supply and demand (Mt)', title='Scrap supply and demand',\n",
    "      xlabel='Year')\n",
    "fig.set_dpi(dpi)\n",
    "\n",
    "# Cathode supply and demand\n",
    "ax=axes[0]\n",
    "var = ['Pri. ref. prod.','Sec. ref. prod.','SX-EW supply']\n",
    "data = many.results_sorted.loc['copper'].loc[idx[:n,2001:],var].dropna().groupby(level=1).mean().div(1e3)\n",
    "var = ['Total demand']\n",
    "line1 = many.results_sorted.loc['copper'].loc[idx[:n,2001:],var].dropna().groupby(level=1).mean().div(1e3)\n",
    "var = ['Direct melt']\n",
    "line2 = many.results_sorted.loc['copper'].loc[idx[:n,2001:],var].dropna().groupby(level=1).mean().div(1e3)\n",
    "line = line1.iloc[:,0]-line2.iloc[:,0]\n",
    "# fig,ax=plt.subplots(figsize=(5,5.5))\n",
    "ax.stackplot(data.index, data.T, \n",
    "             labels=['Primary refined\\nproduction','Secondary refined\\nproduction','SX-EW supply'], \n",
    "             colors=['#006d2c','#2ca25f','#4cbb9f'])\n",
    "ax.plot(line,color='k',label='Cathode demand')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(reversed(handles), reversed(labels),loc='upper left',fontsize=17.5)\n",
    "ax.set(ylabel='Cathode supply and demand (Mt)', title='Cathode supply and demand',\n",
    "      xlabel='Year')\n",
    "fig.set_dpi(dpi)\n",
    "\n",
    "# Concentrate supply and demand\n",
    "ax=axes[1]\n",
    "var = ['Conc. supply','SX-EW supply']\n",
    "data = many.results_sorted.loc['copper'].loc[idx[:n,2001:],var].dropna().groupby(level=1).mean().div(1e3)\n",
    "var = ['Conc. demand']\n",
    "line = many.results_sorted.loc['copper'].loc[idx[:n,2001:],var].dropna().groupby(level=1).mean().div(1e3)\n",
    "# fig,ax=plt.subplots(figsize=(5,5.5))\n",
    "ax.stackplot(data.index, data.T, \n",
    "             labels=['Concentrate supply','SX-EW supply'], \n",
    "             colors=['#00bbcc','#2b8cbe'])\n",
    "ax.plot(line,color='k',label='Concentrate demand')\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(reversed(handles), reversed(labels),loc='upper left',fontsize=17.5)\n",
    "ax.set(ylabel='Concentrate supply and demand (Mt)', title='Concentrate supply and demand',\n",
    "      xlabel='Year')\n",
    "fig.set_dpi(dpi)\n",
    "fig.tight_layout()\n",
    "\n",
    "# Conc. supply-demand as lines\n",
    "# fig,ax=plt.subplots()\n",
    "# data = many.results_sorted[['Mine production','Conc. demand']].loc['copper'].loc[idx[:n,2001:],:]\\\n",
    "#     .div(1e3).stack().reset_index().rename(\n",
    "#     columns={'level_1':'Year','level_2':'Variable',0:'Value'}).dropna()\n",
    "# dataa = data.loc[data['Variable']=='Refined price']\n",
    "# datab = data.loc[data['Variable']!='Refined price']\n",
    "# sns.lineplot(data=data, x='Year',y='Value', hue='Variable',ax=ax)\n",
    "# ax.set(title='Concentrate supply and demand',\n",
    "#        ylabel='Concentrate supply and demand (Mt)',\n",
    "#        xlabel='Year'\n",
    "#       )\n",
    "# fig.set_size_inches(5.5,5.5)\n",
    "# fig.set_dpi(dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc55b978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many.results_sorted['Ref. supply'].loc['copper'].loc[0].loc[2001:]-\\\n",
    "  many.results_sorted['Ref. demand'].loc['copper'].loc[0].loc[2001:]\n",
    "v = many.results_sorted.loc['copper'].loc[0].loc[2001:]\n",
    "pd.concat([\n",
    "    v[['Pri. ref. prod.','Sec. ref. prod.','SX-EW supply']].sum(axis=1),\n",
    "    v['Ref. supply']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6dae94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "integ1 = Integration(data_folder='generalization/data',\n",
    "                    commodity='Cu', \n",
    "                    price_to_use='log', \n",
    "                    simulation_time=np.arange(2001,2041),\n",
    "                    input_hyperparam=many.hyperparam_sorted.loc['copper'][0], \n",
    "                    historical_price_rolling_window=5)\n",
    "integ1.hyperparam.loc['primary_overhead_regression2use','Value']='None'\n",
    "integ1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba76ad9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    integ.total_demand.loc[1981:,'Global'],\n",
    "    many.results_sorted['Total demand'].loc['copper'].loc[0]\n",
    "],axis=1)\n",
    "integ.total_demand.loc[1981:,'Global']-\\\n",
    "    many.results_sorted['Total demand'].loc['copper'].loc[0]\n",
    "ih = integ.hyperparam['Value'].copy()\n",
    "sh = many.hyperparam_sorted.loc['copper'][0].copy()\n",
    "ind = np.intersect1d(ih.index,sh.index)\n",
    "ind = [i for i in ind if type(ih[i]) in [float,int]]\n",
    "(ih.loc[ind]-\\\n",
    "sh.loc[ind]).sum()\n",
    "\n",
    "pd.concat([\n",
    "    integ.mining.supply_series,\n",
    "    integ1.mining.supply_series,\n",
    "    many.results_sorted['Mine production'].loc['copper'].loc[0]\n",
    "],axis=1)\n",
    "\n",
    "# ml = integ1.mining.ml.generate_df()\n",
    "# ml['head_grade_pct'].unstack().mean(axis=1).plot()\n",
    "# ml['opening'].unstack().mean().plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a493555",
   "metadata": {},
   "outputs": [],
   "source": [
    "integ.mining.ml.generate_df()['head_grade_pct'].unstack().mean(axis=1).plot()\n",
    "integ1.mining.ml.generate_df()['head_grade_pct'].unstack().mean(axis=1).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cc3cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.results_sorted['Mean total minesite cost'].loc['copper'].loc[0].plot(title='Mean minesite cost')\n",
    "many.results_sorted.columns\n",
    "# plot_best_scenario_sd(many,commodity='copper',best=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc561a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.hyperparam_sorted.loc['copper'].loc['mine_cost_change_per_year'].head(25).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96ed5d-4923-45ac-98fe-6a9459dc1fef",
   "metadata": {},
   "source": [
    "## Parameter distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce49b91",
   "metadata": {
    "code_folding": [
     1
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_demand_parameter_correlation(many_mcpe0,n=10)\n",
    "fig,ax,df = plot_important_parameter_scatter(many,n=25,n_most_important=5,\n",
    "                                             scale_y_for_legend=1,legend=False,dpi=100,\n",
    "                                             best_or_median='best RMSE',\n",
    "                                             mining_or_integ='demand',\n",
    "                                             scale_fig_height=1.5,\n",
    "                                             split_params=True,\n",
    "                                             normalize=True,\n",
    "                                             x_for_stat_sig=True\n",
    "                                            );\n",
    "# nice_feature_importance_plot(mine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c421511",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_mcpe0.integ.hyperparam.loc[idx[:,'primary_price_resources_contained_elas'],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2f9868-5a82-4bda-b686-da2066d8ff69",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "table,means = make_parameter_mean_std_table(many,25)\n",
    "table.to_excel('table.xlsx')\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa3a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_table = table.replace(dict(zip(table.values.flatten(),\n",
    "                                   [i.replace(' (','\\n(') for i in table.values.flatten()])))\\\n",
    "    .sort_index().T.sort_index().droplevel(0)\n",
    "alt_means = means.div(abs(means).max()).T.droplevel(0)\n",
    "fig,ax = plt.subplots()\n",
    "sns.heatmap(alt_means,\n",
    "            ax=ax,\n",
    "            annot=alt_table,\n",
    "            fmt='s',\n",
    "            annot_kws={'fontsize':17},\n",
    "            xticklabels=True,\n",
    "            yticklabels=True,\n",
    "            cmap='vlag',\n",
    "           )\n",
    "# ax.set_yticks(np.arange(ax.get_yticks()[0],ax.get_yticks()[-1]+1,1))\n",
    "ax.tick_params(labelbottom=True, labeltop=True)\n",
    "fig.set_size_inches(17,17)\n",
    "fig.set_dpi(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ef0fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.tick_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a598915-2443-4fed-8c84-ba71e9e4b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_important_parameter_scatter(many_mcpe0,'mining',n=10,n_most_important=22,split_params=False,\n",
    "                                scale_fig_width=4,legend=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9eba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c673d0",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "df = plot_violin_all(many, n_most_important=[16,25], dpi=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a5036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def checking_parameter_constraints(self, commodity):\n",
    "    indiv = self.integ\n",
    "    varsi = ['sector_specific_dematerialization_tech_growth','sector_specific_price_response',\n",
    "      'region_specific_price_response','intensity_response_to_gdp','primary_commodity_price_elas_sd',\n",
    "             'mine_cost_change_per_year','incentive_mine_cost_change_per_year',\n",
    "       'initial_ore_grade_decline','primary_oge_scale']\n",
    "    varsi = ['sector_specific_dematerialization_tech_growth','intensity_response_to_gdp',\n",
    "             'mine_cost_change_per_year']\n",
    "    def get_values_and_plot(indiv, commodity):\n",
    "        vv = indiv.rmse_df.loc[commodity].sort_values(by='score',axis=1).loc[varsi].T.head(25).stack()\n",
    "        vv = vv.unstack().div(vv.unstack().mean()).stack()\n",
    "        vv.rename(make_parameter_names_nice(\n",
    "            vv.index.get_level_values(1).unique()),level=1,inplace=True)\n",
    "        vv1 = vv.reset_index().rename(columns={'level_0':'scenario','level_1':'parameter',0:'value'})\n",
    "        vv1 = get_values(indiv,commodity)\n",
    "        fig,ax=plt.subplots()\n",
    "        sns.scatterplot(vv1, x='parameter', y='value', ax=ax)\n",
    "        ax.tick_params(axis='x',rotation=90)\n",
    "        ax.set(ylabel='value divided by mean',title='checking new parameter constraints\\ncopper, best 25 fits')\n",
    "    \n",
    "    else:\n",
    "        for commodity in indiv.rmse_df.index.get_level_values(0).unique():\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2dfbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_1samp(vv.unstack()['Intensity decline per year'],popmean=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4292668",
   "metadata": {},
   "outputs": [],
   "source": [
    "for commodity in many_mcpe0.integ.rmse_df.index.get_level_values(0).unique():\n",
    "    if commodity=='copper':\n",
    "        best = many_mcpe0.integ.rmse_df.loc[commodity].loc['score'].sort_values().head(25).index\n",
    "        fig,ax = plot_given_columns(many_mcpe0.integ,commodity,columns=best, end_year=2040)\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee27d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_mcpe0.integ.indiv_copper.mine_data#.unstack().unstack().stack().stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edb875c",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_best_scenario_sd(many_mcpe0.integ,commodity='copper',best=-1, scrap_scenario=693)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b6dfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8e2a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_b.multi_scenario_results.loc['copper']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d747e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_b.multi_scenario_results.loc[idx['lead',:,1,:],'Mine production'].droplevel([0,1,2])\n",
    "base_test = many_b.multi_scenario_hyperparam.loc[idx['lead',:],:].unstack(1).dropna(how='all',axis=1)\\\n",
    "    .droplevel(1,axis=1).droplevel(0)\n",
    "tune_test = many_mcpe0.integ.hyperparam.loc['lead',:]\n",
    "ind = list(np.intersect1d(base_test.index,tune_test.index))\n",
    "ind.remove('simulation_time')\n",
    "base_test,tune_test = base_test.loc[ind],tune_test.loc[ind]\n",
    "check = base_test[1]==tune_test\n",
    "check = check[~check]\n",
    "check.loc[['elas' not in i for i in check.index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45fc7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_test.loc['sector_specific_price_response']\n",
    "sum([i in base_test.loc['sector_specific_price_response'].values \n",
    "     for i in tune_test.loc['sector_specific_price_response'].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce930b08",
   "metadata": {},
   "source": [
    "Make it so we can update parameters in 2020, feed in scenario info more easily, get supply curves,\n",
    "\n",
    "lineplot of parameter distributions so the parameters are on the x axis and value on y, maybe transparency/color weighted by score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdecc7",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "commodity = 'aluminum'\n",
    "n=25\n",
    "x = pd.read_pickle('data/tuned_rmse_df_out.pkl')\n",
    "score = x.loc[idx[commodity,'score'],:]\n",
    "r = [i for i in x.index.get_level_values(1).unique() if np.any([j in i for j in ['R2','RMSE','score']])]\n",
    "x = x.drop(r,level=1)\n",
    "x.index = x.index.set_names([0,commodity])\n",
    "x = x.loc[commodity]\n",
    "cols = score.sort_values().head(n).index\n",
    "fig,ax=plt.subplots()\n",
    "x = x[cols]\n",
    "x.plot(legend=False,ax=ax,alpha=0.3)\n",
    "ax.set_xticks(np.arange(0,x.shape[0]))\n",
    "ax.set_xticklabels(x.index)\n",
    "ax.tick_params(axis='x',rotation=90)\n",
    "\n",
    "x_s = x.stack().reset_index().rename(columns={commodity:'Parameter', 'level_1':'Scenario number', 0:'Value'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fc18a",
   "metadata": {
    "code_folding": [
     0,
     45
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanup_whole(source):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        for i in np.arange(0,source.shape[0]):\n",
    "            for j in np.arange(0,source.shape[1]):\n",
    "                col = source.columns[j]\n",
    "                ix = source[col].iloc[i]\n",
    "                if type(ix)==str:\n",
    "                    ind = source.index[i]\n",
    "                    if ' ' in ix:\n",
    "                        source.loc[(ind[0],ind[1]+'a'),col]=float(ix.split(' ')[0])\n",
    "                        source.loc[(ind[0],ind[1]+'b'),col]=float(ix.split(' ')[-1])\n",
    "                    elif '-' in ix:\n",
    "                        source.loc[(ind[0],ind[1]+'a'),col]=float(ix.split('-')[0])\n",
    "                        source.loc[(ind[0],ind[1]+'b'),col]=float(ix.split('-')[-1])\n",
    "                    source.iloc[i,j]=np.nan\n",
    "        source = source.dropna(how='all')\n",
    "        return source               \n",
    "\n",
    "def load_sources(many):\n",
    "    sources = pd.read_excel('Sources for elasticities.xlsx',index_col=[0,1])\n",
    "    sources = sources.loc[:,'Steel':'Zinc'].dropna(how='all')\n",
    "    sources.rename(dict(zip(sources.index.get_level_values(0).unique(),\n",
    "                            [i.replace('supply-demand imbalance','SD') for i in sources.index.get_level_values(0).unique()])),level=0,inplace=True)\n",
    "    sources_to_rmse = {\n",
    "        'Scrap supply elasticity to secondary price':'Collection elasticity to scrap price',\n",
    "        'Price elasticity to supply-demand imbalance':'Refined price elasticity to SD',\n",
    "        'Ore treated elasticity to total cash margin':'Mine CU elasticity to TCM',\n",
    "        'Primary supply elasticity to price':'Mine CU elasticity to TCM',\n",
    "        'Primary demand elasticity to price':'Demand elasticity to price',\n",
    "        'Ore grade decline per year':'Ore grade elasticity to COT distribution mean',\n",
    "                      }\n",
    "    sources.rename(sources_to_rmse,level=0,inplace=True)\n",
    "    sources = cleanup_whole(sources)\n",
    "    new_ind = []\n",
    "    for e,i in enumerate(sources.index):\n",
    "        if 'Demand' in i[0]:\n",
    "            new_ind += [(i[0].replace('Demand','Intensity'),i[1]+' - demand')]\n",
    "            if 'GDP' in i[0]:\n",
    "                sources.iloc[e] = sources.iloc[e] - 1\n",
    "        else:\n",
    "            new_ind += [i]\n",
    "    sources.index = pd.MultiIndex.from_tuples(new_ind)\n",
    "    many.sources = sources.copy()\n",
    "    \n",
    "def plot_compare_tuning_with_literature(many, commodities=None):\n",
    "    load_sources(many)\n",
    "    sources = many.sources.copy()\n",
    "    if hasattr(many,'integ'):\n",
    "        rmse_df = many.integ.rmse_df_sorted.loc[:,:25].copy()\n",
    "    else:\n",
    "        rmse_df = many.rmse_df_sorted.loc[:,:25].copy()\n",
    "    rmse_df.rename(make_parameter_names_nice(rmse_df.index.get_level_values(1).unique()),level=1,inplace=True)\n",
    "    many.rmse_df_nice = rmse_df.copy()\n",
    "    \n",
    "    if commodities is None:\n",
    "        sources_comm = [i.lower() for i in sources.columns]\n",
    "        commodities = np.intersect1d(rmse_df.index.get_level_values(0).unique(), sources_comm)\n",
    "    \n",
    "    fig_list = []\n",
    "    for commodity in commodities:\n",
    "        fig = plot_compare_tuning_with_literature_one(many, sources, commodity)\n",
    "    \n",
    "def plot_compare_tuning_with_literature_one(many, sources, commodity):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        commodity_s = commodity.capitalize()\n",
    "\n",
    "        if hasattr(many,'integ'):\n",
    "            rmse_df = many.integ.rmse_df_sorted.loc[:,:25].copy()\n",
    "        else:\n",
    "            rmse_df = many.rmse_df_sorted.loc[:,:25].copy()\n",
    "        rmse_df = rmse_df.rename(make_parameter_names_nice(rmse_df.index.get_level_values(1).unique()),level=1)\n",
    "        sources_ind = sources[commodity_s].dropna().index.get_level_values(0).unique()\n",
    "        sources_ind = [i.replace('Demand','Intensity') for i in sources_ind]\n",
    "        inter = np.intersect1d(sources_ind,  rmse_df.index.get_level_values(1).unique())\n",
    "        \n",
    "        fig, ax = easy_subplots(inter)\n",
    "        n_bins = 30\n",
    "\n",
    "        for i,a in zip(inter,ax):\n",
    "            source_a = sources.copy().loc[idx[i,:],commodity_s].droplevel(0).dropna()\n",
    "            model = rmse_df.copy().loc[idx[commodity,i],:]\n",
    "            bin0 = min(model.min(), source_a.min())\n",
    "            bin1 = max(model.max(), source_a.max())\n",
    "            bins = np.linspace(bin0,bin1,n_bins)\n",
    "            if 'Intensity' in i:\n",
    "                demand_ind = [q for q in sources.index.get_level_values(1) if '- demand' in q]\n",
    "                not_demand_ind = [q for q in sources.index.get_level_values(1) if '- demand' not in q]\n",
    "                source_a = sources.loc[idx[i,not_demand_ind],commodity_s].dropna()\n",
    "                source_b = sources.loc[idx[i,demand_ind],commodity_s].dropna()\n",
    "                bin0 = min(bin0,source_b.min())\n",
    "                bin1 = max(bin1,source_b.max())\n",
    "                bins = np.linspace(bin0,bin1,n_bins)\n",
    "            a.hist(model, density=True, alpha=0.5, bins=bins, label='Model output')\n",
    "            a.hist(source_a, density=True, alpha=0.5, bins=bins, label='Literature')\n",
    "            if 'Intensity' in i:\n",
    "                a.hist(source_b, density=True, alpha=0.5, bins=bins, label='Literature, demand')\n",
    "            a.legend()\n",
    "            a.set(title=i, ylabel='Density', xlabel='Parameter value')\n",
    "        fig.suptitle(commodity_s,weight='bold',y=1)\n",
    "        fig.tight_layout(pad=0.1)\n",
    "        return fig\n",
    "\n",
    "plot_compare_tuning_with_literature(many.integ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2b8c28",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many_sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a7521",
   "metadata": {},
   "source": [
    "## Adding rmse_df to future run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b49dc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce67efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# many_b.msh = many_b.multi_scenario_hyperparam.copy().stack().droplevel(1).unstack()\n",
    "# many_b.msr = many_b.multi_scenario_results.copy().stack().droplevel(1).unstack()\n",
    "msh = many_b.msh.copy()\n",
    "msr = many_b.msr.copy()\n",
    "for i in ['hyperparam_set_group','hyperparam_set_number']:\n",
    "    if i in msh.index.get_level_values(1).unique():\n",
    "        msh.drop(i, level=1, inplace=True)\n",
    "\n",
    "types = pd.Series([type(i) for i in msh.iloc[:,0]],msh.index)\n",
    "types = (types == float) | (types == int) | (types == np.float64)\n",
    "changing_hyperparam = msh.loc[types].copy()\n",
    "changing_hyperparam = changing_hyperparam.loc[\n",
    "    ~(changing_hyperparam.apply(lambda x: x-x.mean(),axis=1)<1e-6).all(axis=1)]\n",
    "\n",
    "to_hist_dict = dict(zip(['Total demand','Mine production','Refined price'],\n",
    "                        ['Total demand','Primary supply','Primary commodity price']))\n",
    "to_rmse_dict = dict(zip(['Total demand','Primary supply','Primary commodity price'],\n",
    "                        ['RMSE Total demand','RMSE Primary supply','RMSE Primary commodity price']))\n",
    "score = changing_hyperparam.loc[idx[:,\n",
    "                                    ['RMSE Total demand','RMSE Primary commodity price','RMSE Primary supply']],\n",
    "                                :]\n",
    "hist = many_b.historical_data.loc[idx[:,2001],:].rename(columns=to_rmse_dict)[\n",
    "    to_rmse_dict.values()].stack().unstack(1)[2001]\n",
    "score = score.div(hist,axis=0)\n",
    "score = np.log(score.groupby(level=0).sum().div(3))\n",
    "score = pd.DataFrame(score.stack(),columns=['score']).stack().unstack(1)\n",
    "changing_hyperparam = pd.concat([changing_hyperparam, score]).sort_index()\n",
    "many_b.rmse_df = changing_hyperparam.copy()\n",
    "\n",
    "rmse_df_sorted = pd.DataFrame()\n",
    "results_df_sorted = pd.DataFrame()\n",
    "for commodity in changing_hyperparam.index.get_level_values(0).unique():\n",
    "    rmse_ph = changing_hyperparam.loc[commodity]\n",
    "    cols = rmse_ph.loc['score'].sort_values().index\n",
    "    rmse_sorted = rmse_ph.T.sort_values(by='score').reset_index(drop=True).T\n",
    "    rmse_sorted = pd.concat([rmse_sorted],keys=[commodity])\n",
    "    rmse_df_sorted = pd.concat([rmse_df_sorted, rmse_sorted])\n",
    "    \n",
    "    results_ph = msr.loc[commodity].unstack()\n",
    "    results_ph = results_ph.loc[cols].reset_index(drop=True).stack()\n",
    "    results_ph = pd.concat([results_ph],keys=[commodity])\n",
    "    results_df_sorted = pd.concat([results_df_sorted, results_ph])\n",
    "\n",
    "many_b.rmse_df_sorted = rmse_df_sorted.copy()\n",
    "many_b.results_sorted = results_df_sorted.copy()\n",
    "many_b.results = msr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "for commodity in many_b.rmse_df.index.get_level_values(0).unique():\n",
    "    fig,ax=plt.subplots()\n",
    "    x = many_b.rmse_df_sorted.loc[idx[commodity,'score'],:]\n",
    "    x = x[(x<x.quantile(0.95))]\n",
    "    x.plot(ax=ax,label='b')\n",
    "    many_sg.integ.rmse_df_sorted.loc[idx[commodity,'score'],:].plot(ax=ax,label='sg')\n",
    "    ax.set(title=commodity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679562f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many = many_sg.integ\n",
    "commodity='lead'\n",
    "get_X_df_y_df(many, commodity=commodity)\n",
    "X_df = many.X_df.copy()\n",
    "y_df = many.y_df.copy()\n",
    "\n",
    "# import shap\n",
    "import fasttreeshap as shap # works exactly the same for the TreeExplainer function, should be fasterfrom sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test, ind_train, ind_test = train_test_split(X_df.values, y_df.values.reshape(y_df.shape[0],), \n",
    "                                                    X_df.index, test_size=0.33, random_state=42)\n",
    "data = pd.concat([X_df, y_df],axis=1).droplevel(0)\n",
    "\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MLPRegressor(hidden_layer_sizes=(500,),activation='logistic', batch_size=20, solver='lbfgs', max_iter=1000000,learning_rate='invscaling',random_state=0)\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "prediction = model.predict(X_test)\n",
    "m = do_a_regress(pd.Series(prediction), pd.Series(y_test))[1]\n",
    "m.rsquared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583cb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06dc4356",
   "metadata": {},
   "source": [
    "## Checking baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7ed45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many.integ.sources.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187db821",
   "metadata": {
    "code_folding": [
     4
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "commodities = many_b.multi_scenario_results.index.get_level_values(0).unique()\n",
    "commodity = 'aluminum'\n",
    "parameter = 'Mine production'\n",
    "parameter = 'Refined price'\n",
    "parameter = 'Total demand'\n",
    "\n",
    "# def plot_future_line_and_hist_one_commodity(many, commodity, parameter, restrict=0.95, use_sns=False, plot_historical=False, color='tab:blue', fig=None, ax=None, dpi=50):\n",
    "#     \"\"\"\n",
    "#     Plots the combined set of transparent lines for historical-future time\n",
    "#     progression of the given parameter and commodity, with a histogram aligning\n",
    "#     with the y axis that shows the 2040 value distribution.\n",
    "\n",
    "#     --------\n",
    "#     many: Many() object, must have multi_scenario_results object\n",
    "#     commodity: str, lowercase commodity form\n",
    "#     parameter: str, any column in many.multi_scenario_results\n",
    "#     restrict: float or False, whether to restrict the y-axis limits to exclude\n",
    "#         outliers, with float values corresponding to the percentile being\n",
    "#         plotted (e.g. 0.95 causes the middle 95% of max/min values to be shown)\n",
    "#     use_sns: bool, no functionality yet but figured it would be to have the\n",
    "#         sns.lineplot shading functionality\n",
    "#     plot_historical: bool, whether to show the historical values alongside the\n",
    "#         simulated\n",
    "#     color: str, matplotlib color for the lines and histogram\n",
    "#     fig: figure on which to plot\n",
    "#     ax: axes on which to plot, must be a list of length 2, as the histogram\n",
    "#         plots on ax[1]\n",
    "#     dpi: int, dots per square inch, controls figure resolution\n",
    "#     ----------\n",
    "\n",
    "#     Returns: comm (the data being plotted), fig\n",
    "#     \"\"\"\n",
    "#     if ax is None:\n",
    "#         fig,ax=easy_subplots(2,2,width_ratios=[4,1], sharey=False, width_scale=0.6)\n",
    "#     comm = many.multi_scenario_results.loc[commodity][parameter]\n",
    "#     comm = comm.unstack().T.loc[2001:]\n",
    "#     hist_data_dict = dict(zip(many.objective_results_map.values(), many.objective_results_map.keys()))\n",
    "#     if parameter in hist_data_dict and plot_historical:\n",
    "#         hist = many.historical_data.loc[commodity][hist_data_dict[parameter]].loc[2001:2019]\n",
    "#         dicty = get_unit(comm, hist, parameter)\n",
    "#         comm = dicty['simulated']\n",
    "#         hist = dicty['historical']\n",
    "#         unit = dicty['unit']\n",
    "#     else:\n",
    "#         dicty = get_unit(comm, comm, parameter)\n",
    "#         comm = dicty['simulated']\n",
    "#         unit = dicty['unit']\n",
    "\n",
    "#     comm.plot(legend=False, alpha=0.05, linewidth=1, color=color,\n",
    "#               ylabel=f'{parameter} ({unit})',\n",
    "#               xlabel='Year',\n",
    "#               ax=ax[0],\n",
    "#              ).grid(axis='x')\n",
    "\n",
    "#     if parameter in hist_data_dict and plot_historical:\n",
    "#         hist.plot(\n",
    "#                 color='k',\n",
    "#                 alpha=0.5,\n",
    "#                 ax=ax[0],\n",
    "#             ).grid(axis='x')\n",
    "#     if restrict:\n",
    "#         restrict = 1-(1-restrict)/2\n",
    "#         booly = (comm.min()>-comm.min().quantile(1-restrict)) &\\\n",
    "#                 (comm.max()<comm.max().quantile(restrict))\n",
    "#         restricted = booly[booly].index\n",
    "#     else: restricted = comm.columns\n",
    "#     comm.loc[2040][restricted].plot.hist(orientation='horizontal', ax=ax[1],\n",
    "#                              color=color\n",
    "#                             ).grid(axis='x')\n",
    "#     if restrict:\n",
    "#         ax[0].set(ylim=(-comm.min().quantile(1-restrict), comm.max().quantile(restrict)))\n",
    "#         ax[1].set(ylim=(-comm.min().quantile(1-restrict), comm.max().quantile(restrict)))\n",
    "\n",
    "#     ax[1].set(xlabel='2040 Freq.', yticklabels=[])\n",
    "\n",
    "#     fig.suptitle(f'{commodity.capitalize()} {parameter.lower()}',y=0.9, x=0.58, weight='bold')\n",
    "#     fig.tight_layout()\n",
    "#     fig.set_dpi(dpi)\n",
    "#     plt.show()\n",
    "#     plt.close()\n",
    "#     return comm, fig\n",
    "\n",
    "\n",
    "for commodity in commodities:\n",
    "    comm, fig = plot_future_line_and_hist_one_commodity(many_b, commodity, parameter, restrict=0.95,\n",
    "                                                       plot_historical=True)\n",
    "# comm, fig = plot_future_line_and_hist_one_commodity(many_b, 'aluminum', parameter, restrict=0.95,color='tab:blue',\n",
    "#                                                        plot_historical=True,dpi=50)\n",
    "# comm, fig = plot_future_line_and_hist_one_commodity(many_b, 'silver', parameter, restrict=0.95,color='seagreen',\n",
    "#                                                        plot_historical=True,dpi=50)\n",
    "\n",
    "# rscale=[4,1]\n",
    "# fig,ax = easy_subplots(len(commodities)*2, 6, \n",
    "#                        width_ratios = list(np.tile(rscale,3)), width_scale=0.6,\n",
    "#                       )\n",
    "# for e,commodity in enumerate(commodities):\n",
    "#     a = ax[2*e:2*e+2]\n",
    "#     plot_future_line_and_hist_one_commodity(many_b, commodity, parameter, fig=fig, ax=a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d7ed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_b.multi_scenario_results.loc['gold']['Mine production'].unstack().sort_index()#.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30514852-c720-4ba9-98dd-61209a1d569c",
   "metadata": {},
   "source": [
    "## Checking train vs test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81d778c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table, means, pvals, alt_means, fig = plot_colorful_table2(many_sg, stars='uniform', dpi=50, rand_size=None);\n",
    "table15, means15, pvals15, alt_means15, fig = plot_colorful_table2(many_15, stars='uniform', dpi=50, rand_size=None);\n",
    "table16, means16, pvals16, alt_means16, fig = plot_colorful_table2(many_16, stars='uniform', dpi=50, rand_size=None);\n",
    "table17, means17, pvals17, alt_means17, fig = plot_colorful_table2(many_17, stars='uniform', dpi=50, rand_size=None);\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1893205",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "means_all = pd.concat([means, means15, means16, means17], \n",
    "                      keys=['Full','Split, 2014','Split, 2015', 'Split, 2016']).unstack(0).droplevel(0,axis=1)\n",
    "means_all = means_all.T\n",
    "means_relative = means_all.apply(lambda x: x/means_all.loc[x.name[0]].loc['Full'], axis=1)\n",
    "means_relative[abs(means_relative)>3] = np.nan\n",
    "\n",
    "means_all_sub = means_all.copy().drop('Split, 2015',level=1)\n",
    "means_coef_of_full = means_all_sub.groupby(level=0).std()/means_all_sub.loc[idx[:,'Full'],:].droplevel(1)\n",
    "means_coef_of_var = means_all_sub.groupby(level=0).std()/means_all_sub.groupby(level=0).mean()\n",
    "means_coef_of_var = abs(means_coef_of_var)\n",
    "means_coef_of_var_nan = means_coef_of_var.copy()\n",
    "# means_coef_of_var_nan[abs(means_coef_of_var_nan)>7] = np.nan\n",
    "fig, ax = plt.subplots(figsize=(14,14))\n",
    "sns.heatmap(means_coef_of_var_nan, ax=ax, yticklabels=True,\n",
    "            annot = means_coef_of_var, vmin=0, vmax=2,\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2c0bfb",
   "metadata": {
    "code_folding": [
     135
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_given_columns_for_paper(many, commodity, columns, column_name=None, \n",
    "                                 ax=None, column_subset=None, start_year=None, end_year=2019, \n",
    "                                 show_all_lines=False, r2_on_own_line=True, dpi=50):\n",
    "    \"\"\"\n",
    "    Plots historical vs simulated demand, mining,\n",
    "    and primary commodity price for a given commodity\n",
    "    and whichever hyperparameter sets given as the\n",
    "    `columns` variable. Can get the n_best_scenarios\n",
    "    using the get_best_columns function, or more manually\n",
    "    by running get_train_test_scores on an rmse_df (or\n",
    "    commodity subset), then running its output through\n",
    "    get_commodity_scores, e.g.\n",
    "\n",
    "    rr = get_train_test_scores(many_test.integ.rmse_df.loc['silver'])\n",
    "    s1 = get_commodity_scores(rr,None)\n",
    "    then passing s1.loc[s1.selection2].index as the columns input.\n",
    "\n",
    "    -------------\n",
    "    many: Many() object, needs to have results object of its own,\n",
    "        so if you ran get_multiple to load many, you should pass\n",
    "        many.integ to this function\n",
    "    commodity: str, commodity name in lowercase form\n",
    "    columns: list, int, or None. If list, columns from rmse_df to \n",
    "        plot. If int, will selected the int lowest-score (RMSE)\n",
    "        columns. If None, will select the 25 lowest-score columns.\n",
    "        Will highlight the lowest-score one if no column_subset is \n",
    "        passed.\n",
    "    column_name: str, gets included in the plot title if not None\n",
    "    ax: matplotlib axes object, can leave out and this will\n",
    "        create its own plot for you.\n",
    "    column_subset: list, allows you to select a subset of the\n",
    "        passed columns to highlight, or to just plot two groups\n",
    "        of parameter sets simultaneously, since column_subset\n",
    "        and columns do not have to intersect. Pass a list or\n",
    "        array of numbers corresponding to rmse_df columns.\n",
    "    dpi: dots per inch, controls resolution. Only functions if\n",
    "        the ax input is None.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig,ax=easy_subplots(3, dpi=dpi)\n",
    "    else:\n",
    "        fig = 0\n",
    "    if columns is None:\n",
    "        columns = many.rmse_df.loc[commodity].sort_values(by='score',axis=1).iloc[:,:25].columns\n",
    "    elif type(columns)==int:\n",
    "        columns = many.rmse_df.loc[commodity].sort_values(by='score',axis=1).iloc[:,:columns].columns\n",
    "    objective_results_map = {'Total demand':'Total demand','Primary commodity price':'Refined price',\n",
    "                                 'Primary demand':'Conc. demand','Primary supply':'Mine production',\n",
    "                                'Conc. SD':'Conc. SD','Scrap SD':'Scrap SD','Ref. SD':'Ref. SD'}\n",
    "    for i,a in zip(['Total demand','Primary commodity price','Primary supply'], ax):\n",
    "        results = many.results_sorted.copy()[objective_results_map[i]].sort_index()\\\n",
    "            .loc[idx[commodity,:,2001:end_year]].droplevel(0).unstack(0)\n",
    "        if 'SD' not in i:\n",
    "            historical_data = many.historical_data.copy()[i].loc[commodity].loc[:2019]\n",
    "            if start_year is not None:\n",
    "                historical_data = historical_data.loc[start_year:]\n",
    "        else:\n",
    "            historical_data = pd.Series(results.min(),[0])\n",
    "        results_ph = results.copy()\n",
    "#         results = results[columns]\n",
    "        results = results.loc[:,:25]\n",
    "\n",
    "        diction = get_unit(results, historical_data, i)\n",
    "        results, historical_data, unit = [diction[i] for i in ['simulated','historical','unit']]\n",
    "#         results_ph *= results[columns[0]].mean()/results_ph[columns[0]].mean()\n",
    "        if show_all_lines:\n",
    "            sim_line = a.plot(results,linewidth=1,color='gray',alpha=0.3,label=results.columns)\n",
    "            if column_subset is None:\n",
    "                best_line= a.plot(results[columns[0]],linewidth=6,label='Simulated',color='tab:blue')\n",
    "            else:\n",
    "                best_line= a.plot(results_ph[column_subset],linewidth=1,label='Simulated',color='tab:blue')\n",
    "        else:\n",
    "            sns_results = results.stack().reset_index().rename(columns={\n",
    "                'level_0':'Year','level_1':'Scenario',0:'Value'\n",
    "            })\n",
    "            sim_line = sns.lineplot(data=sns_results, x='Year', y='Value', ax=a)\n",
    "            sim_line = sim_line.get_lines()\n",
    "            if len(sim_line)>1:\n",
    "                sim_line[-1].set_linestyle(['-','--',':','-.'][int((len(sim_line)-1)/2)])\n",
    "        mins = min(historical_data.min(),results[0].min())*0.95\n",
    "        maxs = max(historical_data.max(),results[0].max())*1.1\n",
    "        hist_line = a.plot(historical_data,label='Historical',color='k',linewidth=6)\n",
    "        inter = np.intersect1d(results.index,historical_data.index)\n",
    "        if show_all_lines:\n",
    "            m = sm.GLS(historical_data.loc[inter], \n",
    "                       sm.add_constant(results[columns[0]].loc[inter])\n",
    "                      ).fit(cov_type='HC3')\n",
    "        else:\n",
    "            m = sm.GLS(historical_data.loc[inter], \n",
    "                       sm.add_constant(sim_line[-1].get_data()[1])\n",
    "                      ).fit(cov_type='HC3')\n",
    "        mse = round(m.mse_resid**0.5,2)\n",
    "        mse = round(m.rsquared,2)\n",
    "        if column_name is not None:\n",
    "            title=f'{i}, {column_name} {commodity},\\n'+r'$R^2$'+f'={mse}, scenario {columns[0]}'\n",
    "        else:\n",
    "            title=f'{i}, {commodity}'\n",
    "        if i=='Primary commodity price':\n",
    "            maxs *= 1.1\n",
    "            title = title.replace('Primary commodity','Refined metal')\n",
    "        elif i=='Primary supply' and show_all_lines:\n",
    "            maxs *= 1.1\n",
    "        title = title.replace('Primary supply','Mine production')\n",
    "        a.set(title=title,\n",
    "              ylabel=i+' ('+unit+')',xlabel='Year',ylim=(mins,maxs))\n",
    "#         a.text(0.05,0.9, r'$R^2$'+f'={mse}', transform=a.transAxes)\n",
    "\n",
    "        if len(sim_line)<10 and show_all_lines:\n",
    "            a.legend()\n",
    "        elif len(sim_line)==1:\n",
    "            r2_handle = Line2D([0],[0], color='w', linewidth=0)\n",
    "            if r2_on_own_line:\n",
    "                a.legend([hist_line[0], sim_line[0], r2_handle],\n",
    "                         ['Historical','Simulated',r'$R^2$'+f'={mse}'])\n",
    "            else:\n",
    "                a.legend([hist_line[0], sim_line[0]],\n",
    "                         ['Historical','Simulated'+r' $R^2$'+f'={mse}'], loc='upper left')\n",
    "                sim_line[-1].set_label('Simulated:'+r' $R^2$'+f'={mse}')\n",
    "\n",
    "        elif 'best_line' not in locals():\n",
    "            lines = [i for i in sim_line if i.get_color()!='k']\n",
    "            labels = [i.get_label().replace('Simulated',j) for i,j in zip(lines,many.many_keys)][:-1]\n",
    "            a.legend([hist_line[0]]+lines,\n",
    "                     ['Historical']+labels+[many.plot_label+r': $R^2$'+f'={mse}'], loc='upper left')\n",
    "            sim_line[-1].set_label(many.plot_label+r': $R^2$'+f'={mse}')\n",
    "            a.set_ylim(a.get_ylim()[0], a.get_ylim()[1]*(1+len(sim_line)/4*0.2))\n",
    "        else:\n",
    "            r2_handle = Line2D([0],[0], color='w', linewidth=0)\n",
    "            if r2_on_own_line:\n",
    "                a.legend([hist_line[0], best_line[0], sim_line[0], r2_handle],\n",
    "                         ['Historical','Simulated, best','Simulated, other',r'$R^2$'+f'={mse}'])\n",
    "            else:\n",
    "                a.legend([hist_line[0], best_line[0], sim_line[0]],\n",
    "                         ['Historical','Simulated, best:'+r' $R^2$'+f'={mse}','Simulated, other'])\n",
    "    return fig,ax\n",
    "\n",
    "def plot_best_fits(many, show_all_lines=False, dpi=50):\n",
    "    \"\"\"\n",
    "    many: Many object or dict full of labeled Many objects\n",
    "    \"\"\"\n",
    "    if show_all_lines and type(many)==dict:\n",
    "        show_all_lines=False\n",
    "        warnings.warn('dict type input for the many variable is not accepted with show_all_lines=True; show_all_lines has been set to False')\n",
    "    if type(many)==Many:\n",
    "        many_dict = {'':many}\n",
    "    else:\n",
    "        many_dict = many\n",
    "        many = many[list(many.keys())[0]]\n",
    "    commodities=['Ag','Sn','Al','Steel']\n",
    "#     commodities=['Cu','Ni','Pb','Zn','Au']\n",
    "#     commodities=['Cu','Ni','Pb','Zn','Au','Ag','Sn','Al','Steel']\n",
    "#     commodities=['Au']\n",
    "#     commodities=[many.element_commodity_map[i].lower() for i in commodities]\n",
    "    fig, ax = easy_subplots(len(commodities)*3)\n",
    "    for e,comm in enumerate(commodities):\n",
    "        for many in many_dict:\n",
    "            a = ax[3*e:3*(e+1)]\n",
    "            many_dict[many].plot_label = many\n",
    "            many_dict[many].many_keys = list(many_dict.keys())\n",
    "            fig1,a=plot_given_columns_for_paper(many_dict[many],\n",
    "                                                comm,\n",
    "                                                columns=None, \n",
    "                                                show_all_lines=show_all_lines, \n",
    "                                                r2_on_own_line=False,\n",
    "                                                ax=a\n",
    "                                               )\n",
    "            fig.tight_layout()\n",
    "            fig.set_dpi(dpi)\n",
    "    return fig\n",
    "\n",
    "fig_fits = plot_best_fits({'Full':many_sg, \n",
    "                'To 2016':many_17, \n",
    "                'To 2015':many_16, \n",
    "                'To 2014':many_15}, \n",
    "               show_all_lines=False, dpi=150);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a465c8bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r2_df = pd.DataFrame()\n",
    "for a in fig_fits.get_axes():\n",
    "    title = a.get_title()\n",
    "    lines = a.get_lines()\n",
    "    labels = [i.get_label() for i in lines]\n",
    "    labels = [i.replace('Simulated','Full') for i in labels if i!='Historical']\n",
    "    keys = [i.split(':')[0] for i in labels]\n",
    "    r2 = [float(i.split('=')[1]) for i in labels]\n",
    "    r2_series = pd.Series(r2,keys)\n",
    "    r2_series.name = title.split(', ')[1]\n",
    "    r2_series = pd.concat([r2_series], keys=[title.split(',')[0]])\n",
    "    r2_df = pd.concat([r2_df, r2_series],axis=1)\n",
    "r2 = r2_df.stack().unstack(level=1)\n",
    "r2_difference = r2.mul(-1).add(r2['Full'],axis=0)\n",
    "fig,ax=plt.subplots(figsize=(7,10))\n",
    "sns.heatmap(r2_difference, yticklabels=True, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_difference.stack()[(r2_difference.stack()<r2_difference.stack().quantile(0.9))&\n",
    "                      (r2_difference.stack()>r2_difference.stack().quantile(0.8))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6efefe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cov = means_coef_of_var.rename(columns=many_sg.element_commodity_map)\n",
    "cov.columns = [i.lower() for i in cov.columns]\n",
    "r2_diff_max = r2_difference.std(axis=1)\n",
    "new_corr = pd.DataFrame()\n",
    "for k in ['Mine production', 'Refined metal price', 'Total demand']:\n",
    "    r2_diff_indiv = r2_diff_max.loc[k] # Mine production, Refined metal price, Total demand\n",
    "    # many_sg.element_commodity_map\n",
    "    correlation_matrix = pd.concat([\n",
    "        cov,\n",
    "        pd.DataFrame(r2_diff_indiv).T]).T.corr()\n",
    "    new_corr_ph = correlation_matrix[0].drop(0)\n",
    "    new_corr_ph.name = k\n",
    "    new_corr = pd.concat([new_corr, new_corr_ph],axis=1)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(8,12))\n",
    "feature_importance(many_sg)\n",
    "ascending_importance = many_sg.importances_df['Best test R2'].sort_values().rename(\n",
    "    make_parameter_names_nice(many_sg.importances_df.index))\n",
    "new_corr = new_corr.loc[ascending_importance.index]\n",
    "sns.heatmap(new_corr, ax=ax, annot=True, cmap='bwr')\n",
    "ax.set(title='Correlation between R2 variance\\nand CoV for parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d350f00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a1c9e",
   "metadata": {
    "code_folding": [
     0,
     20
    ]
   },
   "outputs": [],
   "source": [
    "def get_r2():\n",
    "    years = np.arange(2001,2020)\n",
    "    r2_values = pd.DataFrame()\n",
    "    commodities = many_sg.rmse_df.index.get_level_values(0).unique()\n",
    "    historical_names = {'Total demand':'Total demand', 'Mine production':'Primary supply', \n",
    "                        'Refined price':'Primary commodity price'}\n",
    "    rsquared_df = pd.DataFrame()\n",
    "    for param in ['Total demand', 'Mine production', 'Refined price']:\n",
    "        rsquared_df_ph = pd.DataFrame()\n",
    "        for comm in commodities:\n",
    "            regr_y = many_sg.historical_data[historical_names[param]].loc[comm].loc[years]\n",
    "            for many,label in zip([many_sg, many_15, many_16, many_17],['Full','To 2014','To 2015','To 2016']):\n",
    "                regr_x = many.results_sorted[param].loc[comm].loc[idx[:25,years]].groupby(level=1).mean()\n",
    "                m = do_a_regress(regr_x, regr_y,plot=False)[1]\n",
    "                rsquared_df_ph.loc[comm, label] = m.rsquared\n",
    "        rsquared_df_ph = pd.concat([rsquared_df_ph],keys=[param])\n",
    "        rsquared_df = pd.concat([rsquared_df, rsquared_df_ph])\n",
    "    return rsquared_df\n",
    "\n",
    "def get_r2_from_plot(fig_fits):\n",
    "    \"\"\"\n",
    "    Getting R2 differences from plot above\n",
    "    \"\"\"\n",
    "    r2_df = pd.DataFrame()\n",
    "    for a in fig_fits.get_axes():\n",
    "        title = a.get_title()\n",
    "        lines = a.get_lines()\n",
    "        labels = [i.get_label() for i in lines]\n",
    "        labels = [i.replace('Simulated','Full') for i in labels if i!='Historical']\n",
    "        keys = [i.split(':')[0] for i in labels]\n",
    "        r2 = [float(i.split('=')[1]) for i in labels]\n",
    "        r2_series = pd.Series(r2,keys)\n",
    "        r2_series.name = title.split(', ')[1]\n",
    "        r2_series = pd.concat([r2_series], keys=[title.split(',')[0]])\n",
    "        r2_df = pd.concat([r2_df, r2_series],axis=1)\n",
    "    r2 = r2_df.stack().unstack(level=1)\n",
    "    return r2\n",
    "\n",
    "r2 = get_r2()\n",
    "r2_difference = r2.mul(-1).add(r2['Full'],axis=0)\n",
    "\n",
    "# R2 differences\n",
    "largest_diff = r2_difference.apply(lambda x: x.loc[abs(x).idxmax()], axis=1)\n",
    "largest_diff = r2_difference.drop(columns='Full')\n",
    "# if 'To 2015' in largest_diff.columns:\n",
    "#     largest_diff.drop(columns='To 2015',inplace=True)\n",
    "largest_diff = largest_diff.stack()\n",
    "\n",
    "fig,axes = easy_subplots(2)\n",
    "ax = axes[0]\n",
    "data = abs(largest_diff)\n",
    "v = ax.hist(data, cumulative=True, histtype='step', density=True, linewidth=6, bins=50)\n",
    "n = v[0]\n",
    "\n",
    "new_ticks = []\n",
    "for x in [0.01, 0.02, 0.03, 0.04, 0.05, 0.1,0.4]:\n",
    "    print('Percent of R2 changes less than {:.02f}: {:.1f}'.format(x,(data<x).sum()/len(data)*100))\n",
    "    err = 0.005\n",
    "#     val = []\n",
    "#     while len(val)==0:\n",
    "#         val = v[0][abs(v[1][:-1]-x)<err]\n",
    "#         err *= 1.2\n",
    "#     new_ticks += [np.mean(val)]\n",
    "    \n",
    "ax.set(xlim=(ax.get_xlim()[0], data.max()),\n",
    "       title=r'$R^2$ difference from full tuning'+'\\ncumulative density',\n",
    "       xlabel=r'Absolute difference in $R^2$',\n",
    "       ylabel='Cumulative density',\n",
    "#        yticks=new_ticks,\n",
    "       yticks=[n[0],n[1],n[3], n[6], np.mean(v[0][abs(v[1][:-1]-0.1)<0.005]), 1]\n",
    "      );\n",
    "ax.grid(True, axis='both')\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%0.3f'))\n",
    "\n",
    "# Coef of variation\n",
    "ax = axes[1]\n",
    "data = abs(means_coef_of_var.stack()).mul(100)\n",
    "v = ax.hist(data,\n",
    "               bins=50, cumulative=True, density=True, histtype='step',\n",
    "               linewidth=6,\n",
    ")\n",
    "n = v[0]\n",
    "ax.set(\n",
    "    title='Coefficient of variation cumulative denstiy,\\n all parameters & commodities, full tuning',\n",
    "    xlabel='Coefficient of variation (%)',\n",
    "    ylabel='Cumulative fraction',\n",
    "    yticks=[n[0],n[1],n[3], 1],\n",
    "    xlim=(-5,200),\n",
    ")\n",
    "ax.yaxis.set_major_formatter(mpl.ticker.FormatStrFormatter('%0.3f'))\n",
    "ax.grid(True, axis='both')\n",
    "\n",
    "print('')\n",
    "for x in [25,50,75,100]:\n",
    "    print('Percent of coefficients of variation less than {:.0f}: {:.1f}'.format(x,(data<x).sum()/len(data)*100))\n",
    "fig.tight_layout()\n",
    "\n",
    "print('\\nLargest 10% of R2 differences')\n",
    "display(largest_diff[(largest_diff>largest_diff.quantile(0.9))])\n",
    "\n",
    "print('\\nAdditional next 10% of R2 differences')\n",
    "display(largest_diff[(largest_diff>largest_diff.quantile(0.8))&(largest_diff<largest_diff.quantile(0.9))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27223be2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ad40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([new_corr['Mine production'], ascending_importance],axis=1).rename(\n",
    "#     columns={'Best test R2':'Importance','Mine production':'R2-CoV Correlation'}).plot()\n",
    "r2_diff_max\n",
    "results = pd.DataFrame()\n",
    "for comm in means_coef_of_var.columns:\n",
    "    if comm in cov.columns:\n",
    "        comm2 = comm\n",
    "    else:\n",
    "        comm2 = many_sg.element_commodity_map[comm].lower()\n",
    "    concat = pd.concat([\n",
    "        cov[comm2],\n",
    "        means_coef_of_var[comm]], axis=1, keys=['Importance','CoV'])\n",
    "    \n",
    "    m = do_a_regress(concat['CoV'], concat['Importance'], plot=False)[1]\n",
    "    results.loc[comm, 'pval'] = m.pvalues['CoV']\n",
    "    results.loc[comm, 'coef'] = m.params['CoV']\n",
    "    results.loc[comm, 'corr'] = concat.corr()['CoV']['Importance']\n",
    "\n",
    "\"\"\"\n",
    "For the relationship between feature importance and parameter coefficient of variance \n",
    "across different training periods, the median regression coefficient is near zero; \n",
    "the mean and median correlation are negative. P-values for the regression coefficients\n",
    "are generally large with the exception of lead, which shows a statistically significant\n",
    "negative relationship between feature importance and parameter variation. \n",
    "\n",
    "Additionally, \n",
    "\"\"\"\n",
    "pd.concat([results,\n",
    "           pd.DataFrame(results.mean(),columns=['Mean']).T,\n",
    "           pd.DataFrame(results.median(),columns=['Median']).T,\n",
    "           pd.DataFrame(results.apply(lambda x: stats.ttest_1samp(x.loc[:'Zn'],popmean=0)[1],axis=0),\n",
    "                        columns=['t-test p-value']).T,\n",
    "          ]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ad70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_difference\n",
    "keep = [i for i in many_sg.rmse_df.index.get_level_values(1).unique()\n",
    "        if 'R2' not in i and 'RMSE' not in i and 'score' not in i]\n",
    "rmse_df = many_sg.rmse_df_sorted.loc[idx[:,keep],:25]\n",
    "cov = abs(rmse_df.std(axis=1)/abs(rmse_df.mean(axis=1))).unstack(0)\n",
    "cov = cov.rename(make_parameter_names_nice(cov.index))\n",
    "cov = cov.drop('Region specific intensity elasticity to price')\n",
    "# cov = cov.div(cov.max(axis=1).max(),axis=0)\n",
    "sns.heatmap(cov)\n",
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([new_corr['Mine production'], ascending_importance],axis=1).rename(\n",
    "#     columns={'Best test R2':'Importance','Mine production':'R2-CoV Correlation'}).plot()\n",
    "r2_diff_max\n",
    "results = pd.DataFrame()\n",
    "for comm in means_coef_of_var.columns:\n",
    "    if comm in cov.columns:\n",
    "        comm2 = comm\n",
    "    else:\n",
    "        comm2 = many_sg.element_commodity_map[comm].lower()\n",
    "    concat = pd.concat([\n",
    "        cov[comm2],\n",
    "        means_coef_of_var[comm]], axis=1, keys=['Importance','CoV'])\n",
    "    \n",
    "    m = do_a_regress(concat['CoV'], concat['Importance'], plot=False)[1]\n",
    "    results.loc[comm, 'pval'] = m.pvalues['CoV']\n",
    "    results.loc[comm, 'coef'] = m.params['CoV']\n",
    "    results.loc[comm, 'corr'] = concat.corr()['CoV']['Importance']\n",
    "\n",
    "\"\"\"\n",
    "For the relationship between feature importance and parameter coefficient of variance \n",
    "across different training periods, the median regression coefficient is near zero; \n",
    "the mean and median correlation are negative. P-values for the regression coefficients\n",
    "are generally large with the exception of lead, which shows a statistically significant\n",
    "negative relationship between feature importance and parameter variation. \n",
    "\n",
    "Additionally, \n",
    "\"\"\"\n",
    "pd.concat([results,\n",
    "           pd.DataFrame(results.mean(),columns=['Mean']).T,\n",
    "           pd.DataFrame(results.median(),columns=['Median']).T,\n",
    "           pd.DataFrame(results.apply(lambda x: stats.ttest_1samp(x.loc[:'Zn'],popmean=0)[1],axis=0),\n",
    "                        columns=['t-test p-value']).T,\n",
    "          ]).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcfad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity_level_feature_importance_heatmap(many_sg)\n",
    "many_sg.importances_df_reformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415ee68",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def heatmap_with_feature_importance():\n",
    "    annotate=True\n",
    "    fig, ax = plt.subplots(figsize=(15,10))\n",
    "    blanky = pd.DataFrame(means_coef_of_var['Ag']).rename(columns={'Ag':''})\n",
    "    blanky.loc[:] = np.nan\n",
    "\n",
    "    # adding R2 difference\n",
    "    # renamer = dict([(i,many_sg.commodity_element_map[i.capitalize()]) for i in r2_diff_max.index.levels[1]])\n",
    "    for_r2_diff = r2_diff_max.unstack(1)\n",
    "    for_r2_diff = pd.concat([\n",
    "        for_r2_diff,\n",
    "        pd.DataFrame(np.nan, for_r2_diff.index,['']),\n",
    "        for_r2_diff\n",
    "    ],axis=1)\n",
    "    quantile = for_r2_diff.stack().quantile(0.9)\n",
    "    print(f'R2 difference divided by 90th percentile: {quantile}')\n",
    "    for_r2_diff /= quantile\n",
    "    for_r2_diff.loc[:,'All'] = np.nan\n",
    "    for_r2_diff = pd.concat([\n",
    "        pd.DataFrame(np.nan, [''], for_r2_diff.columns),\n",
    "        for_r2_diff,\n",
    "    ])\n",
    "    for_r2_diff_nn = for_r2_diff*quantile\n",
    "\n",
    "    # making CoV+Importance dataframe\n",
    "    cov_importance_df = pd.concat([\n",
    "            means_coef_of_var,\n",
    "            blanky,\n",
    "            many_sg.importances_df_reformed/many_sg.importances_df_reformed.max().max(),\n",
    "        ],axis=1)\n",
    "    cov_importance_df_nn = pd.concat([\n",
    "            means_coef_of_var,\n",
    "            blanky,\n",
    "            many_sg.importances_df_reformed,\n",
    "        ],axis=1)\n",
    "\n",
    "    # combining cov_importance with for_r2_diff\n",
    "    cov_importance = pd.concat([\n",
    "        cov_importance_df,\n",
    "        for_r2_diff,\n",
    "    ])\n",
    "    cov_importance_nn = pd.concat([\n",
    "        cov_importance_df_nn,\n",
    "        for_r2_diff_nn,\n",
    "    ])\n",
    "\n",
    "    sns.heatmap(\n",
    "        cov_importance.drop(columns='All'), \n",
    "        ax = ax,\n",
    "        xticklabels=True,\n",
    "        yticklabels=True,\n",
    "        vmax=1,\n",
    "        annot = None if not annotate else cov_importance_nn.drop(columns='All'),\n",
    "        annot_kws = {'fontsize':12, },\n",
    "        fmt='.2f',\n",
    "        cbar_kws={'ticks':[], 'label':'Increasing value →'}\n",
    "    )\n",
    "    ax.set(title ='     CoV                                           Importance',\n",
    "           xlabel=r'Relative $R^2$ change                           Relative $R^2$ change   ',\n",
    "          )\n",
    "    \"\"\"\n",
    "    Cross-reference the important ones with those that diverge?\n",
    "    \"\"\"\n",
    "    \n",
    "heatmap_with_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f52cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotate=True\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "blanky = pd.DataFrame(means_coef_of_var['Ag']).rename(columns={'Ag':''})\n",
    "blanky.loc[:] = np.nan\n",
    "\n",
    "# adding R2 difference\n",
    "# renamer = dict([(i,many_sg.commodity_element_map[i.capitalize()]) for i in r2_diff_max.index.levels[1]])\n",
    "for_r2_diff = r2_diff_max.unstack(1)\n",
    "for_r2_diff = pd.concat([\n",
    "    for_r2_diff,\n",
    "    pd.DataFrame(np.nan, for_r2_diff.index,['']),\n",
    "    for_r2_diff\n",
    "],axis=1,keys=['CoV-train','','CoV-parameters'])\n",
    "quantile = for_r2_diff.droplevel(0,axis=1).stack().quantile(0.9)\n",
    "print(f'R2 difference divided by 90th percentile: {quantile}')\n",
    "for_r2_diff /= quantile\n",
    "for_r2_diff.loc[:,'All'] = np.nan\n",
    "for_r2_diff = pd.concat([\n",
    "    pd.DataFrame(np.nan, [''], for_r2_diff.columns),\n",
    "    for_r2_diff,\n",
    "])\n",
    "for_r2_diff_nn = for_r2_diff*quantile\n",
    "\n",
    "# making CoV+Importance dataframe\n",
    "cov_importance_df = pd.concat([\n",
    "        means_coef_of_var,\n",
    "        blanky,\n",
    "        (cov-cov.min().min())/(cov-cov.min().min()).stack().quantile(0.95),\n",
    "    ],axis=1,keys=['CoV-train','','CoV-parameters'])\n",
    "\n",
    "cov_importance_df_nn = pd.concat([\n",
    "        means_coef_of_var,\n",
    "        blanky,\n",
    "        cov,\n",
    "    ],axis=1,keys=['CoV-train','','CoV-parameters'])\n",
    "\n",
    "# combining cov_importance with for_r2_diff\n",
    "cov_importance = pd.concat([\n",
    "    cov_importance_df,\n",
    "    for_r2_diff,\n",
    "])\n",
    "cov_importance_nn = pd.concat([\n",
    "    cov_importance_df_nn,\n",
    "    for_r2_diff_nn,\n",
    "])\n",
    "\n",
    "sns.heatmap(\n",
    "    cov_importance.drop(columns='All').droplevel(0,axis=1), \n",
    "    ax = ax,\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    "    vmax=1,\n",
    "    annot = None if not annotate else cov_importance_nn.drop(columns='All'),\n",
    "    annot_kws = {'fontsize':12, },\n",
    "    fmt='.2f',\n",
    "    cbar_kws={'ticks':[], 'label':'Increasing value →'}\n",
    ")\n",
    "ax.set(title ='CoV - means across training sets     CoV - full tuning parameters    ',\n",
    "       xlabel=r'Relative $R^2$ change                           Relative $R^2$ change   ',\n",
    "      )\n",
    "fig.set(dpi=150)\n",
    "\"\"\"\n",
    "Cross-reference the important ones with those that diverge?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d09ef",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cutoff selection:\n",
    "cutoff_dict = {}\n",
    "for cutoff in [0.2, 0.3, 0.4, 0.5]:\n",
    "    include_vals = (means_coef_of_var>cutoff).any(axis=1)\n",
    "    include_vals = include_vals[include_vals].index\n",
    "    include_vals = include_vals[~include_vals.isin(['Incentive mine cost change per year'])]\n",
    "    cutoff_dict[cutoff] = len(include_vals)\n",
    "print('number of parameters included for different cutoff values:', cutoff_dict)\n",
    "\n",
    "# Using cutoff 0.3:\n",
    "cutoff = 0.3\n",
    "tune_to_list =['Mine production','Refined price','Total demand']\n",
    "results_all = pd.DataFrame()\n",
    "for tune_to in tune_to_list:\n",
    "    means_diff = means_all.stack().unstack(1)\n",
    "    means_diff = means_diff.subtract(means_diff['Full'],axis=0).drop(columns='Full')\n",
    "    means_diff = means_diff.rename(columns={'Split, 2014':'To 2014', 'Split, 2016':'To 2016',\n",
    "                                           'Split, 2015':'To 2015'})\n",
    "    means_diff = abs(means_diff.stack().unstack(0).fillna(0))\n",
    "    r2_d = largest_diff.copy().loc[tune_to]\n",
    "#     r2_d = r2_d.rename(dict([(i,many_sg.commodity_element_map[i.capitalize()]) for i in r2_d.index.get_level_values(0).unique()]))\n",
    "    r2_d = abs(r2_d.sort_index())\n",
    "    means_diff = means_diff.sort_index()\n",
    "    if False:\n",
    "        include_vals = (means_diff.groupby(level=0).std()>cutoff).any()\n",
    "        include_vals = include_vals[include_vals].index\n",
    "        include_vals = include_vals[~include_vals.isin(['Incentive mine cost change per year'])]\n",
    "    elif True:\n",
    "        include_vals = (means_coef_of_var>cutoff).any(axis=1)\n",
    "        include_vals = include_vals[include_vals].index\n",
    "        include_vals = include_vals[~include_vals.isin(['Incentive mine cost change per year'])]\n",
    "    else:\n",
    "        include_vals = ['Intensity elasticity to time','Intensity elasticity to GDP','Mine cost change per year',\n",
    "                        'Ore grade elasticity to COT distribution mean','Mine CU elasticity to TCM',\n",
    "                        'Secondary refinery CU elasticity to price','Fraction of viable mines that open']\n",
    "    combos = []\n",
    "    from itertools import combinations\n",
    "    for i in np.arange(1,len(include_vals)):\n",
    "        combos += list(combinations(include_vals,i))\n",
    "\n",
    "    results = pd.DataFrame()\n",
    "    for e,combo in enumerate(combos):\n",
    "        x = means_diff.loc[:,combo]\n",
    "        r2_d.name = tune_to+' R2 difference from full tuning'\n",
    "        m = sm.GLS(r2_d, sm.add_constant(x)).fit(cov_type='HC3')\n",
    "        results.loc[e,'AIC'] = m.aic\n",
    "        results.loc[e,'BIC'] = m.bic\n",
    "        results.loc[e,'f_pvalue'] = m.f_pvalue\n",
    "        results.loc[e,'rsquared'] = m.rsquared\n",
    "        results.loc[e,'n_sig'] = (m.pvalues<0.1).sum()\n",
    "        results.loc[e,'frac_sig'] = results['n_sig'][e]/len(m.pvalues)\n",
    "        results.loc[e,'m'] = m\n",
    "#     display(results.loc[results.f_pvalue<0.1].sort_values(by='AIC').m.iloc[0].summary())\n",
    "    results = pd.concat([results],keys=[tune_to])\n",
    "    results_all = pd.concat([results_all, results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d306860",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regression table:\n",
    "results_all_3 = results_all.copy()\n",
    "table_stats = pd.DataFrame()\n",
    "for p in ['Total demand', 'Refined price','Mine production']:\n",
    "    results = results_all_3.loc[p]\n",
    "    m = results.loc[results.f_pvalue<0.1].sort_values(by='AIC').m.iloc[0]\n",
    "    out = pd.DataFrame('', index=[], columns=[0,1])\n",
    "    out.loc['R-squared:',1] = round(m.rsquared,3)\n",
    "    out.loc['Adj. R-squared:',1] = round(m.rsquared_adj,3)\n",
    "    out.loc['F-statistic:',1] = round(m.fvalue,3)\n",
    "    out.loc['Prob(F-statistic):',1] = round(m.f_pvalue,3)\n",
    "    out.loc['No. Observations:',1] = m.nobs\n",
    "    out.loc['Df Residuals:',1] = m.df_resid\n",
    "    out.loc['Df Model:',1] = m.df_model\n",
    "    out.loc['Covariance Type:',1] = m.cov_type\n",
    "    out.loc['Parameter',0] = 'Value'\n",
    "    out.loc['Parameter',1] = 'P>|z|'\n",
    "    for x in m.params.index:\n",
    "        out.loc[x.replace('const','Constant'),0]=round(m.params[x],3)\n",
    "        out.loc[x.replace('const','Constant'),1]=round(m.pvalues[x],3)\n",
    "    out = out.fillna('').reset_index(drop=False)\n",
    "    out = pd.concat([out],keys=[p],axis=1)\n",
    "    table_stats = pd.concat([table_stats,out],axis=1)\n",
    "table_stats = table_stats.fillna('').droplevel(1,axis=1)\n",
    "table_stats.to_clipboard()\n",
    "table_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaff117",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results_all_3 = results_all.copy()\n",
    "results = results_all_3.loc['Mine production']\n",
    "results = results_all_3.loc['Refined price']\n",
    "results = results_all_3.loc['Total demand']\n",
    "results.loc[results.f_pvalue<0.1].sort_values(by='AIC').m.iloc[0].summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84873ca8-9d31-4637-84e8-0801734697f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rr = get_train_test_scores(many_test.integ.rmse_df.loc['silver'])\n",
    "s1 = get_commodity_scores(rr,None)\n",
    "fig,ax=easy_subplots(3)\n",
    "plot_given_columns(many_test.integ, 'silver', s1.loc[s1.selection2].index, 'try', ax)\n",
    "fig,ax=easy_subplots(3)\n",
    "plot_given_columns(many_test.integ, 'silver', c, 'try', ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e7fba-33b1-4239-8f26-4e83685cd943",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_best_scenarios_train_test(many_test.integ, weight_price=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c54281-4ef9-4f34-bd23-a9fe8e12420b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_test_score_vs_train_score(many_test.integ,show_selection='selection1')\n",
    "plot_best_scores_history(many_test16.integ,'steel',show_selection='selection1',n_best_train=100)\n",
    "# plot_best_scores_hyperparam_distributions(many_test.integ,'steel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3e3f02-67df-4d66-aaaa-42797d7ef1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparam_ph = generate_clustered_hyperparam(many_test.integ.rmse_df, commodity='aluminum', n_best_scenarios=25,\n",
    "                                              n_per_baseline=50, plot=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f3af8-47fd-48b0-866a-5208ce8bd219",
   "metadata": {},
   "source": [
    "## Unconstrained vs constrained tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca33165-eaec-4356-a2e4-1df72378e8af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compare_constrained_unconstrained_tuning(plot_cummin_rmse=True,log_cummin_rmse=True,plot_best_distributions=True,n_best_distributions=25,demand_or_mining='demand', to_compare = ['_constrain','_unconstrain','_unconstrain1'],dpi=50):\n",
    "    \"\"\"\n",
    "    plot_cummin_rmse: bool, whether to plot the cumulative minimum\n",
    "        RMSE/score value vs the number of Bayesian optimization\n",
    "        runs, for each commodity and tuning method\n",
    "    log_cummin_rmse: bool, whether to take the log10 of RMSE so it is\n",
    "        easier to visualize\n",
    "    plot_best_distributions: bool, whether to plot the distributions\n",
    "        for the parameters tuned for each commodity and tuning method\n",
    "    n_best_distributions: int, number of best parameters to use in\n",
    "        plot_best_distributions\n",
    "    mining_or_demand: str, either `mining` or `demand`, selects which\n",
    "        pre-tuning method parameters are shown\n",
    "    to_compare: list, list containing the filename_modifier strings\n",
    "        for each tuning method to compare\n",
    "    dpi: float, dots per inch, figure resolution\n",
    "    \"\"\"\n",
    "    ready_commodities = ['Al','Au','Sn','Cu','Ni','Ag','Zn','Pb','Steel']\n",
    "    element_commodity_map = {'Al':'Aluminum','Au':'Gold','Cu':'Copper','Steel':'Steel','Co':'Cobalt','REEs':'REEs','W':'Tungsten','Sn':'Tin','Ta':'Tantalum','Ni':'Nickel','Ag':'Silver','Zn':'Zinc','Pb':'Lead','Mo':'Molybdenum','Pt':'Platinum','Te':'Telllurium','Li':'Lithium'}\n",
    "    commodities = [element_commodity_map[i].lower() for i in ready_commodities]\n",
    "\n",
    "    fig_dict = {}\n",
    "    if plot_cummin_rmse:\n",
    "        cummin_fig, cummin_ax = easy_subplots(commodities,dpi=dpi)\n",
    "\n",
    "    if demand_or_mining in ['demand','mining']:\n",
    "        rmse_or_score = 'RMSE'\n",
    "    else:\n",
    "        rmse_or_score = 'score'\n",
    "\n",
    "    rmse_df_all = pd.DataFrame()\n",
    "    \n",
    "    def update_tc(tc):\n",
    "        tc = tc.replace('_','').capitalize().replace('onstrain','onstrained').replace('3','').replace('Unconstrained1','One anchor')\n",
    "        return tc\n",
    "        \n",
    "    for commodity,cummin_a in zip(commodities,cummin_ax):\n",
    "        rmse_df_multi = pd.concat([get_rmse_df_results_hyperparam(commodity=commodity,\n",
    "                                                                  filename_modifier=tc,\n",
    "                                                                  demand_or_mining=demand_or_mining,\n",
    "                                                                  filename_base='_run_hist',\n",
    "                                                                  file_folder='data/Historical tuning')[0]\n",
    "                                   for tc in to_compare],\n",
    "                                  keys=to_compare)\n",
    "        rmse_df_all = pd.concat([rmse_df_all,\n",
    "                                 pd.concat([rmse_df_multi],keys=[commodity])])\n",
    "        expected_parameter_signs = get_expected_parameter_signs(demand_or_mining,rmse_df_multi)\n",
    "\n",
    "        if plot_cummin_rmse:\n",
    "            for tc in to_compare:\n",
    "                rmse_df_multi.loc[tc].loc[idx[:,rmse_or_score]].cummin().plot(\n",
    "                    logy=log_cummin_rmse,\n",
    "                    label=update_tc(tc),\n",
    "                    ax=cummin_a,\n",
    "                    title=f'{commodity.capitalize()}',\n",
    "                    xlabel='n iterations',\n",
    "                    ylabel='RMSE'\n",
    "                )\n",
    "            cummin_a.legend()\n",
    "            # ph.loc[idx[:,'RMSE']].plot()\n",
    "\n",
    "        best_multi = {}\n",
    "        for tc in to_compare:\n",
    "            best = rmse_df_multi.loc[tc].loc[idx[:,rmse_or_score]].sort_values().head(n_best_distributions).index\n",
    "            best_multi[tc] = best\n",
    "        if plot_best_distributions:\n",
    "            demand_params = [i for i in rmse_df_multi.index.get_level_values(2).unique() if not\n",
    "                             np.any([j in i for j in ['RMSE','score','R2','region_specific_price_response']])]\n",
    "            nice_params = make_parameter_names_nice(demand_params)\n",
    "            dist_fig,dist_ax = easy_subplots(demand_params)\n",
    "            for b,a in zip(demand_params,dist_ax):\n",
    "                for tc in to_compare:\n",
    "                    c = expected_parameter_signs[b]\n",
    "                    rmse_df_multi.loc[tc].loc[idx[best_multi[tc],b]].plot.hist(\n",
    "                        ax=a,label=update_tc(tc),alpha=0.5,\n",
    "                        title=f'{nice_params[b]}'\n",
    "                    )\n",
    "                a.legend()\n",
    "                # ph.loc[idx[]].sample(ph.loc[ph.loc[idx[:,'RMSE']]])\n",
    "            dist_fig.tight_layout()\n",
    "            dist_fig.suptitle(f'{commodity.capitalize()}',y=1.05,weight='bold')\n",
    "            fig_dict['dist_fig_'+commodity] = dist_fig\n",
    "    if plot_cummin_rmse:\n",
    "        cummin_fig.tight_layout()\n",
    "        fig_dict['cummin_fig'] = cummin_fig\n",
    "    rmse_df_all.index = pd.MultiIndex.from_tuples(rmse_df_all.index)\n",
    "    return rmse_df_all, fig_dict\n",
    "\n",
    "rmse_df_all, fig_dict = compare_constrained_unconstrained_tuning(plot_cummin_rmse=True,\n",
    "                                         log_cummin_rmse=True,\n",
    "                                         plot_best_distributions=True,\n",
    "                                         demand_or_mining='demand', \n",
    "                                         to_compare = ['_constrain3','_unconstrain3',\n",
    "                                                       '_unconstrain1'],\n",
    "                                         dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4269066a-a44e-4cd4-95ad-772e2aeac6c2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse_df_all, fig_dict = compare_constrained_unconstrained_tuning(plot_cummin_rmse=True,\n",
    "                                         log_cummin_rmse=True,\n",
    "                                         plot_best_distributions=True,\n",
    "                                         demand_or_mining='mining', \n",
    "                                         to_compare = ['_constrain','_unconstrain'],\n",
    "                                         dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97bc736-10a2-4539-83ba-b0e8bec0195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle('data/tuned_rmse_df_out_mpce0_limit.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1367f69-03ae-44e8-b301-459b036004cb",
   "metadata": {},
   "source": [
    "## T-SNE and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de44b43-3591-43b1-8784-5e1df140ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dict, all_hyperparam, pca_results, tsne_results = plot_pca_tsne(\n",
    "    pca_results,tsne_results,demand_or_mining='demand',filename_modifier='_constrain', n_best=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4304aa26-ac2b-4ede-be37-97a6e578a3e5",
   "metadata": {},
   "source": [
    "## Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0302e6-5346-43a4-a324-6fdebceb2a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_of = Many()\n",
    "many_of.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt',commodities=['Al'],\n",
    "                              n_scenarios=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa7e4e-2e61-47e0-9d40-fc6a280bb6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_of.multi_scenario_results_formatted['Mine production']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95096488-6111-4353-b991-cd8ba73def74",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "many_b.processing_option='2040 ratio 2018 hist'\n",
    "feature_importance(many_b,commodity='nickel',objective='Ref. SD',recalculate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "v40 = many_b.multi_scenario_results['Mine production'].droplevel(1).loc[idx[:,:,2040]]\n",
    "v40 = np.log10(v40)\n",
    "h = many_b.multi_scenario_hyperparam.stack().droplevel(1).unstack().loc[idx[:,'sector_specific_dematerialization_tech_growth'],:]\\\n",
    "    .droplevel(1).stack()\n",
    "res = pd.concat([v40,h],axis=1,keys=['mine prod','tcrc elas'])\n",
    "res = res.loc['silver']\n",
    "fig,ax=easy_subplots(2,dpi=50)\n",
    "res.plot.scatter(y='mine prod',x='tcrc elas',ax=ax[0], logy=False)\n",
    "ax[0].set(\n",
    "    xlabel='Intensity decline per year',\n",
    "    ylabel=r'$log_{10}$(Mine production, 2040)',\n",
    "    title='Feature dependence, Silver\\nIntensity decline per year'\n",
    ")\n",
    "\n",
    "h = many_b.multi_scenario_hyperparam.stack().droplevel(1).unstack().loc[idx[:,'sector_specific_price_response'],:]\\\n",
    "    .droplevel(1).stack()\n",
    "res = pd.concat([v40,h],axis=1,keys=['mine prod','tcrc elas'])\n",
    "res = res.loc['silver']\n",
    "res.plot.scatter(y='mine prod',x='tcrc elas',ax=ax[1],c='tab:blue')\n",
    "ax[1].set(\n",
    "    xlabel='Intensity elasticity to price',\n",
    "    ylabel=r'$log_{10}$(Mine production, 2040)',\n",
    "    title='Feature dependence, Silver\\nIntensity response to price'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_b.multi_scenario_hyperparam.index.levels[2].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884d9a0-4a1a-4e05-8f37-acd35565b790",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_all_feature_importance_plots(many_norm,commodity='aluminum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9da53e-d146-46b1-afee-55bb7b5c1e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.feature_plot_figs[0].set_dpi(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e9fc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.integ.importances_df_reformed.sort_values(by='All',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e7244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b19619-88c1-43a3-a09d-d7c8e1a12265",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = commodity_level_feature_importance_heatmap(many.integ, objective=None, normalize=True, dpi=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34666d38-e513-44e1-980a-a3a86a36760a",
   "metadata": {},
   "source": [
    "## Trying SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ae1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruity_rename = {\n",
    "        'tcrc_elas_sd':'Concentrate price\\nS-D response',\n",
    "        'mine_cost_change_per_year':'Mine cost change per year',\n",
    "        'intensity_response_to_gdp':'Demand response to GDP',\n",
    "        'sector_specific_price_response':'Demand response to price',\n",
    "        'scrap_spread_elas_sd':'Scrap price S-D response',\n",
    "        'mine_cost_og_elas':'Mine cost change with ore grade',\n",
    "        'incentive_opening_probability':'Fraction of viable mines that open',\n",
    "        'refinery_capacity_fraction_increase_mining':'Refinery capacity S-D response',\n",
    "        'primary_commodity_price_elas_sd':'Metal price responsiveness',\n",
    "        'sector_dist_electrical':'Demand fraction in electrical sector',\n",
    "        'pri CU price elas':'Pri. refinery price response',\n",
    "        'sec CU price elas':'Sec. refinery concentrate price response',\n",
    "        'sec CU TCRC elas':'Sec. refinery price response',\n",
    "        'primary_oge_scale':'Rate of ore grade decline',\n",
    "        'primary_price_resources_contained_elas':'Exploration response to price',\n",
    "        'collection_elas_scrap_price':'Scrap collection response to price',\n",
    "        'sector_specific_dematerialization_tech_growth':'Demand response to time',\n",
    "        'sec ratio scrap spread elas':'Refinery scrap use response to price',\n",
    "        'incentive_mine_cost_change_per_year':'New mine cost change per year',\n",
    "        'initial_ore_grade_decline':'Reserves grade change per year',\n",
    "        'sector_dist_transport':'Transport fraction of demand',\n",
    "        'direct_melt_elas_scrap_spread':'Direct remelt scrap response to price'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580348a9",
   "metadata": {
    "code_folding": [
     160,
     188
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from skopt import Optimizer\n",
    "\n",
    "\n",
    "\n",
    "def get_case_study_data(many):\n",
    "    case_study = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx', index_col=0)\n",
    "    case_study = case_study[many.results.index.get_level_values(0).unique()]\n",
    "    precious = case_study.copy()[['Au','Ag']]\n",
    "    for i,j in zip(['industrial','transport','construction'],\n",
    "                   ['bar and coin','jewelry','industrial']):\n",
    "        cols = [k for k in precious.index if i in k]\n",
    "        rename_dict = dict(zip(cols, [k.replace(i,j) for k in cols]))\n",
    "        precious = precious.rename(rename_dict)\n",
    "    case_study = case_study.loc[:,~case_study.columns.isin(precious.columns)]\n",
    "    case_study = pd.concat([case_study, precious],axis=1)\n",
    "    case_vars = ['historical_growth_rate','china_fraction_demand','Recycling input rate, Global',\n",
    "                 'Secondary refinery fraction of recycled content, Global','primary_production_mean',\n",
    "                 'primary_ore_grade_mean','primary_commodity_price'\n",
    "                ] + [i for i in case_study.index if 'sector_dist' in i]\n",
    "    case_study = case_study.loc[case_vars].fillna(0)\n",
    "    return case_study\n",
    "\n",
    "def tune_ml_model(many, Regressor=None, objective='Refined price', plot=False):\n",
    "    \"\"\"\n",
    "    Takes in Many object, and creates attributes:\n",
    "    - best_model: best-performing ML model\n",
    "    - fi_data: StandardScaler correction of feature importance data\n",
    "    - fi_data_orig: uncorrected feature importance data\n",
    "    - X_train_df: dataframe corresponding to training data\n",
    "    - X_validate_df: dataframe corresponding to validation data\n",
    "    - X_test_df: dataframe corresponding to testing data\n",
    "    \n",
    "    Returns best_model\n",
    "    \"\"\"\n",
    "    # getting data from fruity_plots function\n",
    "    comms = many.results.index.get_level_values(0).unique()\n",
    "    many.objective = objective\n",
    "\n",
    "    # getting additional parameter data\n",
    "    addl_params = many.rmse_df.copy()\n",
    "    addl_params = addl_params.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n",
    "    addl_params = addl_params.loc[(addl_params.apply(lambda x: x-x.mean(), axis=1)>1e-7).any(axis=1)]\n",
    "    addl_params = addl_params.loc[[i for i in addl_params.index if i[1]!='primary_commodity_price' and \n",
    "                                  'duration' not in i[1] and 'pct_change' not in i[1] and 'hyperparam' not in i[1]]]\n",
    "    addl_params = addl_params.stack().unstack(1)\n",
    "\n",
    "    # getting y data\n",
    "    y_df = many.results[objective].copy()\n",
    "    y_df = y_df.loc[idx[:,:,2040]]\n",
    "    ph = many.objective_results_map\n",
    "    results_objective_map = dict(zip(ph.values(),ph.keys()))\n",
    "    y_df = y_df.unstack()\n",
    "    if objective!='Displacement':\n",
    "        string = results_objective_map[objective]\n",
    "        y_df = y_df.divide(\n",
    "            many.historical_data[string].loc[idx[:,2018]],\n",
    "            axis=0).stack()\n",
    "    if type(y_df)==pd.core.series.Series:\n",
    "        y_df = pd.DataFrame(y_df)\n",
    "    y_df = y_df.rename(columns={y_df.columns[0]:objective})\n",
    "    y_df = y_df.sort_index()\n",
    "    if True:\n",
    "        y_df = np.log10(y_df)\n",
    "\n",
    "    # getting case study data\n",
    "    case_study = get_case_study_data(many)\n",
    "    many.case_study_data = case_study.copy()\n",
    "    case_study = case_study.T.stack()\n",
    "    case_study = pd.concat([case_study for _ in many.rmse_df.columns], axis=1)\n",
    "    case_study.columns = many.rmse_df.columns\n",
    "    case_study = case_study.stack().unstack(1)\n",
    "    \n",
    "    # concatenating\n",
    "    ind = np.intersect1d(addl_params.index, y_df.index)\n",
    "    ind = np.intersect1d(ind, case_study.index)\n",
    "    fi_data = pd.concat([y_df, addl_params.loc[ind], case_study.loc[ind]],axis=1).fillna(0)\n",
    "    \n",
    "    # log columns:\n",
    "    cols = ['primary_commodity_price','primary_production_mean','primary_ore_grade_mean']\n",
    "    for q in cols:\n",
    "        if q in fi_data.columns:\n",
    "            fi_data.loc[:,q] = np.log10(fi_data[q])\n",
    "    \n",
    "    # removing outliers\n",
    "#     fi_data = fi_data.loc[abs(fi_data['Mean change normed'])<80]\n",
    "#     fi_data = fi_data.loc[(y_df<y_df.quantile(0.95)).iloc[:,0]]\n",
    "    many.fi_data_orig = fi_data.copy()\n",
    "\n",
    "    # applying standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    fi_std = scaler.fit_transform(fi_data.values)\n",
    "    fi_data = pd.DataFrame(fi_std,fi_data.index,fi_data.columns)\n",
    "    if 'region_specific_price_response' in fi_data.columns:\n",
    "        fi_data.drop(columns='region_specific_price_response',inplace=True)\n",
    "    many.fi_data = fi_data.copy()\n",
    "\n",
    "    # reformatting for split\n",
    "    X_df = fi_data[[i for i in fi_data.columns if i not in \n",
    "                    ['Mean change normed','Mean change','Scrap collected','RIR',objective]]]\n",
    "    y_df = fi_data[objective]\n",
    "    X, y = X_df.values, y_df.values\n",
    "\n",
    "    # train/test/validate split\n",
    "    many.X_train, many.X_test, many.y_train, many.y_test, many.ind_train, many.ind_test = train_test_split(\n",
    "        X, y, y_df.index, test_size=0.333,random_state=42)\n",
    "    many.X_validate, many.X_test, many.y_validate, many.y_test, many.ind_validate, many.ind_test = train_test_split(\n",
    "        many.X_test, many.y_test, many.ind_test,\n",
    "        test_size=0.5,\n",
    "        random_state=43)\n",
    "    many.X_train_df = pd.DataFrame(many.X_train, many.ind_train, X_df.columns)\n",
    "    many.X_test_df = pd.DataFrame(many.X_test, many.ind_test, X_df.columns)\n",
    "    many.X_validate_df = pd.DataFrame(many.X_validate, many.ind_validate, X_df.columns)\n",
    "    many.y_train_df = pd.Series(many.y_train, many.ind_train)\n",
    "    many.y_test_df = pd.Series(many.y_test, many.ind_test)\n",
    "    many.y_validate_df = pd.Series(many.y_validate, many.ind_validate)\n",
    "\n",
    "#     return many\n",
    "\n",
    "    # Tuning regression hyperparameters:\n",
    "    Regressor_dict = dict(zip(\n",
    "            ['RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor'],\n",
    "            [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor]))\n",
    "    if Regressor is None:\n",
    "        Regressor = [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor][0]\n",
    "    elif type(Regressor)==int:\n",
    "        Regressor = [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor][Regressor]\n",
    "    elif type(Regressor)==str and Regressor != 'all':\n",
    "        Regressor = Regressor_dict[Regressor]\n",
    "        \n",
    "    if type(Regressor)!=str:\n",
    "        get_best_mod(many, Regressor)\n",
    "        if plot:\n",
    "            fig,ax=plt.subplots()\n",
    "            ax.scatter(many.y_train_df.values, many.best_mod.predict(many.X_train_df.values), label='Train')\n",
    "            ax.scatter(many.y_validate_df.values, many.best_mod.predict(many.X_validate_df.values), label='Validate')\n",
    "            ax.scatter(many.y_test_df.values, many.best_mod.predict(many.X_test_df.values), label='Test')\n",
    "            ax.legend()\n",
    "            ax.set(xlabel='Actual', ylabel='Predicted')\n",
    "        return many.best_mod\n",
    "    elif Regressor == 'all':\n",
    "        many.best_mods = {}\n",
    "        for regr in Regressor_dict:\n",
    "            Regressor = Regressor_dict[regr]\n",
    "            best_mod = get_best_mod(many, Regressor)\n",
    "            many.best_mods[regr] = best_mod\n",
    "        if plot:\n",
    "            fig,ax=easy_subplots(list(many.best_mods.keys()))\n",
    "            for regr,a in zip(many.best_mods,ax):\n",
    "                a.scatter(many.y_train_df.values, \n",
    "                           many.best_mods[regr].predict(many.X_train_df.values), label='Train')\n",
    "                a.scatter(many.y_validate_df.values, \n",
    "                           many.best_mods[regr].predict(many.X_validate_df.values), label='Validate')\n",
    "                a.scatter(many.y_test_df.values, \n",
    "                           many.best_mods[regr].predict(many.X_test_df.values), label='Test')\n",
    "                a.legend()\n",
    "                r2_test = round(r2_test,3)\n",
    "                a.set(xlabel='Actual', ylabel='Predicted', title=f'{regr}\\nTest R2: {r2_test}')\n",
    "        return many.best_mods\n",
    "\n",
    "def get_best_mod(many, Regressor):\n",
    "    variables = {'n_estimators':(5,800), 'min_samples_leaf': (0.00000001,0.9999)}\n",
    "    opt = Optimizer([variables[i] for i in variables.keys()])\n",
    "\n",
    "    rmse_min = 1e4\n",
    "    best_params = None\n",
    "    best_mod = None\n",
    "    for i in range(20):\n",
    "        if i==0:\n",
    "            suggested = [800,1e-5]\n",
    "        else:\n",
    "            suggested = opt.ask()\n",
    "        regr = Regressor(random_state=0, n_estimators=suggested[0], min_samples_leaf=suggested[1])\n",
    "        regr.fit(many.X_train, many.y_train)\n",
    "        mse = mean_squared_error(regr.predict(many.X_validate), many.y_validate)**0.5\n",
    "        opt.tell(suggested, mse)\n",
    "        if mse<rmse_min:\n",
    "            rmse_min = mse\n",
    "            best_params = suggested\n",
    "            best_mod = regr\n",
    "        print('iteration:', i, suggested, mse)\n",
    "    print('Best params:', best_params)\n",
    "    print('Train R2:', r2_score(many.y_train, best_mod.predict(many.X_train)))\n",
    "    print('Validation R2:', r2_score(many.y_validate, best_mod.predict(many.X_validate)))\n",
    "    print('Test R2:', r2_score(many.y_test, best_mod.predict(many.X_test)))\n",
    "    many.best_mod = best_mod\n",
    "    return best_mod\n",
    "\n",
    "def from_fruity_feature_importance(many):\n",
    "    case_study_only = False\n",
    "    n_params = 15\n",
    "\n",
    "    importances = pd.Series(many.best_mod.feature_importances_, many.X_test_df.columns)\n",
    "    many.importances = importances.copy()\n",
    "    impor_heat = importances.copy()#.drop('Scrap demand')\n",
    "    actual_sum = impor_heat.sum()\n",
    "    ind = np.intersect1d(impor_heat.index, many.case_study_data.index)\n",
    "    if case_study_only:\n",
    "        impor_heat = impor_heat.loc[ind]\n",
    "#     impor_heat = impor_heat.div(actual_sum,axis=1)\n",
    "    impor_heat /= actual_sum\n",
    "    impor_heat = impor_heat.sort_values(ascending=False).head(n_params)\n",
    "    impor_heat = impor_heat.rename(index=make_parameter_names_nice(impor_heat.index))\n",
    "#     impor_heat = impor_heat.stack().reset_index()\n",
    "    impor_heat = impor_heat.reset_index()\n",
    "    impor_heat = impor_heat.rename(columns={'index':'Parameter',0:'Feature importance'})\n",
    "    # sns.heatmap(impor_heat, ax=ax,\n",
    "    #             yticklabels=True,\n",
    "    #            )\n",
    "    orient = 'v'\n",
    "    figsize = [8,5+impor_heat['Parameter'].unique().shape[0]*11/40]\n",
    "    if orient=='h':\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        kwargs = {'y':'Parameter', 'x':'Feature importance', 'orient':orient}\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=figsize[::-1])\n",
    "        kwargs = {'x':'Parameter', 'y':'Feature importance', 'orient':orient}\n",
    "    sns.barplot(impor_heat, ax=ax, **kwargs)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90);\n",
    "    ax.legend(title='Scrap scenario', alignment='left')\n",
    "\n",
    "tune_ml_model(many_b, objective = 'Mine production', Regressor='all', plot=True)\n",
    "from_fruity_feature_importance(many_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17828b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1c9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax=plt.subplots()\n",
    "# ax.scatter(many_b.y_train_df.values, many_b.best_mod.predict(many_b.X_train_df.values), label='Train')\n",
    "# ax.scatter(many_b.y_validate_df.values, many_b.best_mod.predict(many_b.X_validate_df.values), label='Validate')\n",
    "# ax.scatter(many_b.y_test_df.values, many_b.best_mod.predict(many_b.X_test_df.values), label='Test')\n",
    "# ax.legend()\n",
    "# ax.set(xlabel='Actual', ylabel='Predicted')\n",
    "\n",
    "fig,ax=easy_subplots(list(many_b.best_mods.keys()),use_subplots=True,sharey=True)\n",
    "for regr,a in zip(many_b.best_mods,ax):\n",
    "    a.scatter(many_b.y_train_df.values, \n",
    "               many_b.best_mods[regr].predict(many_b.X_train_df.values), label='Train')\n",
    "    a.scatter(many_b.y_validate_df.values, \n",
    "               many_b.best_mods[regr].predict(many_b.X_validate_df.values), label='Validate')\n",
    "    a.scatter(many_b.y_test_df.values, \n",
    "               many_b.best_mods[regr].predict(many_b.X_test_df.values), label='Test')\n",
    "    a.legend()\n",
    "    r2_test = r2_score(many_b.y_test_df.values, \n",
    "               many_b.best_mods[regr].predict(many_b.X_test_df.values))\n",
    "    r2_test = round(r2_test,3)\n",
    "    a.set(xlabel='Actual', ylabel='Predicted', title=f'{regr}\\nTest R2: {r2_test}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f776f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = ExtraTreesRegressor(random_state=0, n_estimators=800, min_samples_leaf=1e-8)\n",
    "regr.fit(many_b.X_train, many_b.y_train)\n",
    "print('Train R2:', r2_score(many_b.y_train, regr.predict(many_b.X_train)))\n",
    "print('Validation R2:', r2_score(many_b.y_validate, regr.predict(many_b.X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859705fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "import fasttreeshap as shap # works exactly the same for the TreeExplainer function, should be fasterjust_use_et = True\n",
    "for many in [many_b]:\n",
    "    if just_use_et:\n",
    "        regr = regr = ExtraTreesRegressor(random_state=0, n_estimators=800, min_samples_leaf=1e-8)\n",
    "        regr.fit(many.X_train, many.y_train)\n",
    "    else:\n",
    "        regr = many.best_mods['ExtraTreesRegressor']\n",
    "    explainer = shap.TreeExplainer(regr)\n",
    "    shap_df = many.X_test_df.copy()\n",
    "    to_add = many.fi_data.loc[idx[:,:25],:].index\n",
    "    to_add = to_add[~to_add.isin(many.X_test_df.index)]\n",
    "    shap_df = pd.concat([shap_df,many.fi_data.loc[to_add]])\n",
    "    shap_values_object = explainer(shap_df.values)\n",
    "    shap_values = shap_values_object.values\n",
    "    shap_values_df = pd.DataFrame(shap_values, shap_df.index, shap_df.columns)\n",
    "    many.shap_values_df = shap_values_df.copy()\n",
    "    \n",
    "#     explainer_copy = shap.TreeExplainer(regr)\n",
    "#     shap_df = many.X_test_df.copy()\n",
    "#     to_add = many.fi_data.loc[idx[:,:25],:].index\n",
    "#     to_add = to_add[~to_add.isin(many.X_test_df.index)]\n",
    "#     shap_df = pd.concat([shap_df,many.fi_data.loc[to_add]])\n",
    "#     shap_interaction_values = explainer_copy.shap_interaction_values(shap_df.values)\n",
    "#     x = shap_interaction_values\n",
    "#     if many.commodity!=None:\n",
    "#         many.shap_interaction_df = pd.DataFrame(x.reshape(x.shape[0]*x.shape[1],x.shape[2]), shap_df.index, shap_df.columns)\n",
    "#     else:\n",
    "#         many.shap_interaction_df = pd.DataFrame(x.reshape(x.shape[0]*x.shape[1],x.shape[2]), shap_df.index, shap_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "many = many_b\n",
    "many.shap_values_df\n",
    "X_test_df_orig = many.fi_data_orig.loc[many.shap_values_df.index]\n",
    "X_test_df = many.fi_data.loc[many.shap_values_df.index]\n",
    "nice_dict = make_parameter_names_nice(many.fi_data.columns)\n",
    "slopes = pd.Series(np.nan, many.shap_values_df.columns)\n",
    "n_col = int(len(many.shap_values_df.columns)/2)\n",
    "fig,ax = easy_subplots(many.shap_values_df.columns[:n_col])\n",
    "for col,a in zip(many.shap_values_df.columns[:n_col],ax):\n",
    "    a.scatter(x=X_test_df_orig[col], y=many.shap_values_df[col])\n",
    "    a.set(xlabel='Parameter values', ylabel='SHAP values', title=nice_dict[col])\n",
    "    if col in ['intensity_response_to_gdp','sector_specific_price_response']:\n",
    "        a.set(ylim=(-.2,.2))\n",
    "    m = sm.GLS(many.shap_values_df[col], sm.add_constant(X_test_df[col])).fit(cov_type='HC3')\n",
    "    slopes.loc[col] = m.params[col]\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(200)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "fig,ax = easy_subplots(many.shap_values_df.columns[n_col:])\n",
    "for col,a in zip(many.shap_values_df.columns[n_col:],ax):\n",
    "    a.scatter(x=X_test_df_orig[col], y=many.shap_values_df[col])\n",
    "    a.set(xlabel='Parameter values', ylabel='SHAP values', title=nice_dict[col])\n",
    "    if col in ['intensity_response_to_gdp','sector_specific_price_response']:\n",
    "        a.set(ylim=(-.2,.2))\n",
    "    m = sm.GLS(many.shap_values_df[col], sm.add_constant(X_test_df[col])).fit(cov_type='HC3')\n",
    "    slopes.loc[col] = m.params[col]\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(200)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235a17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adaeb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodities = ['Cu','Ni']\n",
    "annotate=True\n",
    "many=many_b\n",
    "fig, ax = easy_subplots(commodities, use_subplots=True, sharex=True, width_scale=0.9, height_scale=1.1)\n",
    "for commodity,a in zip(commodities, ax):\n",
    "#     most_important_params = many.importances.sort_values(ascending=False).head(8).index\n",
    "#     for_waterfall = many.shap_values_df.sort_index().loc[commodity].loc[:,most_important_params]\n",
    "    for_waterfall = many.shap_values_df.sort_index().loc[commodity].loc[:25].median()\n",
    "    if a==ax[0]:\n",
    "        most_important_params = abs(for_waterfall).sort_values().tail(8).index\n",
    "    for_waterfall = for_waterfall[most_important_params]\n",
    "    for_waterfall.rename(fruity_rename, inplace=True)\n",
    "    nice_dict = make_parameter_names_nice(for_waterfall.index)\n",
    "    for_waterfall.rename(nice_dict,inplace=True)\n",
    "    for_waterfall.rename(\n",
    "        {'Secondary refinery fraction of recycled content, global':\n",
    "         'Secondary refinery fraction\\nof recycled content',\n",
    "         'Direct melt fraction elasticity to scrap spread':\n",
    "         'Direct melt fraction\\nelasticity to scrap spread',\n",
    "         'Incentive mine cost change per year':\n",
    "         'Incentive mine cost\\nchange per year',\n",
    "        }, inplace=True)\n",
    "#     for_waterfall.rename(dict(zip(for_waterfall.index,\n",
    "#                                   [i.replace('\\n',' ') for i in for_waterfall.index])),inplace=True)\n",
    "    for_waterfall = for_waterfall.reset_index()\n",
    "    for_waterfall['>0'] = for_waterfall[0]>0\n",
    "    important_hyperparam = many.fi_data_orig.sort_index().loc[\n",
    "        commodity].loc[:,most_important_params].astype(float)\n",
    "    important_hyperparam = important_hyperparam.mean()\n",
    "    \n",
    "    \n",
    "    sns.barplot(for_waterfall.reset_index(), ax=a, x=0, y='index', hue='>0', dodge=False, \n",
    "                palette=['blue','red'])\n",
    "    a.grid(axis='x')\n",
    "#     a.set_xlim(-5.05,11)\n",
    "    a.legend('')\n",
    "    yvals = a.get_yticks()\n",
    "    xmin,xmax = a.get_xlim()\n",
    "    # Annotation:\n",
    "    if annotate:\n",
    "        for y,i,x in zip(yvals, important_hyperparam.round(2), for_waterfall[0]):\n",
    "            annot_color = 'k'\n",
    "            if False:\n",
    "                if x<xmax*0.4 and x>0: \n",
    "                    x=x*2+xmax*0.4\n",
    "                    annot_color='k'\n",
    "                elif x>xmin*0.4 and x<0: \n",
    "                    x=x*2-xmax*0.4\n",
    "                    annot_color='k'\n",
    "            elif False:\n",
    "                if a==ax[0]:\n",
    "                    x = 9 \n",
    "                elif a==ax[1]:\n",
    "                    x = -3\n",
    "            else:\n",
    "                v = 0.02\n",
    "                if x<0:\n",
    "                    x = v\n",
    "                elif x>0:\n",
    "                    x = -v\n",
    "            a.annotate(i, (x, y), horizontalalignment='center', verticalalignment='center', color=annot_color)\n",
    "\n",
    "    a.tick_params(axis='y', which='both', left=True, right=True)\n",
    "    a.set(title=many.element_commodity_map[commodity], \n",
    "          xlabel='Change in metal price ratio\\ndue to parameter value')\n",
    "    a.set_ylabel('')\n",
    "    a.set_xticks([-0.10,-0.05,0,0.05])\n",
    "\n",
    "ax[1].set_yticklabels([])\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecef1b2",
   "metadata": {},
   "source": [
    "### For defense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6a137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mining_shap_values_df = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_b_shap_mine_production.csv', index_col=[0,1])\n",
    "price_shap_values_df = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_b_shap_price.csv', index_col=[0,1])\n",
    "many_b.fi_data = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_b_fi_data.csv', index_col=[0,1])\n",
    "many_b.fi_data_orig = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_b_fi_data_orig.csv', index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417de08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2677e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = price_shap[['shap']].sort_index()\n",
    "x = xy.copy()\n",
    "x['x'] = xy.index\n",
    "x['x2'] = x.x**2\n",
    "x['x3'] = x.x**3\n",
    "x.drop(columns='shap',inplace=True)\n",
    "y = xy.values\n",
    "import statsmodels.api as sm\n",
    "m = sm.GLS(y,sm.add_constant(x)).fit(cov_type='HC3')\n",
    "v = m.params\n",
    "plt.plot(x.index, v.const+v.x*x.x+v.x2*x.x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb74a32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter    \n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "params = ['incentive_mine_cost_change_per_year']\n",
    "param = params[0]\n",
    "\n",
    "mine_cost_shap_m = mining_shap_values_df.loc[:,param]\n",
    "mine_cost_shap_p = price_shap_values_df.loc[:,param]\n",
    "# mine_cost_shap.groupby(level=0).mean()\n",
    "# mine_cost = many_b.case_study_data.loc['Secondary refinery fraction of recycled content, Global']\n",
    "# plt.scatter(mine_cost, mine_cost_shap.groupby(level=0).median())\n",
    "mine_cost = many_b.fi_data_orig.loc[mine_cost_shap_m.index,param]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[6.75,5.7])\n",
    "# ax.scatter(mine_cost, mine_cost_shap_p, color=plt.color_sequences['Dark2'][1])\n",
    "\n",
    "width, poly, uni_size, middle = 400, 3, 100, 0.8\n",
    "\n",
    "mine_shap = pd.concat([mine_cost, mine_cost_shap_m],axis=1,keys=['data','shap']).set_index('data')\n",
    "mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).mean().plot(ax=ax, color='#167355')\\\n",
    "    .grid(axis='x')\n",
    "# ax.scatter(mine_cost, mine_cost_shap_m)\n",
    "lb = mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile((1-middle)/2)\n",
    "ub = mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile(1-(1-middle)/2)\n",
    "ax.fill_between(lb.index, lb, ub, color='#167355', alpha=0.4)\n",
    "# mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).mean().plot(ax=ax)\n",
    "# mine_filt = mine_shap[['shap']].sort_index().apply(savgol_filter, window_length=width, polyorder=poly)\n",
    "# mine_filt.plot(ax=ax,color='k')\n",
    "# mine_filt = mine_shap[['shap']].sort_index().apply(uniform_filter1d, size=uni_size)\n",
    "# mine_filt.plot(ax=ax,color='k')\n",
    "ax.set(xlim=(-11.690742879493992, 11.800456526405199), ylim=(-0.31672935258319007, 0.15963262004037718),\n",
    "      xlabel=None)\n",
    "fig.set_dpi(300)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[6.75,5.7])\n",
    "price_shap = pd.concat([mine_cost, mine_cost_shap_p],axis=1,keys=['data','shap']).set_index('data')\n",
    "# ax.scatter(mine_cost, mine_cost_shap_p)\n",
    "price_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).mean().plot(ax=ax, color='#484671')\\\n",
    "    .grid(axis='x')\n",
    "lb = price_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile((1-middle)/2)\n",
    "ub = price_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile(1-(1-middle)/2)\n",
    "ax.fill_between(lb.index, lb, ub, color='#484671', alpha=0.4)\n",
    "# price_filt = price_shap[['shap']].sort_index().apply(savgol_filter, window_length=width, polyorder=poly)\n",
    "# price_filt.plot(ax=ax,color='k')\n",
    "# price_filt = price_shap[['shap']].sort_index().apply(uniform_filter1d, size=uni_size)\n",
    "# price_filt.plot(ax=ax,color='k')\n",
    "# ax.plot(x.index, v.const+v.x*x.x+v.x2*x.x2, color='k')\n",
    "# mine_cost_g = mine_cost.groupby(level=0).median()\n",
    "# mine_cost_shap_g = mine_cost_shap.groupby(level=0).median()\n",
    "# ax.set(xlim=(-11.690742879493992, 11.800456526405199), ylim=(-0.36736509335182116, 0.18857329350258228),\n",
    "#       xlabel=None)\n",
    "# if want same as above:\n",
    "ax.set(xlim=(-11.690742879493992, 11.800456526405199), ylim=(-0.31672935258319007, 0.15963262004037718),\n",
    "      xlabel=None)\n",
    "fig.set_dpi(300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd5f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba71b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_ref_frac_shap = many_b.shap_values_df.loc[:,'Secondary refinery fraction of recycled content, Global']\n",
    "# sec_ref_frac_shap.groupby(level=0).mean()\n",
    "# sec_ref_frac = many_b.case_study_data.loc['Secondary refinery fraction of recycled content, Global']\n",
    "# plt.scatter(sec_ref_frac, sec_ref_frac_shap.groupby(level=0).median())\n",
    "sec_ref_frac = many_b.fi_data_orig.loc[sec_ref_frac_shap.index,\n",
    "                                       'Secondary refinery fraction of recycled content, Global']\n",
    "# plt.scatter(sec_ref_frac, sec_ref_frac_shap)\n",
    "sec_ref_frac_g = sec_ref_frac.groupby(level=0).median()\n",
    "sec_ref_frac_shap_g = sec_ref_frac_shap.groupby(level=0).median()\n",
    "\n",
    "fig,ax=plt.subplots(figsize=(6,4.5))\n",
    "\n",
    "y = do_a_regress(1-sec_ref_frac_g, sec_ref_frac_shap_g, plot=False)[0]\n",
    "x = np.linspace(0.05,.95)\n",
    "y = y['const']+y['slope']*x\n",
    "ax.plot(x,y, linestyle='--', color='k', alpha=0.3,zorder=1)\n",
    "\n",
    "ax.scatter(1-sec_ref_frac_g, sec_ref_frac_shap_g, s=500, linewidths=2, edgecolors='k',\n",
    "           c=np.linspace(0,1,len(sec_ref_frac_g)), cmap='Dark2',zorder=2)\n",
    "\n",
    "ax.set(xlabel='Direct remelt fraction of\\nscrap processing', ylabel='Parameter effect on\\n2040/2019 price ratio',\n",
    "       title='Direct remelt fraction\\neffect on price risk', ylim=np.array(ax.get_ylim())*np.array([1.07,1.07])\n",
    ")\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(30)\n",
    "fig.patch.set_alpha(0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "pd.concat([\n",
    "    1-sec_ref_frac_g,\n",
    "    sec_ref_frac_shap_g\n",
    "],axis=1,keys=['Fraction value','SHAP value']).sort_values(by='SHAP value',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082331b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ab7d4-b8cc-4865-8d84-1cd2650a94ee",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    # warnings.simplefilter('error')\n",
    "    # sh = SHAP(many_norm.demand,commodity='aluminum',split_frac=0.9, use_train_data=True, standard_scaler=True, objective='RMSE')\n",
    "    sh = SHAP(many_sg.integ,commodity='lead',split_frac=0.6, use_train_data=False, standard_scaler=True, objective=None)\n",
    "    sh.initialize()\n",
    "    sh.plot_shap(dpi=50,highlight_best=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e73fb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh = SHAP(many_sg.integ,commodity='lead',split_frac=0.6, use_train_data=False, standard_scaler=True, objective=None)\n",
    "sh.gamma_facet()\n",
    "fig=plot_sri_matrices(sh,n_param=3,width_scale=1.2,dpi=200)\n",
    "fig.axes[0].set_title(f'Synergy matrix, {sh.commodity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf03a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sh.summary_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf46baf-dd1a-4952-b079-4c038a02cce4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_all_sri_matrices(many_sg.integ, commodities=['aluminum'],\n",
    "                      standard_scaler=True,\n",
    "                      dummies=False, split_frac=0.6, \n",
    "                      use_train_data=False, objective=None,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4825a2dd-27ca-426e-8338-2b541c55d48d",
   "metadata": {},
   "source": [
    "## Integrated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b825c1-43cb-424d-a99d-eae76eaaaee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca59ffc-a5e4-4020-b7c0-f414bf83e559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# many.mining.mine_supply_sorted[0].unstack(0)-many.mining.historical_data['Primary supply'].unstack(0).loc[2001:2019]\n",
    "integ = Many()\n",
    "integ.get_variables('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb727d-1449-468b-89f5-d76f0f1b8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([\n",
    "    integ.hyperparam.loc[idx[:,'primary_production'],0].droplevel(1),\n",
    "    integ.historical_data['Primary supply'].unstack()[2001]\n",
    "],axis=1)\n",
    "check = integ.results_sorted['Conc. supply'].sort_index().loc[idx[:,0,:2019]].dropna().droplevel(1).unstack(0)\n",
    "fig,ax=easy_subplots(check.columns)\n",
    "for i,a in zip(check.columns,ax):\n",
    "    check[i].plot(ax=a,title=i)\n",
    "    a.hlines(integ.historical_data['Primary supply'].unstack()[2001][i],check.index[0],check.index[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4df2d5-d62c-4fe7-9c99-b14b44bbd7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit(simulated, historical, param):\n",
    "    simulated, historical = simulated.copy(), historical.copy()\n",
    "    if np.any([i in param.lower() for i in ['price','tcrc','spread']]):\n",
    "        unit = 'USD/t'\n",
    "    elif 'CU' in param or 'SR' in param:\n",
    "        unit = 'fraction'\n",
    "    else:\n",
    "        min_simulated = abs(simulated).min() if len(simulated.shape)<=1 else abs(simulated).min().min()\n",
    "        max_simulated = abs(simulated).max() if len(simulated.shape)<=1 else abs(simulated).max().max()\n",
    "        mean_simulated = abs(simulated).mean() if len(simulated.shape)<=1 else abs(simulated).mean().mean()\n",
    "        min_historical = abs(historical).min() if len(historical.shape)<=1 else abs(historical).min().min()\n",
    "        max_historical = abs(historical).max() if len(historical.shape)<=1 else abs(historical).max().max()\n",
    "        mean_historical = abs(historical).mean() if len(historical.shape)<=1 else abs(historical).mean().mean()\n",
    "        if np.mean([mean_historical,mean_simulated])>1000:\n",
    "            historical /= 1000\n",
    "            simulated /= 1000\n",
    "            unit = 'Mt'\n",
    "        elif np.mean([mean_historical,mean_simulated])<1:\n",
    "            historical *= 1000\n",
    "            simulated *= 1000\n",
    "            unit = 't'\n",
    "        else:\n",
    "            unit = 'kt'\n",
    "    return {'simulated':simulated, 'historical':historical, 'unit':unit}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c25c3-0354-49ed-9ab9-f09e4292c0b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "historical = integ.historical_data.copy().sort_index().loc[idx[:,2001:2019],:]\n",
    "simulated = integ.results_sorted.sort_index().copy().loc[idx[:,:,2001:2019],:]\n",
    "n_best = 10\n",
    "commodities = simulated.index.get_level_values(0).unique()\n",
    "hist_sim_map = {'Total demand':'Total demand', 'Primary supply':'Conc. supply', 'Primary commodity price':'Refined price'}\n",
    "for commodity in commodities:\n",
    "    fig,ax = easy_subplots(3)\n",
    "    for hist_param,a in zip(hist_sim_map.keys(),ax):\n",
    "        sim_param = hist_sim_map[hist_param]\n",
    "        hist = historical.loc[commodity][hist_param]\n",
    "        sim  = simulated.loc[commodity][sim_param]\n",
    "        unit_dict = get_unit(simulated=sim, historical=hist, param=hist_param)\n",
    "        hist = unit_dict['historical']\n",
    "        sim = unit_dict['simulated']\n",
    "        sim_others = sim.copy().drop(0).unstack(0).loc[:,:n_best]\n",
    "        sim = sim.loc[0]\n",
    "        unit = unit_dict['unit']\n",
    "        a.plot(sim_others, linewidth=2,color='gray',alpha=0.5)\n",
    "        hist_line = a.plot(hist,label='Historical',linewidth=6,color='k')[0]\n",
    "        sim_line = a.plot(sim,label='Simulated', linewidth=6,color='tab:blue')[0]\n",
    "        mins = min(hist.min(),sim.min())*0.95\n",
    "        maxs = max(hist.max(),sim.max())*1.05\n",
    "        a.set(ylabel=f'{hist_param} ({unit})', ylim=(mins,maxs))\n",
    "        a.set_title(f'Best {hist_param.lower()}',weight='bold')\n",
    "        a.legend([hist_line,sim_line],['Historical','Simulated'])\n",
    "        \n",
    "        \n",
    "    fig.suptitle(commodity.capitalize(), weight='bold',y=0.94,x=0.515)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5febb77b-0e60-4f96-b52f-9a386d8bf398",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2199d979-82ff-47c5-98dc-93680d74dedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sd_one_commodity(results_sorted,commodity):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('error')\n",
    "        results = results_sorted.loc[idx[commodity,0,2001:2019],:].droplevel([0,1])\n",
    "        variables = ['Total','Conc','Ref.','Scrap','Spread','TCRC','Refined','CU','SR','Direct','Mean total','mine grade','Conc. SD','Ref. SD','Scrap SD']\n",
    "        fig,ax=easy_subplots(variables)\n",
    "        for var,a in zip(variables,ax):\n",
    "            parameters = [i for i in results.columns if var in i and ('SD' not in i or 'SD' in var)]\n",
    "            res = results[parameters].loc[2001:]\n",
    "            if res.shape[1]==1: res = res.iloc[:,0]\n",
    "            out_dict = get_unit(res,res,parameters[0])\n",
    "            res, unit = out_dict['simulated'], out_dict['unit']\n",
    "            param_str = ', '.join(parameters) if len(', '.join(parameters))<30 else ',\\n'.join(parameters)\n",
    "            res.plot(ax=a,title=f'{param_str} ({unit})')\n",
    "        fig.tight_layout()\n",
    "commodity = commodities[0]\n",
    "print(commodity)\n",
    "plot_sd_one_commodity(simulated,'aluminum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c04eb36-4579-4a66-8fdc-f270b9097a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "integ.rmse_df_sorted.loc[commodity][0]\n",
    "big_df = pd.read_pickle('data/steel_run_hist_all_3p.pkl')\n",
    "r = big_df.loc['rmse_df'].iloc[-1]\n",
    "r.index = pd.MultiIndex.from_tuples(r.index)\n",
    "r = r.loc[:,0]\n",
    "r = r.unstack(0)\n",
    "(r.drop('score').iloc[6:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104c15f8-5ac4-4a86-8bd4-a5f8dc6371c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = pd.concat([big_df.loc['hyperparam',i]['Value'] for i in big_df.columns],keys=big_df.columns,axis=1)\n",
    "hyp.loc[hyp.index.isin(r.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6cbd11-7ecd-489e-9eb4-7f52fb950ac2",
   "metadata": {},
   "source": [
    "Checking runs as they go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a82b071-cb0e-4267-8498-4921fa33dd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = Individual(material='Steel',filename='data/steel_run_hist_all_3p2.pkl')\n",
    "indiv.get_results_hyperparam_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10214020-3784-4580-a9a2-9c1fb711103f",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_data = indiv.historical_data\n",
    "sorted_index = indiv.rmse_df.loc['score'].sort_values(ascending=True).index\n",
    "rmse_df_sorted = indiv.rmse_df.loc[:,sorted_index].T.reset_index(drop=True).T\n",
    "results_sorted = indiv.results.loc[idx[sorted_index,:],:].unstack().reset_index(drop=True).stack()\n",
    "\n",
    "\n",
    "historical = historical_data.copy().sort_index().loc[2001:2019,:]\n",
    "simulated = results_sorted.sort_index().copy().loc[idx[:,2001:2019],:]\n",
    "n_best = 10\n",
    "commodities = simulated.index.get_level_values(0).unique()\n",
    "hist_sim_map = {'Total demand':'Total demand', 'Primary supply':'Conc. supply', 'Primary commodity price':'Refined price'}\n",
    "fig,ax = easy_subplots(3)\n",
    "for hist_param,a in zip(hist_sim_map.keys(),ax):\n",
    "    sim_param = hist_sim_map[hist_param]\n",
    "    hist = historical[hist_param]\n",
    "    sim  = simulated[sim_param]\n",
    "    unit_dict = get_unit(simulated=sim, historical=hist, param=hist_param)\n",
    "    hist = unit_dict['historical']\n",
    "    sim = unit_dict['simulated']\n",
    "    sim_others = sim.copy().drop(0).unstack(0).loc[:,:n_best]\n",
    "    sim = sim.loc[0]\n",
    "    unit = unit_dict['unit']\n",
    "    a.plot(sim_others, linewidth=2,color='gray',alpha=0.5)\n",
    "    hist_line = a.plot(hist,label='Historical',linewidth=6,color='k')[0]\n",
    "    sim_line = a.plot(sim,label='Simulated', linewidth=6,color='tab:blue')[0]\n",
    "    mins = min(hist.min(),sim.min())*0.95\n",
    "    maxs = max(hist.max(),sim.max())*1.05\n",
    "    a.set(ylabel=f'{hist_param} ({unit})', ylim=(mins,maxs))\n",
    "    a.set_title(f'Best {hist_param.lower()}',weight='bold')\n",
    "    a.legend([hist_line,sim_line],['Historical','Simulated'])\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d31871-e34e-41d9-b87f-80087cefc63c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce90a4-f4f7-4880-9dc6-245b3152a12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9843696-abbf-47ed-8aef-1d1c33aa8d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist-pareto_df[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3b9af-c15a-4d3e-bcba-cb6192f533b2",
   "metadata": {},
   "source": [
    "## Determining which hyperparameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60ee2d2-2b2e-4fec-9e78-193d88b32b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_scenarios_no_pareto_way(many,n=10,verbosity=0):\n",
    "    indiv_str_list = [i for i in dir(many.integ) if 'indiv' in i]\n",
    "    indiv_list = [getattr(many.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "    for indiv,string in zip(indiv_list,indiv_str_list):\n",
    "        indiv.commodity = string.split('_')[1]\n",
    "        indiv.find_pareto(plot=False)\n",
    "        hist = indiv.historical_data.loc[2001].loc[['Total demand','Primary supply','Primary commodity price']]\n",
    "        hist = hist.rename(dict(zip(hist.index, [i+' RMSE' for i in hist.index])))\n",
    "        # if not hasattr(indiv,'pareto_ind'):\n",
    "        pareto_df = indiv.rmse_df.copy().loc[[i for i in indiv.rmse_df.index if 'RMSE' in i]].astype(float)\n",
    "        quantile = 1\n",
    "        pareto_way = pareto_df.columns\n",
    "        change = 0.1\n",
    "        sign = len(pareto_way)>n\n",
    "        not_sign = not sign\n",
    "        while len(pareto_way)!=n and quantile<=1 and abs(change)>1e-8:\n",
    "            quantile -= change\n",
    "            pareto_way = pareto_df.apply(lambda x: x-hist*quantile<0).all()\n",
    "            pareto_way = pareto_df.loc[:,pareto_way].columns\n",
    "            sign = len(pareto_way)>n\n",
    "            if sign==not_sign:\n",
    "                not_sign = not sign\n",
    "                change /= -10\n",
    "        if verbosity>1: print('\\t',indiv.commodity,len(pareto_df.columns),len(pareto_way))\n",
    "        pareto_df_sub = pareto_df[pareto_way]\n",
    "        pareto_hyperparam = indiv.rmse_df[pareto_way].drop(pareto_df.index).drop([i for i in indiv.rmse_df.index if 'R2' in i or i=='score'])\n",
    "        indiv.no_pareto_hyperparam_subset = pareto_hyperparam.copy()\n",
    "        indiv.no_pareto_way = pareto_way\n",
    "        \n",
    "def find_best_scenarios_pareto_way(many,n=10,verbosity=0):\n",
    "    indiv_str_list = [i for i in dir(many.integ) if 'indiv' in i]\n",
    "    indiv_list = [getattr(many.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "    for indiv,string in zip(indiv_list,indiv_str_list):\n",
    "        indiv.commodity = string.split('_')[1]\n",
    "        indiv.find_pareto(plot=False)\n",
    "        # if not hasattr(indiv,'pareto_ind'):\n",
    "        indiv.pareto_ind = indiv.rmse_df.loc['is_pareto'].loc[indiv.rmse_df.loc['is_pareto']].index\n",
    "        pareto_df = indiv.rmse_df.copy()[indiv.pareto_ind].loc[[i for i in indiv.rmse_df.index if 'RMSE' in i]].astype(float)\n",
    "        hist = indiv.historical_data.loc[2001].loc[['Total demand','Primary supply','Primary commodity price']]\n",
    "        hist = hist.rename(dict(zip(hist.index, [i+' RMSE' for i in hist.index])))\n",
    "        quantile = 1\n",
    "        pareto_way = pareto_df.columns\n",
    "        change = 0.1\n",
    "        sign = len(pareto_way)>n\n",
    "        not_sign = not sign\n",
    "        if sign:\n",
    "            while len(pareto_way)!=n and quantile<=1 and quantile>0 and abs(change)>1e-8:\n",
    "                # pareto_way = pareto_df.apply(lambda x: x<pareto_df.quantile(quantile,axis=1)).all()\n",
    "                pareto_way = pareto_df.apply(lambda x: x-hist*quantile<0).all()\n",
    "                pareto_way = pareto_df.loc[:,pareto_way].columns\n",
    "                sign = len(pareto_way)>n\n",
    "                if sign==not_sign:\n",
    "                    not_sign = not sign\n",
    "                    change /= -10\n",
    "                quantile -= change\n",
    "        if verbosity>1: print('\\t',indiv.commodity,len(pareto_way))\n",
    "        pareto_df_sub = pareto_df[pareto_way]\n",
    "        pareto_hyperparam = indiv.rmse_df[pareto_way].drop(pareto_df.index).drop([i for i in indiv.rmse_df.index if 'R2' in i or i=='score'])\n",
    "        indiv.pareto_hyperparam_subset = pareto_hyperparam.copy()\n",
    "        indiv.pareto_way = pareto_way\n",
    "        \n",
    "def find_best_scenarios_weighting_all(many, verbosity=0):\n",
    "    indiv_str_list = [i for i in dir(many.integ) if 'indiv' in i]\n",
    "    indiv_list = [getattr(many.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "    for indiv,string in zip(indiv_list,indiv_str_list):\n",
    "        commodity = string.split('_')[1]\n",
    "        indiv.commodity = commodity\n",
    "        if verbosity>1: print('\\t',commodity)\n",
    "        combos = list(permutations(['price','mining','demand'],2))\n",
    "        vals = np.arange(-6,6,0.5)\n",
    "        vals = [10.**i for i in vals]\n",
    "        combo_mult = list(set(list(permutations(np.repeat(vals,2),2))))\n",
    "        idx_list = [indiv.rmse_df.astype(float).loc['score'].idxmin()]\n",
    "        output = pd.DataFrame(indiv.rmse_df.loc[['Primary commodity price RMSE','Primary supply RMSE','Total demand RMSE'],idx_list[0]])\n",
    "        output.loc['Price multiplier',idx_list[0]] = 1\n",
    "        output.loc['Mining multiplier',idx_list[0]] = 1\n",
    "        output.loc['Demand multiplier',idx_list[0]] = 1\n",
    "        for combo in combos:\n",
    "            for mult in combo_mult:\n",
    "                price = indiv.rmse_df.copy().loc['Primary commodity price RMSE']\n",
    "                mining= indiv.rmse_df.copy().loc['Primary supply RMSE']\n",
    "                demand= indiv.rmse_df.copy().loc['Total demand RMSE']\n",
    "\n",
    "                def get_mul(string,mult,combo):\n",
    "                    if string in combo:\n",
    "                        return np.array(mult)[[i==string for i in combo]][0]\n",
    "                    return 1\n",
    "                price_mul, mining_mul, demand_mul = [get_mul(string,mult,combo) for string in ['price','mining','demand']]\n",
    "\n",
    "                price_norm = price#/indiv.historical_data['Primary commodity price'][2001]\n",
    "                mining_norm= mining#/indiv.historical_data['Primary supply'][2001]\n",
    "                demand_norm= demand#/indiv.historical_data['Total demand'][2001]\n",
    "\n",
    "                new_rmse = pd.concat([price_norm*price_mul, mining_norm*mining_mul, demand_norm*demand_mul],axis=1)\n",
    "                new_score = np.log(new_rmse.div(3).sum(axis=1))\n",
    "                new_rmse['score'] = new_score\n",
    "                new_rmse['Price multiplier'] = price_mul\n",
    "                new_rmse['Mining multiplier']= mining_mul\n",
    "                new_rmse['Demand multiplier']= demand_mul\n",
    "                best = new_score.idxmin()\n",
    "                idx_list += [best]\n",
    "                if best not in output.columns:\n",
    "                    output[best] = new_rmse.T[best]\n",
    "        real_output = indiv.rmse_df.loc[['Primary commodity price RMSE','Primary supply RMSE','Total demand RMSE'],np.unique(idx_list)]\n",
    "        both_outputs = pd.concat([output,real_output],keys=['Weighted','Original'])\n",
    "\n",
    "        indiv.best_scenarios_adjusted_rmse = output.copy()\n",
    "        indiv.best_scenarios_original_rmse = real_output.copy()\n",
    "        indiv.best_scenarios = both_outputs.copy()\n",
    "\n",
    "def find_best_scenarios_the_way(many,n=10,verbosity=0):\n",
    "    indiv_list = [getattr(many.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "    for indiv in indiv_list:\n",
    "        quantile = 1\n",
    "        change = 0.1\n",
    "        real_output = indiv.best_scenarios_original_rmse.copy().astype(float)\n",
    "        the_way = indiv.best_scenarios_original_rmse.columns\n",
    "        sign = len(the_way)>n\n",
    "        not_sign = not sign\n",
    "        hist = indiv.historical_data.loc[2001].loc[['Total demand','Primary supply','Primary commodity price']]\n",
    "        hist = hist.rename(dict(zip(hist.index, [i+' RMSE' for i in hist.index])))\n",
    "        if sign:\n",
    "            while len(the_way)!=n and quantile<=1 and abs(change)>1e-12:\n",
    "                the_way = real_output.apply(lambda x: x-hist*quantile<0).all()\n",
    "                the_way = indiv.best_scenarios.loc[:,the_way].columns\n",
    "                sign = len(the_way)>n\n",
    "                if sign==not_sign:\n",
    "                    not_sign = not sign\n",
    "                    change /= -10\n",
    "                quantile -= change\n",
    "        if verbosity>1: print('\\t',indiv.commodity,len(the_way))\n",
    "        indiv.the_way = the_way\n",
    "        \n",
    "def find_best_scenarios_score_way(many,n=10,verbosity=0):\n",
    "    indiv_list = [getattr(many.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "    for indiv in indiv_list:\n",
    "        indiv.score_way = indiv.rmse_df.loc['score'].sort_values().head(n).index\n",
    "        \n",
    "def plot_best_scenarios_four_ways(many, dpi=50):\n",
    "    indiv_list = [getattr(many.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "    fig_list = []\n",
    "    for indiv in indiv_list:\n",
    "        pareto_way = indiv.pareto_hyperparam_subset.columns\n",
    "        fig,axes = easy_subplots(12,4,dpi=dpi)\n",
    "        n=0\n",
    "        for simulated,name in zip([pareto_way, indiv.the_way, indiv.no_pareto_way, indiv.score_way],['pareto way','the way','no pareto way', 'score way']):\n",
    "            # ax = axes[4*n:4*n+4]\n",
    "            ax = axes[n::4]\n",
    "            for i,a in zip(indiv.all_params[:3], ax):\n",
    "                results = indiv.results.copy()[indiv.objective_results_map[i]].loc[idx[:,2001:]]\n",
    "                if 'SD' not in i:\n",
    "                    historical_data = indiv.historical_data.copy()[i]\n",
    "                else:\n",
    "                    historical_data = pd.Series(results.min(),[0])\n",
    "\n",
    "                results = results.loc[idx[simulated,:]].unstack(0)\n",
    "\n",
    "                diction = get_unit(results, historical_data, i)\n",
    "                results, historical_data, unit = [diction[i] for i in ['simulated','historical','unit']]\n",
    "                sim_line = a.plot(results,linewidth=6,label=results.columns)\n",
    "                mins = min(historical_data.min(),results.min().min())*0.95\n",
    "                maxs = max(historical_data.max(),results.max().max())*1.1\n",
    "                hist_line = a.plot(historical_data,label='Historical',color='k',linewidth=6)\n",
    "\n",
    "                a.set(title=f'Best {i}, {name}',ylabel=i+' ('+unit+')',xlabel='Year',ylim=(mins,maxs))\n",
    "                if len(sim_line)<10:\n",
    "                    a.legend()\n",
    "            n+=1\n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(indiv.commodity.capitalize(),weight='bold',y=1.02,x=0.515)\n",
    "        fig_list += [fig]\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    return fig_list\n",
    "\n",
    "def find_best_scenarios_four_ways(many,n=10,verbosity=0,plot=True,dpi=50,recalculate=False):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    many: object of class Many\n",
    "    n: int, number of best scenarios to keep\n",
    "    verbostiy: int (0,1,2), where 0 has not print outputs\n",
    "      to mark progress, 1 has print outputs for each function\n",
    "      finished, and 2 has print outputs for each commodity\n",
    "      finished within each function\n",
    "    plot: whether to plot the results\n",
    "    dpi: int, dots per inch, figure resolution\n",
    "    recalculate: force recalculation (necessary if n changes)\n",
    "    -------\n",
    "    Each indiv will have the variables no_pareto_way, pareto_way, the_way\n",
    "    ^ These variables are lists of scenario numbers within indiv.rmse_df\n",
    "      corresponding to the best of that commodity within each of the methods\n",
    "      used to find the best scenarios.\n",
    "    \"\"\"\n",
    "    indiv_list = [getattr(many.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "    if np.any([not hasattr(indiv,'no_pareto_way') for indiv in indiv_list]) or recalculate:\n",
    "        if verbosity>0: print('starting no_pareto_way')\n",
    "        find_best_scenarios_no_pareto_way(many,n=n,verbosity=verbosity)\n",
    "        if verbosity>0: print('done with no_pareto_way')\n",
    "    \n",
    "    if np.any([not hasattr(indiv,'pareto_way') for indiv in indiv_list]) or recalculate:\n",
    "        if verbosity>0: print('starting pareto_way')\n",
    "        find_best_scenarios_pareto_way(many,n=n,verbosity=verbosity)\n",
    "        if verbosity>0: print('done with pareto_way')\n",
    "    \n",
    "    if np.any([not hasattr(indiv,'best_scenarios_adjusted_rmse') for indiv in indiv_list]) or recalculate:\n",
    "        if verbosity>0: print('starting weighting_all')\n",
    "        find_best_scenarios_weighting_all(many,verbosity=verbosity)\n",
    "        if verbosity>0: print('done with weighting_all')\n",
    "    \n",
    "    if np.any([not hasattr(indiv,'the_way') for indiv in indiv_list]) or recalculate:\n",
    "        if verbosity>0: print('starting the_way')\n",
    "        find_best_scenarios_the_way(many,n=n,verbosity=verbosity)\n",
    "        if verbosity>0: print('done with the_way')\n",
    "    \n",
    "    if np.any([not hasattr(indiv,'score_way') for indiv in indiv_list]) or recalculate:\n",
    "        if verbosity>0: print('starting score_way')\n",
    "        find_best_scenarios_score_way(many,n=n,verbosity=verbosity)\n",
    "        if verbosity>0: print('done with score_way')\n",
    "    \n",
    "    if plot:\n",
    "        plot_best_scenarios_four_ways(many, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0562c9-d254-4c16-aeb7-baa9b5cdc3fb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "find_best_scenarios_four_ways(many_norm,verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c230441-9189-4d70-89f9-b5bcba8c5a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_list = [getattr(many_norm.integ,indiv_str) for indiv_str in indiv_str_list]\n",
    "way_names = ['score_way','pareto_way','no_pareto_way','the_way']\n",
    "variations_in_ways = pd.DataFrame()\n",
    "for indiv in indiv_list:\n",
    "    ways = [getattr(indiv,i) for i in way_names]\n",
    "    var_in_ways = pd.DataFrame()\n",
    "    for way,way_name in zip(ways,way_names):\n",
    "        hp_only = indiv.rmse_df.drop([i for i in indiv.rmse_df.index if 'R2' in i or 'RMSE' in i or i=='score'])\n",
    "        hp_only = hp_only.loc[:,way]\n",
    "        coef_variation = abs(hp_only.std(axis=1)/hp_only.mean(axis=1))\n",
    "        var_in_ways[way_name] = coef_variation\n",
    "    var_in_ways = pd.concat([var_in_ways],keys=[indiv.commodity])\n",
    "    variations_in_ways = pd.concat([variations_in_ways,var_in_ways])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27329fe8-14d2-4321-abe3-673d22156154",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = indiv_list[0]\n",
    "hp = indiv.rmse_df\n",
    "hp = hp[indiv.score_way].T.sort_values('score').T\n",
    "hp = hp.drop([i for i in indiv.rmse_df.index if 'R2' in i or 'RMSE' in i or i=='score' or 'pareto' in i]).astype(float)\n",
    "pd.concat([hp.min(axis=1),hp.max(axis=1)],axis=1)\n",
    "hp.max(axis=1)/hp.min(axis=1)\n",
    "hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b87e6f-cc7a-4c29-bd92-b6d8740aad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig,ax = easy_subplots(1,width_scale=1.5,height_scale=1.2)\n",
    "# a = ax[0]\n",
    "# # sns.heatmap(pareto_hyperparam,annot=True,fmt='.2f',ax=a)\n",
    "# pareto_hyperparam.T.reset_index(drop=True).plot(ax=a)\n",
    "# a.legend(loc=(1.05,0))\n",
    "def ceofficient_of_variation_vs_quantile(self, quantiles=np.arange(0.1,1.01,0.1)):\n",
    "    coef_of_variation = pd.DataFrame()\n",
    "    for quantile in quantiles:\n",
    "        indiv = self\n",
    "        pareto_df = indiv.rmse_df.copy()[indiv.pareto_ind].loc[[i for i in indiv.rmse_df.index if 'RMSE' in i or i=='score']]\n",
    "        pareto_way = pareto_df.apply(lambda x: x<pareto_df.quantile(quantile,axis=1)).all()\n",
    "        pareto_way = pareto_df.loc[:,pareto_way].columns\n",
    "        pareto_df_sub = pareto_df[pareto_way]\n",
    "        pareto_hyperparam = indiv.rmse_df[pareto_way].drop(pareto_df.index).drop([i for i in indiv.rmse_df.index if 'R2' in i])\n",
    "        ph = abs(pareto_hyperparam.std(axis=1)/pareto_hyperparam.mean(axis=1)).sort_values()\n",
    "        ph.name = quantile\n",
    "        coef_of_variation = pd.concat([coef_of_variation, ph],axis=1)\n",
    "    return coef_of_variation\n",
    "\n",
    "coef_of_variation = ceofficient_of_variation_vs_quantile(self).dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b95ea1-e49d-48a3-be23-a76377083bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pareto_hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76203328-886c-49c3-a5dd-7a508d5f4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_of_variation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a089068e",
   "metadata": {},
   "source": [
    "## Checking for supply elasticity to price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22818db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bac1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import linearmodels.iv.model as lm\n",
    "# https://bashtage.github.io/linearmodels/iv/iv/linearmodels.iv.model.IV2SLS.html\n",
    "\n",
    "comm = 'Cu'\n",
    "scen = [0,1,2,3]\n",
    "mdata = many_sg.results_sorted.sort_index().loc[idx[comm,:25,2001:2040],\n",
    "                                         ['Total demand','Refined price','Mine production']]\n",
    "if type(scen)!=int:\n",
    "    mdata = mdata.droplevel(0)\n",
    "mdata.to_csv('mine_supply_elas.csv')\n",
    "\n",
    "for col in mdata.columns:\n",
    "    for i in np.arange(1,6):\n",
    "        if type(scen)!=int:\n",
    "            mdata[col+f'_lag{i}'] = mdata[col].groupby(level=0).shift(i)\n",
    "        else:\n",
    "            mdata[col+f'_lag{i}'] = mdata[col].shift(i)\n",
    "\n",
    "mdata = sm.add_constant(np.log(mdata))\n",
    "# mdata = sm.add_constant(mdata.dropna())\n",
    "# mdata['scenario'] = mdata.index.get_level_values(0)\n",
    "# scen_dummies = pd.get_dummies(mdata['scenario'])\n",
    "# mdata = pd.concat([mdata.drop(columns='scenario'),scen_dummies], axis=1)\n",
    "\n",
    "# mining_cols = [i for i in mdata.columns if 'Mine' in str(i) and '_' in str(i)]\n",
    "# price_cols = [i for i in mdata.columns if 'price' in str(i)]\n",
    "# demand_cols = [i for i in mdata.columns if 'demand' in str(i)]\n",
    "# mlr = lm.IV2SLS(dependent=mdata['Mine production'], \n",
    "#                  exog=mdata[mining_cols+scen], \n",
    "#                  endog=mdata[price_cols], \n",
    "#                  instruments=mdata[demand_cols]).fit(cov_type='kernel', debiased=True)\n",
    "# mlr.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4175c909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sm.GLS(mdata['Mine production'], mdata[['const']+price_cols]).fit(cov_type='HC3').summary()\n",
    "mdata.loc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62eaf28",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Running scrap demand scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9752a7",
   "metadata": {},
   "source": [
    "Selecting the best scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364cee80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T12:31:57.208042Z",
     "start_time": "2022-07-07T12:31:57.193083Z"
    }
   },
   "outputs": [],
   "source": [
    "biggie = pd.read_pickle('data/gold_run_hist_newdemand_test7_3p.pkl')\n",
    "new_rmse = pd.concat([biggie.loc['rmse_df'][i] for i in biggie.columns if type(biggie.loc['rmse_df'][i])==pd.core.frame.DataFrame]).drop_duplicates()\n",
    "new_rmse.index = pd.MultiIndex.from_tuples(new_rmse.index)\n",
    "new_rmse=new_rmse.iloc[:,0]\n",
    "score = new_rmse.unstack(0).loc[[i for i in new_rmse.index.get_level_values(1).unique() if 'RMSE' in i]]\n",
    "score = score.apply(lambda x: x/x.min(),axis=1)\n",
    "score.loc['Primary commodity price RMSE'] *= 1\n",
    "ind = score.sum().idxmin()\n",
    "best_params = biggie.loc['hyperparam'][ind].copy()['Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319bb53",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T12:31:57.905504Z",
     "start_time": "2022-07-07T12:31:57.889877Z"
    }
   },
   "outputs": [],
   "source": [
    "best_params.loc['pri CU TCRC elas'],ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83930198",
   "metadata": {},
   "source": [
    "Initializing scenario names, see Sensitivity class docstring and Integration().decode_scrap_scenario_name() method for more details. Initial run contains a baseline and scrap demand scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f616ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T12:32:00.989901Z",
     "start_time": "2022-07-07T12:32:00.958863Z"
    }
   },
   "outputs": [],
   "source": [
    "scenarios1 = ['sd_prpr_0yr_0%tot_0%inc']+\\\n",
    "    ['sd_pr_'+str(yr)+'yr_'+str(round(pct,3))+'%tot_0%inc' for yr in np.arange(1,4,1)\n",
    "     for pct in np.arange(0.01,0.1,0.01)]\n",
    "scenarios2 = ['sd_pr_'+str(yr)+'yr_'+str(round(pct,1))+'%tot_0%inc' for yr in np.arange(1,4,1)\n",
    "     for pct in np.arange(0.1,1,0.1)]\n",
    "scenarios3 = ['sd_pr_'+str(yr)+'yr_'+str(pct)+'%tot_0%inc' for yr in np.arange(1,4,1)\n",
    "     for pct in np.arange(1,11,1)]\n",
    "scenarios4 = ['sd_pr_'+str(yr)+'yr_'+str(pct)+'%tot_0%inc' for yr in np.arange(1,4,1)\n",
    "     for pct in np.arange(1,21,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5e01ea",
   "metadata": {},
   "source": [
    "Running the scenarios, note:\n",
    "- changing simulation_time and changing_base_parameters_series inputs in Sensitivity class initialization\n",
    "- adding scenarios input in Senstivity class initialization\n",
    "- bayesian_tune=False in s.run_monte_carlo\n",
    "- different filename setup\n",
    "- senstivity_parameters input in s.run_monte_carlo such that no variables get randomly reassigned\n",
    "\n",
    "Changed the approach since there is no dependency between these scenarios and more runs per file = more time per run, so we can pd.concat([]) the resulting dataframes once the scenarios are run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f17b0c-fac0-4231-bd4a-435a465dfd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('generalization')\n",
    "from integration_functions import *\n",
    "from Individual import *\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded66f13-6da5-42b4-8754-92aa8acc9ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_df=pd.read_pickle('best_ten_for_aluminum.pkl')\n",
    "best_params_df.loc['region_specific_price_response'] = best_params_df.loc['sector_specific_price_response']\n",
    "scenariosb = ['','sd_pr_1yr_0.01%tot_0%inc','sd_pr_1yr_0.1%tot_0%inc']\n",
    "scenarios2 = ['sd_pr_'+str(yr)+'yr_'+str(round(pct,1))+'%tot_0%inc' for yr in np.arange(1,2,1)\n",
    "     for pct in np.arange(0.2,1.1,0.2)]\n",
    "scenarios3 = ['sd_pr_'+str(yr)+'yr_'+str(round(pct,1))+'%tot_0%inc' for yr in np.arange(1,2,1)\n",
    "     for pct in np.arange(2,21,2)]\n",
    "scenarios = scenariosb+scenarios2+scenarios3\n",
    "scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837df039-3e04-4bd9-9b09-af5d829eda11",
   "metadata": {},
   "source": [
    "Getting hyperparameter distributions by weighting RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec7d1d-40a1-4471-8f87-02466ae37c12",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "commodity = 'aluminum'\n",
    "exponent = 10\n",
    "weights = np.exp(many_norm.integ.rmse_df.loc[idx[commodity,'score'],:])**-exponent\n",
    "params = many_norm.integ.rmse_df.loc[commodity]\n",
    "params = params.drop([i for i in params.index if 'RMSE' in i or 'R2' in i or i=='score'])\n",
    "params.loc['region_specific_price_response'] = params.loc['sector_specific_price_response']\n",
    "param_samp = params.T.sample(n=10000,replace=True,weights=weights,random_state=221017)\n",
    "fig,ax=easy_subplots(param_samp.columns)\n",
    "commodities = ['gold','aluminum','nickel']\n",
    "ax_series = pd.Series(ax[:len(param_samp.columns)], param_samp.columns)\n",
    "for commodity in commodities:\n",
    "    weights = np.exp(many_norm.integ.rmse_df.loc[idx[commodity,'score'],:])**-exponent\n",
    "    params = many_norm.integ.rmse_df.loc[commodity]\n",
    "    params = params.drop([i for i in params.index if 'RMSE' in i or 'R2' in i or i=='score'])\n",
    "    params.loc['region_specific_price_response'] = params.loc['sector_specific_price_response']\n",
    "    param_samp = params.T.sample(n=10000,replace=True,weights=weights)\n",
    "    param_samp.rename(columns=make_parameter_names_nice(param_samp.columns),inplace=True)\n",
    "    bin_widths = pd.concat([\n",
    "        many_norm.integ.rmse_df.loc[idx[commodities,:],:].groupby(level=1).min().min(axis=1),\n",
    "        many_norm.integ.rmse_df.loc[idx[commodities,:],:].groupby(level=1).max().max(axis=1),\n",
    "    ],axis=1,keys=['min','max'])\n",
    "    for i,j in zip(param_samp.columns,params.index):\n",
    "        a = ax_series[j]\n",
    "        a.hist(param_samp[i],label=f'{commodity.capitalize()}',alpha=0.5,\n",
    "               bins=np.linspace(bin_widths['min'][j],bin_widths['max'][j],50))\n",
    "        a.set(title=f'{i}')\n",
    "        a.legend()\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05b440-e6a8-4a53-aa7a-f63d8c3d4560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best10 = weights.sort_values(ascending=False).head(10).index\n",
    "# fig,ax = easy_subplots(params.index)\n",
    "# for i,a in zip(params.index,ax):\n",
    "#     params[best10].loc[i].plot.kde(title=i,ax=a)\n",
    "#     params[best10].loc[i].plot.hist(density=True,title=i,ax=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160cc7db-3d89-44ac-8ee7-03a313a43839",
   "metadata": {},
   "outputs": [],
   "source": [
    "commodity = 'gold'\n",
    "exponent = 10\n",
    "weights = np.exp(many_norm.integ.rmse_df.loc[idx[commodity,'score'],:])**-exponent\n",
    "params = many_norm.integ.rmse_df.loc[commodity]\n",
    "params = params.drop([i for i in params.index if 'RMSE' in i or 'R2' in i or i=='score'])\n",
    "params.loc['region_specific_price_response'] = params.loc['sector_specific_price_response']\n",
    "\n",
    "\n",
    "fig,ax = easy_subplots(params.index)\n",
    "n_samp = 100\n",
    "df = pd.DataFrame(np.nan, params.index, np.arange(0,n_samp))\n",
    "stds = params[weights.sort_values(ascending=False).head(25).index].std(axis=1)\n",
    "rs = 0\n",
    "n_best=2\n",
    "df2 = pd.DataFrame()\n",
    "for n in np.arange(0,n_best):\n",
    "    for i,a in zip(params.index,ax):\n",
    "        sign = np.sign(params[best10[n]][i].mean())\n",
    "        mean = abs(params[best10[n]][i])\n",
    "        std = stds[i]\n",
    "        # df.loc[i] = stats.lognorm.rvs(loc=0,scale=mean,s=std,size=n_samp,random_state=rs)*sign\n",
    "        generated = stats.norm.rvs(loc=mean,scale=std,size=n_samp*100,random_state=rs)\n",
    "        for _ in np.arange(0,3):\n",
    "            generated += mean-np.mean(generated)\n",
    "            generated = generated[generated>0]\n",
    "            if i!= 'mine_cost_tech_improvements':\n",
    "                generated = generated[generated<1]\n",
    "        df.loc[i] = generated[:n_samp]*sign\n",
    "        rs+=1\n",
    "        \n",
    "        df.loc[i].plot.hist(ax=a,title=i,alpha=0.5,label=n)\n",
    "        a.vlines(mean*sign,0,a.get_ylim()[1]*0.9,color='k')\n",
    "        a.vlines(df.loc[i].mean(),0,a.get_ylim()[1]*0.7,color='g')\n",
    "        a.legend()\n",
    "    df2 = pd.concat([df2,df],axis=1)\n",
    "df2 = df2.T.reset_index(drop=True).T\n",
    "# for i,a in zip(params.index,ax):\n",
    "#     df2.loc[i].plot.hist(ax=a,title=i,alpha=0.5,label=n)\n",
    "#     a.vlines(df2.loc[i].mean(),0,a.get_ylim()[1]*0.7,color='g')\n",
    "#     a.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b6d49c-e648-46c4-b5e9-46daf165e1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = param_samp.index.value_counts()\n",
    "plt.scatter(x/x.sum(),many_norm.integ.rmse_df.loc[commodity].loc['score'].loc[x.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd778f8d-337a-4e11-b3ca-2d24a94a8f32",
   "metadata": {},
   "source": [
    "Getting distributions for best hyperparameters and running scrap demand scenarios on them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d15d09-75a5-4485-8287-63ddf7f8c8cf",
   "metadata": {},
   "source": [
    "Individual commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3239f0f-77de-46c2-80ec-d94a879ed188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T12:52:54.683540Z",
     "start_time": "2022-07-07T12:32:18.089791Z"
    }
   },
   "outputs": [],
   "source": [
    "commodity = 'aluminum'\n",
    "exponent = 10\n",
    "element_commodity_map = {'Steel':'Steel','Al':'Aluminum','Au':'Gold','Cu':'Copper','Steel':'Steel','Co':'Cobalt','REEs':'REEs','W':'Tungsten','Sn':'Tin','Ta':'Tantalum','Ni':'Nickel','Ag':'Silver','Zn':'Zinc','Pb':'Lead','Mo':'Molybdenum','Pt':'Platinum','Te':'Telllurium','Li':'Lithium'}\n",
    "col_map = dict(zip(element_commodity_map.values(),element_commodity_map.keys()))\n",
    "weights = np.exp(many_norm.integ.rmse_df.loc[idx[commodity,'score'],:])**-exponent\n",
    "params = many_norm.integ.rmse_df.loc[commodity]\n",
    "params = params.drop([i for i in params.index if 'RMSE' in i or 'R2' in i or i=='score'])\n",
    "params.loc['region_specific_price_response'] = params.loc['sector_specific_price_response']\n",
    "param_samp = params.T.sample(n=10000,replace=True,weights=weights,random_state=221017)\n",
    "\n",
    "hyp_sample = pd.DataFrame(np.nan, np.arange(0,100), param_samp.columns)\n",
    "rs = 1017\n",
    "for i in hyp_sample.index:\n",
    "    for j in hyp_sample.columns:\n",
    "        hyp_sample.loc[i,j] = param_samp[j].sample(random_state=rs).values[0]\n",
    "        rs += 1\n",
    "hyp_sample = hyp_sample.T\n",
    "\n",
    "material=commodity\n",
    "scenario_list = [scenarios]\n",
    "t0 = datetime.now()\n",
    "t_per_batch = pd.Series(np.nan,np.arange(0,len(hyp_sample.columns)))\n",
    "filename_list = []\n",
    "n_scen = len(hyp_sample.columns)*len(scenario_list)\n",
    "for m,best_ind in enumerate(hyp_sample.columns[80:]):\n",
    "    time_remaining = t_per_batch.rolling(5,min_periods=1).mean().dropna().iloc[-1]*(n_scen-m*len(scenario_list))\n",
    "    print(f'Hyperparam set {m}/{len(hyp_sample.columns)},\\nEst. finish time: {str(datetime.now()+time_remaining)}')\n",
    "    best_params = hyp_sample[best_ind]\n",
    "    for n,scenarios in enumerate(scenario_list):\n",
    "        t1 = datetime.now()\n",
    "        thing = '_set'+str(n)\n",
    "        filename='data/'+material+'_run_scenario'+thing+'.pkl'\n",
    "        print('--'*15+filename+'-'*15)\n",
    "        s = Sensitivity(filename,changing_base_parameters_series=col_map[material],notes='Scenario run!',\n",
    "                        additional_base_parameters=best_params,\n",
    "                        simulation_time=np.arange(2019,2041),\n",
    "                        scenarios=scenarios,\n",
    "                        OVERWRITE=m==0,verbosity=0)\n",
    "        s.run_monte_carlo(n_scenarios=2,bayesian_tune=False, sensitivity_parameters=['Nothing, giving a string incompatible with any of the variable names'])\n",
    "        print(f'time for batch: {str(datetime.now()-t1)}')\n",
    "        t_per_batch.loc[m*len(scenario_list)+n] = datetime.now()-t1\n",
    "        filename_list += [filename]\n",
    "print(f'total time elapsed: {str(datetime.now()-t0)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef7d6d2-d699-486e-aaa7-928926f7db57",
   "metadata": {},
   "source": [
    "Multiple commodities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ffddcf-9f9c-42b0-8a08-8fcbdfdb51ed",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "commodity = 'aluminum'\n",
    "exponent = 10\n",
    "element_commodity_map = {'Steel':'Steel','Al':'Aluminum','Au':'Gold','Cu':'Copper','Steel':'Steel','Co':'Cobalt','REEs':'REEs','W':'Tungsten','Sn':'Tin','Ta':'Tantalum','Ni':'Nickel','Ag':'Silver','Zn':'Zinc','Pb':'Lead','Mo':'Molybdenum','Pt':'Platinum','Te':'Telllurium','Li':'Lithium'}\n",
    "col_map = dict(zip(element_commodity_map.values(),element_commodity_map.keys()))\n",
    "for commodity in ['Steel','Al','Au','Sn','Cu','Ni','Ag','Zn','Pb']:\n",
    "    commodity=element_commodity_map[commodity].lower()\n",
    "    weights = np.exp(many_norm.integ.rmse_df.loc[idx[commodity,'score'],:])**-exponent\n",
    "    params = many_norm.integ.rmse_df.loc[commodity]\n",
    "    params = params.drop([i for i in params.index if 'RMSE' in i or 'R2' in i or i=='score'])\n",
    "    params.loc['region_specific_price_response'] = params.loc['sector_specific_price_response']\n",
    "    param_samp = params.T.sample(n=10000,replace=True,weights=weights,random_state=221017)\n",
    "\n",
    "    hyp_sample = pd.DataFrame(np.nan, np.arange(0,100), param_samp.columns)\n",
    "    rs = 1017\n",
    "    for i in hyp_sample.index:\n",
    "        for j in hyp_sample.columns:\n",
    "            hyp_sample.loc[i,j] = param_samp[j].sample(random_state=rs).values[0]\n",
    "            rs += 1\n",
    "    hyp_sample = hyp_sample.T\n",
    "\n",
    "    material=commodity\n",
    "    scenario_list = [scenarios]\n",
    "    t0 = datetime.now()\n",
    "    t_per_batch = pd.Series(np.nan,np.arange(0,len(hyp_sample.columns)))\n",
    "    filename_list = []\n",
    "    n_scen = len(hyp_sample.columns)*len(scenario_list)\n",
    "    for m,best_ind in enumerate(hyp_sample.columns[80:]):\n",
    "        if m>1 and len(t_per_batch.dropna())<=3:\n",
    "            time_remaining = (t_per_batch.dropna().sum()/len(t_per_batch.dropna()))*(n_scen-m*len(scenario_list))\n",
    "        elif m>1:\n",
    "            time_remaining = (t_per_batch.dropna().iloc[-3:].sum()/3)*(n_scen-m*len(scenario_list))\n",
    "        else: time_remaining = (datetime(2022,10,17,23,21,50,0)-datetime(2022,10,17,23,13,20,0))*(n_scen-m*len(scenario_list))\n",
    "        print(f'Hyperparam set {m}/{len(hyp_sample.columns)},\\nEst. finish time: {str(datetime.now()+time_remaining)}')\n",
    "        best_params = hyp_sample[best_ind]\n",
    "        for n,scenarios in enumerate(scenario_list):\n",
    "            t1 = datetime.now()\n",
    "            thing = '_set'\n",
    "            filename='data/'+material+'_run_scenario'+thing+'.pkl'\n",
    "            print('--'*15+filename+'-'*15)\n",
    "            s = Sensitivity(filename,changing_base_parameters_series=col_map[material.capitalize()],notes='Scenario run!',\n",
    "                            additional_base_parameters=best_params,\n",
    "                            simulation_time=np.arange(2019,2041),\n",
    "                            scenarios=scenarios,\n",
    "                            OVERWRITE=m==0,verbosity=0)\n",
    "            s.run_monte_carlo(n_scenarios=2,bayesian_tune=False, sensitivity_parameters=['Nothing, giving a string incompatible with any of the variable names'])\n",
    "            print(f'time for batch: {str(datetime.now()-t1)}')\n",
    "            t_per_batch.loc[m*len(scenario_list)+n] = datetime.now()-t1\n",
    "            filename_list += [filename]\n",
    "    print(f'total time elapsed: {str(datetime.now()-t0)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ec6a3-d1f3-4771-b82c-0cfdfeb10264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b47b932b-b12e-4a2c-b758-3d54bf948333",
   "metadata": {},
   "source": [
    "Base running scrap demand scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4376d-15ad-4444-9918-5e954221dfe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T12:52:54.683540Z",
     "start_time": "2022-07-07T12:32:18.089791Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# also send pickle file\n",
    "material='aluminum'\n",
    "t0 = datetime.now()\n",
    "for n,scenarios in enumerate([scenarios2]):\n",
    "    t1 = datetime.now()\n",
    "    thing = '_set'+str(n)\n",
    "    filename='data/'+material+'_run_scenario'+thing+'.pkl'\n",
    "    col_map = {'aluminum':'Al','steel':'Steel','gold':'Au'}\n",
    "    print('--'*15+filename+'-'*15)\n",
    "    s = Sensitivity(filename,changing_base_parameters_series='Al',notes='Scenario run!',\n",
    "                    additional_base_parameters=best_params,\n",
    "                    simulation_time=np.arange(2019,2041),\n",
    "                    scenarios=scenarios,\n",
    "                    OVERWRITE=True,verbosity=0)\n",
    "    s.run_monte_carlo(n_scenarios=2,bayesian_tune=False, sensitivity_parameters=['Nothing, giving a string incompatible with any of the variable names'])\n",
    "    print(f'time for batch: {str(datetime.now()-t1)}')\n",
    "filename_list = ['data/'+material+'_run_scenario_set'+str(n)+'.pkl' for n in np.arange(0,len(scenarios))]\n",
    "print(f'total time elapsed: {str(datetime.now()-t0)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af706b40-cdab-4e53-95f5-2a67058490e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = Individual(material='Al',filename='data/aluminum_run_scenario_set0.pkl')\n",
    "indiv.hyperparam.loc[best_params.index] #just checking that everything got moved into the hyperparameters correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235572a1-f026-4c10-a400-205f2af8ee52",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv.plot_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e2d73",
   "metadata": {},
   "source": [
    "Checking that the increase in scrap demand matches that in the hyperparameter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c96a5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:57:01.728639Z",
     "start_time": "2022-07-07T13:57:01.528891Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "indiv = Individual('Al',3,'data/aluminum_run_scenario_set0.pkl')\n",
    "pd.concat([indiv.results[['Additional direct melt','Additional secondary refined']].dropna().loc[idx[:,:],:].sum(axis=1).groupby(level=0).sum()/\\\n",
    "                indiv.results['Total demand'].loc[idx[:,2019]],\n",
    "           indiv.hyperparam.loc['direct_melt_pct_change_tot']-1],axis=1,keys=['Calculated for each scenario, fraction of total demand','From hyperparameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97a3c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T12:23:24.278181Z",
     "start_time": "2022-07-07T12:23:24.261225Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3cbde355",
   "metadata": {},
   "source": [
    "# Interpreting scrap demand scenario results\n",
    "- Michelena, I recommend making a class that holds all your work so we can guarantee it is reproducible.\n",
    "- Would use the Individual class to get result and hyperparameter variables for each pickle file, then go from there. \n",
    "- Try looking at changes in mine production (conc. demand), scrap supply, and scrap demand over time. A great way to do this is to subtract the values in the baseline scenario (no scrap demand changes) from each other scenario, then using the pandas.DataFrame().cumsum() function to look at the cumulative change over time. \n",
    "- You could try plotting the cumulative change over the first 5/10/20 years vs the size of the scrap demand increase (the scrap demand increase caused by the manufacturer, so the 'Additional direct melt'+'Additional secondary refined' columns in the results dataframe, not the 'Scrap demand' column)\n",
    "- Can try dividing the mining change by the scrap demand increase (again, I'd use the sum of the Additional direct melt and Additional secondary refined columns rather than the Scrap demand column) to get the efficiency of the scrap demand increase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0968a607",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_s = Many()\n",
    "many_s.load_data('2023-01-31 09_54_22_2_fruity_both_alt')\n",
    "\n",
    "many_d = Many()\n",
    "many_d.load_data('2023-01-28 15_31_00_1_fruity_demand_alt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9811c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_d.results.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa8f8c-9441-4fa0-a960-e3479aff168c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# many=Many()\n",
    "# many.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set')\n",
    "# figs,axs=plot_multi_scenario_response_all(many)\n",
    "# figs,axs=plot_multi_scenario_response(many,'tin')\n",
    "multi_scenario_results = many.multi_scenario_results.loc['aluminum'].copy()\n",
    "multi_scenario_hyperparam = many.multi_scenario_hyperparam.loc['aluminum'].copy()\n",
    "ph = multi_scenario_results[param].unstack(1)\n",
    "\n",
    "diff_ph = ph.subtract(ph[0],axis=0).loc[idx[:,2019:],:]\n",
    "diff_ph = diff_ph.rename(columns=dict(zip(diff_ph.columns,\n",
    "                                     pd.MultiIndex.from_tuples([(multi_scenario_hyperparam.loc[idx[0,'secondary_refined_duration'],i],round(multi_scenario_hyperparam.loc[idx[0,['secondary_refined_pct_change_tot','direct_melt_pct_change_tot']],i].sum()-1,3)) for i in multi_scenario_hyperparam.columns]))))\n",
    "diff_ph.columns = pd.MultiIndex.from_tuples(diff_ph.columns)\n",
    "diff_ph.columns = diff_ph.columns.set_names(['duration','change'])\n",
    "diff_ph = diff_ph.groupby(level=0).median()\n",
    "# diff_ph = diff_ph.stack().stack().reset_index().rename(columns={'level_0':'hyperparam',0:'diff_baseline'})\n",
    "# diff_ph['diff_baseline']/=indiv.results['Total demand'].unstack(0).loc[2019]/100\n",
    "# diff_ph['change'] = round((diff_ph['change']-1)*100,1)\n",
    "\n",
    "diff_ph.loc[:,idx[1,1.2]].plot.hist()\n",
    "diff_ph.loc[:,idx[1,1.1]].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098a5c4-7cff-41cf-a637-e96c139e272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(many.multi_scenario_results.columns.sort_values())\n",
    "ph = many.multi_scenario_results.copy()\n",
    "param = 'Total demand'\n",
    "ph[param].loc[idx[:,:,:,2019:]].unstack(2).subtract(ph[param].loc[idx[:,:,:,2019:]].unstack(2)[0],axis=0).stack().unstack(-2).T.plot(legend=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da778e4-4f78-4dd1-8711-88eb77c994d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially at 25% dm, total demand=1\n",
    "demand = pd.Series(1,np.arange(2019,2025))\n",
    "fraction = pd.Series(0.25,np.arange(2019,2025))\n",
    "additional = pd.Series(0.1, np.arange(2019,2025))\n",
    "# additional.loc[2019]=0\n",
    "price = pd.Series(100,np.arange(2019,2025))\n",
    "direct_melt = demand*fraction\n",
    "\n",
    "for i in price.index[1:]:\n",
    "    temp_direct_melt = direct_melt-additional\n",
    "    temp_demand = demand-additional\n",
    "    temp_fraction = direct_melt/temp_demand\n",
    "    temp_fraction.loc[i] = temp_fraction[i-1]*price[i]/price[i-1]\n",
    "    direct_melt.loc[i] = temp_fraction[i]*temp_demand[i] + additional[i]\n",
    "    fraction.loc[i] = direct_melt[i]/demand[i]\n",
    "    \n",
    "pd.concat([\n",
    "    direct_melt,\n",
    "    fraction,\n",
    "    temp_demand,\n",
    "    temp_direct_melt,\n",
    "    temp_fraction,\n",
    "],keys=['direct melt','fraction','temp demand','temp direct melt','temp fraction'],axis=1).index\n",
    "# '''\n",
    "# go from 25% direct melt. Our company is previously consuming only refined, and switches to only dm.\n",
    "# So \n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db35c3-21eb-4399-9484-226393e0e05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9950a53-618a-4782-8710-27ef52ba6ae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = stackplot_scrap_demand(many_act_hist, 'gold', 2, 21)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fa91a9-e87f-495a-b4f9-bf21ccb45da9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "commodities=['Al','Au','Ag','Zn','Pb','Sn','Ni','Cu']\n",
    "# many_alt, many_set, many_alt_hist, many_hist = Many(), Many(), Many(), Many()\n",
    "# many_alt_g = Many()\n",
    "# many_g = Many()\n",
    "\n",
    "many_act, many_act_hist = Many(), Many()\n",
    "many_act10,many_act_hist10 = Many(), Many()\n",
    "# many_alt.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_hist',commodities=commodities)\n",
    "# many_set.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_hist',commodities=commodities)\n",
    "# many_alt_hist.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_hist_g',commodities=commodities)\n",
    "# many_hist.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_hist_g',commodities=commodities)\n",
    "# many_g.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_hist_g_new',commodities=commodities)\n",
    "# many_alt_g.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_hist_g_new',commodities=commodities)\n",
    "\n",
    "many_act_hist.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_hist_act',commodities=commodities,n_scenarios=10)\n",
    "many_act.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_act',commodities=commodities,n_scenarios=10)\n",
    "many_act_hist10.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_hist_act_10yr',commodities=commodities,n_scenarios=10)\n",
    "many_act10.load_future_scenario_runs(verbosity=2, scenario_name_base='_run_scenario_set_alt_act_10yr',commodities=commodities,n_scenarios=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56380abb-bd9c-4f2c-ab63-83e82ec6a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_best_scenario_sd(many_act_hist,commodity='nickel',best=np.arange(0,100),scrap_scenario=0,\n",
    "                      plot_supply_demand_stack=False,legend=False,end_year=2019);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cf299d-a8f5-462e-b09a-e6e1e6eccb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_crashing(many):\n",
    "    if not hasattr(many,'multi_scenario_results_original'):\n",
    "        many.multi_scenario_results_original = many.multi_scenario_results.copy()\n",
    "    many.multi_scenario_results = many.multi_scenario_results_original.copy()\n",
    "    if 2010 in many.multi_scenario_results.index.levels[3]:\n",
    "        ph = many.multi_scenario_results.loc[idx[:,:,0,:],'Conc. supply'].unstack().T.loc[2010:]\n",
    "    else:\n",
    "        ph = many.multi_scenario_results.loc[idx[:,:,0,:],'Conc. supply'].unstack().T\n",
    "    # ph = ph.loc[:,(ph.loc[2040]>ph.loc[2019])]\n",
    "    ph = ph.loc[:,ph.diff().std().sort_values().head(10).index]\n",
    "    ind = ph.copy().T.stack().index\n",
    "    many.multi_scenario_results = many.multi_scenario_results.loc[idx[:,ind.get_level_values(1).unique(),:],:].sort_index()\n",
    "    print(many.multi_scenario_results.loc['aluminum'].index.get_level_values(0).unique())\n",
    "    \n",
    "def restore(many):\n",
    "    many.multi_scenario_results = many.multi_scenario_results_orginal.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b7eed-ee15-4dde-a8f3-32b33010585d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for many_ph in [many_s, many_d]:\n",
    "    remove_crashing(many_ph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abefe1d9-020b-476a-9470-23bdd6e52159",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# figs,axs=plot_multi_scenario_response_all(many_act_hist,param='Mine production',divisor='Mine production',\n",
    "#                                           to_year=[2025,2030,2040])\n",
    "figs,axs=plot_multi_scenario_response(many_s,commodity=['gold','aluminum','copper'], \n",
    "                                      param='Mine production',divisor='Mine production',\n",
    "                                      plot_rir=True, if_line_plot_separate=False,\n",
    "                                      to_year=[2040],dpi=500)\n",
    "axs[0].set(title='Impact of increasing recycling content on mine production',\n",
    "           ylabel='Annual mine production\\ndifference (% 2019 value)',xlabel='Recycling input rate')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb00c9",
   "metadata": {},
   "source": [
    "## Fruity plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13fb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruity_rename = {\n",
    "        'tcrc_elas_sd':'Concentrate price\\nS-D response',\n",
    "        'mine_cost_change_per_year':'Mine cost change per year',\n",
    "        'intensity_response_to_gdp':'Demand response to GDP',\n",
    "        'sector_specific_price_response':'Demand response to price',\n",
    "        'scrap_spread_elas_sd':'Scrap spread S-D response',\n",
    "        'mine_cost_og_elas':'Mine cost change with ore grade',\n",
    "        'incentive_opening_probability':'Fraction of viable mines that open',\n",
    "        'refinery_capacity_fraction_increase_mining':'Refinery capacity S-D response',\n",
    "        'primary_commodity_price_elas_sd':'Metal price responsiveness',\n",
    "        'sector_dist_electrical':'Demand fraction in electrical sector',\n",
    "        'pri CU price elas':'Pri. refinery price response',\n",
    "        'sec CU price elas':'Sec. refinery concentrate price response',\n",
    "        'sec CU TCRC elas':'Sec. refinery price response',\n",
    "        'primary_oge_scale':'Rate of ore grade decline',\n",
    "        'primary_price_resources_contained_elas':'Exploration response to price',\n",
    "        'collection_elas_scrap_price':'Scrap collection response to price',\n",
    "        'sector_specific_dematerialization_tech_growth':'Demand response to time',\n",
    "        'sec ratio scrap spread elas':'Refinery scrap use response to price',\n",
    "        'Secondary refinery fraction of recycled content, Global':'Secondary refinery fraction\\nof recycled content'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f6e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaab6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "many = many_d\n",
    "total_scrap_shock = many.hyperparam.loc[idx['Au',:, ['direct_melt_pct_change_tot',\n",
    "                                                       'secondary_refined_pct_change_tot']],\n",
    "                      'Value'].unstack().astype(float).sum(axis=1).add(-1)\n",
    "\n",
    "ind = total_scrap_shock[total_scrap_shock==1.1].droplevel(0).index\n",
    "base_ind = total_scrap_shock[total_scrap_shock==-1].droplevel(0).index\n",
    "res = many.results.loc['Au'].copy()\n",
    "\n",
    "i=31\n",
    "yrs = np.arange(2001,2041)\n",
    "colors = list(sns.color_palette('Dark2'))\n",
    "fig,ax=easy_subplots(3,width_scale=0.8)\n",
    "a = ax[0]\n",
    "res['dm rest of market'] = res['Direct melt']-res['Additional direct melt']\n",
    "res['sr rest of market'] = res['Sec. ref. cons.']-res['Additional secondary refined']\n",
    "res['tot rest of market'] = res['Direct melt']-res['Additional direct melt']+\\\n",
    "                            res['Sec. ref. cons.']-res['Additional secondary refined']\n",
    "\n",
    "ph = res[['dm rest of market','Additional direct melt']].loc[ind[i]]\n",
    "a.stackplot(ph.index, ph.T, labels=ph.columns, colors=colors[:2])\n",
    "# res.loc[idx[base_ind,:]]['Direct melt'].unstack(0).loc[2001:].median(axis=1).plot(legend=False,ax=a)\n",
    "res.loc[base_ind[i]]['Direct melt'].loc[yrs].plot(ax=a,color='k').grid(axis='x')\n",
    "a.set(title='Direct melt scrap demand', xlabel='Year', ylabel='Scrap demand (kt)')\n",
    "a.tick_params(axis='x', direction='out')\n",
    "\n",
    "a = ax[1]\n",
    "ph = res[['sr rest of market','Additional secondary refined']].loc[ind[i]]\n",
    "a.stackplot(ph.index, ph.T, labels=ph.columns, colors=[colors[0],colors[2]])\n",
    "# res.loc[idx[base_ind,:]]['Direct melt'].unstack(0).loc[2001:].median(axis=1).plot(legend=False,ax=a)\n",
    "res.loc[base_ind[i]]['Sec. ref. cons.'].loc[yrs].plot(ax=a,color='k').grid(axis='x')\n",
    "a.set(title='Refined scrap demand', xlabel='Year', ylabel='Scrap demand (kt)')\n",
    "a.tick_params(axis='x', direction='out')\n",
    "\n",
    "a = ax[2]\n",
    "ph = res[['tot rest of market','Additional direct melt', 'Additional secondary refined']].loc[ind[i]]\n",
    "a.stackplot(ph.index, ph.T, labels=ph.columns, colors=colors[:3])\n",
    "# res.loc[idx[base_ind,:]]['Direct melt'].unstack(0).loc[2001:].median(axis=1).plot(legend=False,ax=a)\n",
    "res.loc[base_ind[i]]['Scrap demand'].loc[yrs].plot(ax=a,color='k').grid(axis='x')\n",
    "a.set(title='Total scrap demand', xlabel='Year', ylabel='Scrap demand (kt)')\n",
    "a.tick_params(axis='x', direction='out')\n",
    "# a.legend()\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(50)\n",
    "plt.show()\n",
    "plt.close()\n",
    "# [i for i in many_d.hyperparam.index.levels[2] if 'secon' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283cb714",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78493160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50c80c8",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csd = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx',index_col=0)\n",
    "comms = many_s.results.index.get_level_values(0).unique()\n",
    "fruity_consumption = pd.Series({\n",
    "    'Steel':75,\n",
    "    'Au':0.017,\n",
    "    'Co':12,\n",
    "    'REEs':2.2,\n",
    "    'W':1*2.261,\n",
    "    'Sn':0.7,\n",
    "    'Ta':0.2*2.442,\n",
    "    'Al':450,\n",
    "    'Cu':24.5,\n",
    "})\n",
    "\n",
    "total_consumption = many_s.historical_data['Total demand'].loc[idx[:,2019]]\n",
    "total_consumption = total_consumption.loc[total_consumption.index.isin(fruity_consumption.index)]\n",
    "\n",
    "fruity_parts = pd.concat([\n",
    "    fruity_consumption,\n",
    "    total_consumption,\n",
    "    fruity_consumption/total_consumption * 100,\n",
    "],axis=1,keys=['Firm consumption (kt)', 'Total consumption (kt)', 'Percentage of total consumption'])\n",
    "\n",
    "output = pd.DataFrame()\n",
    "for many in [many_d, many_s]:\n",
    "    comms = many.results.index.get_level_values(0).unique()\n",
    "    data = fruity_plots(many, comms, get_data_only=True)[0]\n",
    "    fig,ax=plt.subplots()\n",
    "    sns.barplot(data, x='Scrap demand', y='Mean change normed', hue='Commodity', ax=ax)\n",
    "    plt.close()\n",
    "\n",
    "    # Getting the lines from the plot:\n",
    "    lines = []\n",
    "    xlines = []\n",
    "    for line in ax.lines:\n",
    "        lines += [line.get_ydata()] # these are the lines for the errorbars\n",
    "        xlines += [line.get_xdata()] # these are the lines for the errorbars    \n",
    "\n",
    "    # Arranging into the upper value of the confidence interval\n",
    "    conf_interval_uppers = pd.DataFrame(np.array(lines)[:,1].reshape(len(comms),\n",
    "                                                                     len(data['Scrap demand'].unique())),\n",
    "                 comms, data['Scrap demand'].unique())\n",
    "\n",
    "    # Checking pvalues for difference from zero \n",
    "    pvals = data.set_index(['Commodity','Scenario number','Scrap demand'])[\n",
    "        'Mean change normed'].groupby(level=[0,2]).apply(\n",
    "        lambda x: stats.ttest_1samp(x, 0)[1]).round(2).unstack(0)\n",
    "\n",
    "    # Estimating the transition point to statistically significant\n",
    "    transition_point = pd.concat([\n",
    "        ((conf_interval_uppers<0).cumsum(axis=1)==1).idxmax(axis=1),\n",
    "        ((conf_interval_uppers<0).cumsum(axis=1)==2).idxmax(axis=1),\n",
    "        ((conf_interval_uppers<0).cumsum(axis=1)==3).idxmax(axis=1),\n",
    "        (conf_interval_uppers>0).iloc[:,::-1].idxmax(axis=1),\n",
    "        ((conf_interval_uppers<0).T.astype(int).rolling(5, center=True, min_periods=1).mean()>0.5).idxmax(),\n",
    "        (((pvals<0.05)) & \n",
    "         ((data.groupby(['Commodity','Scrap demand']).median()['Mean change normed'].unstack(0)<0))).idxmax(),\n",
    "        (((pvals<0.2)) & \n",
    "         ((data.groupby(['Commodity','Scrap demand']).median()['Mean change normed'].unstack(0)<0))).idxmax(),\n",
    "    ],axis=1, keys=[0,1,2,3,4,'pval','80%'])\n",
    "\n",
    "    data['Displacement'] = data['Mean change normed']/data['Scrap demand']*-1\n",
    "    median_displacement = data.groupby(['Commodity']).median()['Displacement']\n",
    "    closest_to_fruity = list(zip(fruity_parts.dropna().index, [data['Scrap demand'].unique()[\n",
    "        np.argmin(abs(fruity_parts['Percentage of total consumption'][i]-data['Scrap demand'].unique()))]\n",
    "                                                               for i in fruity_parts.dropna().index]))\n",
    "    fruity_displacement = data.groupby(['Commodity','Scrap demand']).median().loc[\n",
    "        closest_to_fruity].droplevel(1)['Displacement']\n",
    "    fruity_mining_red = data.groupby(['Commodity','Scrap demand']).median().loc[\n",
    "        closest_to_fruity].droplevel(1)['Mean change']\n",
    "    fruity_mining_std = data.groupby(['Commodity','Scrap demand']).std().loc[\n",
    "        closest_to_fruity].droplevel(1)['Mean change']\n",
    "    fruity_mining_red_pct = data.groupby(['Commodity','Scrap demand']).median().loc[\n",
    "        closest_to_fruity].droplevel(1)['Mean change normed']\n",
    "    \n",
    "    sd1 = ', scrap demand only' if many==many_d else ', scrap demand + collection'\n",
    "    sd2 = '' if many==many_d else ' + collection'\n",
    "    output_ph = pd.concat([\n",
    "        transition_point['pval'],\n",
    "        transition_point['80%'],\n",
    "        median_displacement,\n",
    "        fruity_displacement,\n",
    "        fruity_mining_red,\n",
    "        fruity_mining_std,\n",
    "        fruity_mining_red_pct,\n",
    "        pvals.unstack().loc[closest_to_fruity].droplevel(1),\n",
    "    ],keys=['Transition to statistical significance (%)'+sd1,\n",
    "            'Transition to statistical significance 80% conf (%)'+sd1,\n",
    "            'Median mining reduction per increase in scrap demand'+sd2,\n",
    "            'Fruity mining reduction per increase in scrap demand'+sd2,\n",
    "            'Fruity mining change (kt)'+sd1,\n",
    "            'Fruity mining std (kt)'+sd1,\n",
    "            'Fruity mining change (%)'+sd1,\n",
    "            'p-value'+sd1,\n",
    "           ],axis=1)\n",
    "    output = pd.concat([output,output_ph],axis=1)\n",
    "    \n",
    "fruity_parts = pd.concat([\n",
    "    fruity_consumption,\n",
    "    total_consumption,\n",
    "    fruity_consumption/total_consumption * 100,\n",
    "],axis=1,keys=['Firm consumption (kt)', 'Total consumption (kt)', 'Percentage of total consumption'])\n",
    "\n",
    "out_table = pd.concat([fruity_parts, output],axis=1)\n",
    "out_table = out_table.dropna()#.rename({'W':'W (WO3 eq.)', 'Ta':'Ta (Ta2O5 eq)'})\n",
    "out_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777bf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f606b93f",
   "metadata": {},
   "source": [
    "Impact factors from: \n",
    "https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0101298 \n",
    "\n",
    "SI:\n",
    "https://view.officeapps.live.com/op/view.aspx?src=https%3A%2F%2Fstorage.googleapis.com%2Fplos-corpus-prod%2F10.1371%2Fjournal.pone.0101298%2F1%2Fpone.0101298.s001.docx%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dwombat-sa%2540plos-prod.iam.gserviceaccount.com%252F20230307%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20230307T222819Z%26X-Goog-Expires%3D86400%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D0d130cc97bb06ddd254d74bf58661216a6100bb96416c5278329c130d292c1841bd0c70dd35e34ca7d0eaca967eca9f85811b3cdc266e83c92a1abe9a4b91c41dfed3da7871f69109ab9b48d6cedd9b47d8097d06ff2bf844f547ae67b3accb5d834fc4144d1d12549f73da7d8f62fdf4abfbd2f8a968aa96dad9d5ee79522fb331421fb752cccea801f995b77a0915e25a7ce8617ab1ab410596c6b0d1850ea78734baeff135eb58382e4e975b1d22dc6c35565a8173bcbe1305f43ee098159a503323b72b2b8943b64c3930f6dd2f18410077337338d8d34280467717160a3cd68e3b66465d07160873586f098d6ab5a58f00d931c618d5d7770f65684de6b&wdOrigin=BROWSELINK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59b8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "fruity_mining_red = data.groupby(['Commodity','Scrap demand']).median().loc[\n",
    "        closest_to_fruity].droplevel(1)['Mean change']\n",
    "\n",
    "bardata = data.set_index(['Commodity','Scrap demand']).loc[closest_to_fruity]\n",
    "upperq = bardata.groupby(level=0).quantile(0.95)['Mean change']\n",
    "lowerq = bardata.groupby(level=0).quantile(0.05)['Mean change']\n",
    "bardata = bardata.reset_index()\n",
    "# bardata['Mean change'] = bardata.apply(lambda x: x['Mean change'] if x['Mean change']<upperq[x['Commodity']] \n",
    "#                         and x['Mean change']>lowerq[x['Commodity']] else np.nan,axis=1)\n",
    "# bardata.reset_index()\n",
    "bardata['CO2'] = bardata.apply(lambda x: x['Mean change']*mining_impact_factors[x['Commodity']],axis=1)\n",
    "sns.barplot(bardata,\n",
    "    x='Commodity', y='CO2',errorbar=('ci',90)\n",
    ")\n",
    "# plt.ylim(-60,20)\n",
    "bardata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9496a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31137c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pct = out_table[['Fruity mining change (%), scrap demand only',\n",
    "                 'Fruity mining change (%), scrap demand + collection',\n",
    "                  ]].div(-1)\n",
    "pct = pct.sort_values(by='Fruity mining change (%), scrap demand + collection', ascending=False)\n",
    "pct_alpha = pct.copy()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# sns.barplot(pct, x='level_0', y=0, hue='level_1', ax=ax)\n",
    "pct\n",
    "\n",
    "\n",
    "pct['Fruity mining change (%), scrap demand only'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand only']\n",
    "    <out_table['Percentage of total consumption'])\n",
    "pct['Fruity mining change (%), scrap demand + collection'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand + collection']\n",
    "    <out_table['Percentage of total consumption'])\n",
    "pct_alpha['Fruity mining change (%), scrap demand only'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand only']\n",
    "    >out_table['Percentage of total consumption'])\n",
    "pct_alpha['Fruity mining change (%), scrap demand + collection'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand + collection']\n",
    "    >out_table['Percentage of total consumption'])\n",
    "pct.plot.bar(ax=ax)\n",
    "pct_alpha.plot.bar(ax=ax, alpha=0.4).grid(axis='x')\n",
    "ax.get_legend().remove()\n",
    "ax.set(ylabel='Potential mining reduction\\n(% 2019 mining)', title='100% recycled material → Potential mining reductions')\n",
    "fig.set_dpi(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d6831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For making bubbles\n",
    "co2 = bar_co2.copy()+alpha_bar_co2\n",
    "kt = bar.copy()+alpha_bar\n",
    "ore = bar_ore.copy()+alpha_bar_ore\n",
    "perc = pct.copy()+pct_alpha\n",
    "for k in [co2,kt,ore,perc]:\n",
    "    k /= k.max().max()\n",
    "    k.rename(columns=dict(zip(k.columns,[j.split(', ')[1] for j in k.columns])),inplace=True)\n",
    "(co2+kt+ore+perc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f890c6be",
   "metadata": {
    "code_folding": [
     4,
     18,
     33
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bar = abs(out_table[['Fruity mining change (kt), scrap demand only',\n",
    "           'Fruity mining change (kt), scrap demand + collection']])\n",
    "alpha_bar = bar.copy()\n",
    "bar['Fruity mining change (kt), scrap demand only'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand only']\n",
    "    <out_table['Percentage of total consumption'])\n",
    "bar['Fruity mining change (kt), scrap demand + collection'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand + collection']\n",
    "    <out_table['Percentage of total consumption'])\n",
    "alpha_bar['Fruity mining change (kt), scrap demand only'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand only']\n",
    "    >out_table['Percentage of total consumption'])\n",
    "alpha_bar['Fruity mining change (kt), scrap demand + collection'] *= (\n",
    "    out_table['Transition to statistical significance 80% conf (%), scrap demand + collection']\n",
    "    >out_table['Percentage of total consumption'])\n",
    "mining_impact_factors = pd.Series({\n",
    "    'Steel':1.5,\n",
    "    'Al':12.2,\n",
    "    'Cu':3.1,\n",
    "    'Ni':9.7,\n",
    "    'Pb':2.19,\n",
    "    'Zn':3.1,\n",
    "    'Au':18206,\n",
    "    'Sn':17.1,\n",
    "    'Te':21.9,\n",
    "    'W':12.6,\n",
    "    'Ag':245.6,\n",
    "    'Ta':260,\n",
    "})\n",
    "new_scrap_impact = pd.Series({\n",
    "    'Steel':np.nan,\n",
    "    'Al':0.4,\n",
    "    'Cu':0.2,\n",
    "    'Ni':0.7,\n",
    "    'Pb':0.7,\n",
    "    'Zn':np.nan,\n",
    "    'Au':922,\n",
    "    'Sn':np.nan,\n",
    "    'Te':np.nan,\n",
    "    'W':np.nan,\n",
    "    'Ag':15.7,\n",
    "    'Ta':np.nan\n",
    "})\n",
    "\n",
    "old_scrap_impact = pd.Series({\n",
    "    'Steel':np.nan,\n",
    "    'Al':1.4,\n",
    "    'Cu':1.8,\n",
    "    'Ni':0.7,\n",
    "    'Pb':0.7,\n",
    "    'Zn':np.nan,\n",
    "    'Au':922,\n",
    "    'Sn':np.nan,\n",
    "    'Te':np.nan,\n",
    "    'W':np.nan,\n",
    "    'Ag':15.7,\n",
    "    'Ta':np.nan\n",
    "})# https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0101298 \n",
    "bar_co2 = bar.mul(mining_impact_factors.loc[mining_impact_factors.index.isin(bar.index)],axis=0)\n",
    "alpha_bar_co2 = alpha_bar.mul(mining_impact_factors.loc[\n",
    "    mining_impact_factors.index.isin(alpha_bar.index)],axis=0)\n",
    "# bar = bar.div(bar['Firm consumption (kt)'],axis=0)\n",
    "\n",
    "# Mining metal reduction:\n",
    "fig,ax = easy_subplots(2, width_ratios=[3,4], width_scale=0.7, height_scale=1.1)\n",
    "bar.loc[['Steel','Al','Cu']].plot.bar(ax=ax[0], legend=False).grid(False)\n",
    "bar.loc[[i for i in bar.index if i not in ['Steel','Al','Cu']]].plot.bar(\n",
    "    log=False, ax=ax[1], legend=False).grid(False)\n",
    "alpha_bar.loc[['Steel','Al','Cu']].plot.bar(ax=ax[0], legend=False, alpha=0.3).grid(False)\n",
    "alpha_bar.loc[[i for i in alpha_bar.index if i not in ['Steel','Al','Cu']]].plot.bar(\n",
    "    log=False, ax=ax[1], legend=False, alpha=0.3).grid(False)\n",
    "ax[0].set(ylabel='Potential mining reduction (kt metal content)')\n",
    "ax2 = ax[1].twinx()\n",
    "ax2.set_ylabel('Potential mining reduction (kt)', labelpad=40)\n",
    "ax[1].tick_params(right=True, labelright=True, labelleft=False, left=False)\n",
    "ax2.tick_params(right=False, labelright=False, labelleft=False, left=False)\n",
    "ax2.grid(False)\n",
    "fig.tight_layout(pad=0)\n",
    "fig.suptitle('100% recycled material → Potential mining reductions',weight='bold',y=1.06,fontsize=24)\n",
    "fig.set_dpi(350)\n",
    "\n",
    "# Mining CO2 reduction:\n",
    "fig,ax=plt.subplots(figsize=(7,6))\n",
    "bar_co2.plot.bar(legend=False,ax=ax).grid(axis='x')\n",
    "alpha_bar_co2.plot.bar(legend=False,ax=ax, alpha=0.3).grid(axis='x')\n",
    "mining_impact_factors\n",
    "ax.set_title('100% recycled material → Potential $CO_2$ reductions',pad=20)\n",
    "ax.set_ylabel('Potential $CO_2$ emissions avoided (kt)')\n",
    "fig.set_dpi(350)\n",
    "\n",
    "# Mining ore volumes:\n",
    "grade = data.groupby('Commodity').mean()['Grade']\n",
    "bar_ore = bar.div(grade.loc[grade.index.isin(bar.index)]/100, axis=0)\n",
    "alpha_bar_ore = alpha_bar.div(grade.loc[grade.index.isin(bar.index)]/100, axis=0)\n",
    "fig, ax = plt.subplots()\n",
    "bar_ore.plot.bar(ax=ax, legend=False).grid(axis='x')\n",
    "alpha_bar_ore.plot.bar(ax=ax, legend=False, alpha=0.3).grid(axis='x')\n",
    "ax.set(ylabel='Potential ore processing red. (kt)', title='100% recycled material → Potential ore reductions')\n",
    "fig.set_dpi(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a7c72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(bar_co2+alpha_bar_co2).rename(columns={\n",
    "    'Fruity mining change (kt), scrap demand only':'Fruity CO2 change (kt), scrap demand only',\n",
    "    'Fruity mining change (kt), scrap demand + collection':'Fruity CO2 change (kt), scrap demand + collection'\n",
    "}).to_csv('mean_co2_change_per_year.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49015f98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logx = True\n",
    "fig,ax=plt.subplots(figsize=(6,5.5))\n",
    "out_table_plot = out_table.copy()\n",
    "out_table_plot = out_table_plot.rename({'W (WO3 eq.)':'W', 'Ta (Ta2O5 eq)':'Ta'})\n",
    "out_table_plot.columns = ['Firm consumption','Total consumption','Apple market share','Min. for impact, demand',\n",
    "                          'Min. for impact, demand (80% conf.)',\n",
    "                          'Median efficiency, demand', 'Fruity efficiency, demand',\n",
    "                          'Fruity mining change (kt), demand',\n",
    "                          'Fruity mining std (kt), demand',\n",
    "                          'Fruity mining change (%), demand',\n",
    "                          'p-value, demand',\n",
    "                          'Min. for impact, demand + collection', \n",
    "                          'Min. for impact, demand + collection (80% conf.)',\n",
    "                          'Median efficiency, demand + collection', 'Fruity efficiency, demand + collection',\n",
    "                          'Fruity mining change (kt), demand + collection',\n",
    "                          'Fruity mining std (kt), demand + collection',\n",
    "                          'Fruity mining change (%), demand + collection',\n",
    "                          'p-value, demand + collection'\n",
    "                         ]\n",
    "if logx:\n",
    "    out_table_plot = np.log10(out_table_plot)\n",
    "sns_table1 = out_table_plot[\n",
    "    ['Apple market share']+[i for i in out_table_plot.columns if 'Min.' in i]\n",
    "].stack().reset_index().replace({'W (WO3 eq.)':'W', 'Ta (Ta2O5 eq)':'Ta'})\n",
    "# ax.scatter(out_table.index, out_table['Percentage of total consumption'])\n",
    "# ax.scatter(out_table.index, out_table[[i for i in out_table.columns if 'Transition' in i][0]])\n",
    "# ax.scatter(out_table.index, out_table[[i for i in out_table.columns if 'Transition' in i][1]])\n",
    "\n",
    "sns_table1 = sns_table1.sort_values(by='level_0')\n",
    "sns.scatterplot(sns_table1, x=0, y='level_0', hue='level_1', style='level_1', markers=['.','D','s','X','d'],\n",
    "                s=400, \n",
    "               palette='Dark2',alpha=0)\n",
    "sns.scatterplot(sns_table1.loc[sns_table1['level_1']=='Apple market share'], \n",
    "                x=0, y='level_0', hue='level_1', style='level_1', markers=['.'],\n",
    "                s=400, \n",
    "               palette='Dark2',)\n",
    "if logx:\n",
    "    ax.set_xticks([-2,-1,0,np.log10(0.5),1, np.log10(2),np.log10(4), np.log10(20)])\n",
    "    ax.set_xticklabels([10.**i if 10.**i<1 else str(int(round(10.**i,0))) for i in ax.get_xticks()])\n",
    "ax.set(title='',\n",
    "       xlabel='Market fraction (%)',\n",
    "       ylabel=''\n",
    "      )\n",
    "xlim = ax.get_xlim()\n",
    "for comm, y in zip(sns_table1['level_0'].unique(), ax.get_yticks()):\n",
    "    ax.fill_between(out_table_plot.loc[comm,\n",
    "                                       ['Min. for impact, demand (80% conf.)','Min. for impact, demand']], \n",
    "                    [y-0.25],[y], alpha=0.4, color='#1b9e77')\n",
    "    ax.fill_between([out_table_plot.loc[comm,\n",
    "                                       'Min. for impact, demand'],40], \n",
    "                    [y-0.25],[y], alpha=1, color='#1b9e77')\n",
    "    ax.fill_between(out_table_plot.loc[comm,\n",
    "                                       ['Min. for impact, demand + collection (80% conf.)',\n",
    "                                        'Min. for impact, demand + collection']], \n",
    "                    [y+0.25],[y], alpha=0.4, color='#d95f02')\n",
    "    \n",
    "    ax.fill_between([out_table_plot.loc[comm,\n",
    "                                        'Min. for impact, demand + collection'],40], \n",
    "                    [y+0.25],[y], alpha=1, color='#d95f02')\n",
    "    \n",
    "ax.set_xlim(xlim)\n",
    "ax.set(title='Minimum market fractions\\nto significantly reduce mining')\n",
    "ax.legend('')\n",
    "fig.set_dpi(350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f696ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disp_table = out_table[[i for i in out_table.columns if 'Fruity mining change (kt)' in i]].copy()\n",
    "# disp_table = disp_table**0.5\n",
    "# disp_table /= disp_table.min().min()/0.4\n",
    "sns_table2 = out_table[\n",
    "    ['Percentage of total consumption']+[i for i in out_table.columns if 'Fruity' in i]\n",
    "].reset_index().replace({'W (WO3 eq.)':'W', 'Ta (Ta2O5 eq)':'Ta'})\n",
    "\n",
    "sns_table2\n",
    "sns.scatterplot(sns_table2.sort_values(by='Fruity mining reduction per increase in scrap demand + collection'), \n",
    "                x='Fruity mining change (%), scrap demand + collection', \n",
    "                y='Fruity mining reduction per increase in scrap demand + collection',\n",
    "                hue='index',\n",
    "                s=300,\n",
    "                palette='BuGn'\n",
    "               )\n",
    "plt.legend(loc=(1.1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b786f-73b9-4f2b-baa9-936561ee0fa7",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fruity_plots(many, commodities, dpi=400, use_rir=False, \n",
    "                 param='Mine production', years=np.arange(2019,2041), get_data_only=False):\n",
    "    \n",
    "    # getting the scrap supply/demand change information\n",
    "    scrap_params = [\n",
    "        'direct_melt_duration','direct_melt_pct_change_inc','direct_melt_pct_change_tot',\n",
    "        'secondary_refined_duration','secondary_refined_pct_change_inc','secondary_refined_pct_change_tot',\n",
    "        'collection_rate_duration','collection_rate_pct_change_inc','collection_rate_pct_change_tot',\n",
    "    ]\n",
    "    scenario_details = many.hyperparam.loc[idx[:,:,scrap_params+['hyperparam_set_group', 'hyperparam_set_number']],\n",
    "                                           'Value'].unstack().astype(float)\n",
    "    scenario_details.loc[:,scrap_params[2::3]] -= 1\n",
    "    scenario_details.loc[:,'Scrap demand'] = scenario_details.loc[\n",
    "        :,['direct_melt_pct_change_tot','secondary_refined_pct_change_tot']].sum(axis=1).replace(-2,0)\n",
    "    scenario_details.loc[:,'Scrap collected'] = scenario_details['collection_rate_pct_change_tot']\n",
    "    scenario_deets = round(scenario_details[['Scrap demand','Scrap collected']].copy()*100,3)\n",
    "\n",
    "    # getting single parameter\n",
    "    result_param = many.results[param].unstack()\n",
    "\n",
    "    # getting baseline reference scenarios for each scenario run\n",
    "    ind = scenario_deets['Scrap demand']==-2\n",
    "    n_scen = ((scenario_details['hyperparam_set_group']==1) & \n",
    "              (scenario_details['hyperparam_set_number']==0)).loc['Zn'].sum()\n",
    "    n_runs = int(scenario_details.loc['Zn'].shape[0]/n_scen)\n",
    "    baseline_ref = pd.Series(np.array([np.repeat(i*n_scen,n_scen) for i in np.arange(0,n_runs)]).flatten())\n",
    "    baseline_ref = pd.concat([baseline_ref for i in comms],axis=0,keys=comms)\n",
    "    baseline_ref.name = 'Baseline ref'\n",
    "    result_param = pd.concat([result_param,baseline_ref],axis=1)\n",
    "\n",
    "    # subtracting off baseline runs\n",
    "    diff_baseline = result_param.apply(lambda x: x-result_param.loc[idx[x.name[0],x['Baseline ref']],:], axis=1)\n",
    "    diff_baseline.drop('Baseline ref',axis=1,inplace=True)\n",
    "    diff_baseline = diff_baseline[years]\n",
    "    diff_mean = diff_baseline.mean(axis=1)\n",
    "    \n",
    "    # getting RIR\n",
    "    rir = (many.results['Scrap demand']/many.results['Total demand']).loc[idx[:,:,2020]]\n",
    "    rir.name = 'RIR'\n",
    "    \n",
    "    # getting grade\n",
    "    grade = (many.results['Mean mine grade']).loc[idx[:,:,2020]]\n",
    "    grade.name = 'Grade'\n",
    "    \n",
    "    # getting to data for plotting, concatenating on the scrap supply/demand changes and normalized results\n",
    "    data = pd.concat([\n",
    "        diff_mean,\n",
    "        diff_mean/result_param[2019]*100,\n",
    "        scenario_deets,\n",
    "        rir,\n",
    "        grade,\n",
    "    ],axis=1).rename(columns={0:'Mean change',1:'Mean change normed'})\n",
    "    data = data.loc[data['Scrap demand']!=0]\n",
    "    data = data.loc[idx[commodities,:],:]\n",
    "    data = data.reset_index().rename(columns={'level_0':'Commodity','level_1':'Scenario number'})\n",
    "    if use_rir:\n",
    "        data = data.loc[data['RIR']<0.8]\n",
    "        data.loc[:,'RIR'] = round(data['RIR'],1)\n",
    "        case_study_data = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx',index_col=0)\n",
    "        for comm in commodities:\n",
    "            min_rir = case_study_data.loc['Recycling input rate, Global'][comm]\n",
    "            data.loc[(data['RIR']<min_rir) & (data['Commodity']==comm), 'RIR'] = min_rir\n",
    "    data = data.loc[data['Mean change normed']>-100]\n",
    "    if get_data_only:\n",
    "        return data, data\n",
    "    \n",
    "    # plot initialization\n",
    "    if len(commodities)>1 and len(commodities)<4:\n",
    "        fig, a = plt.subplots(figsize=(18,4))\n",
    "        kwd_args = {'hue':'Commodity','errwidth':3}\n",
    "    elif len(commodities)<4:\n",
    "        fig, a = plt.subplots(figsize=(14,4))\n",
    "        kwd_args = {}\n",
    "    else:\n",
    "        fig,a = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    # plotting\n",
    "    if use_rir==False and len(commodities)<4:\n",
    "        sns.barplot(data, ax=a,\n",
    "                    x='Scrap demand', y='Mean change normed', **kwd_args)\n",
    "        ticks = np.sort(data['Scrap demand'].unique())\n",
    "        ticks = [x if x<1 else int(round(x,0)) for x in ticks]\n",
    "        a.set_xticklabels(ticks);\n",
    "    elif len(commodities)<4:\n",
    "        sns.lineplot(data, ax=a,\n",
    "                    x='RIR', y='Mean change normed', hue='Commodity')\n",
    "    else:\n",
    "        heatmap_data = data.groupby(['Commodity', 'Scrap demand']).median()['Mean change normed'].unstack()\n",
    "        sns.heatmap(heatmap_data.sort_values(by=40), ax=a,\n",
    "            cbar_kws={'label':'Annual mine production\\ndifference (% 2019 value)'}\n",
    "        )\n",
    "\n",
    "    # labeling\n",
    "    collection_effort = 'No additional collection effort' if many==many_d else 'Equivalent collection effort'\n",
    "    xlabel = 'Additional scrap demand (% of annual demand)' if use_rir==False else 'Recycling input rate'\n",
    "    if len(commodities)<4:\n",
    "        a.set(title=f'Impact of increasing recycling content on mine production\\n{collection_effort}',\n",
    "                   ylabel='Annual mine production\\ndifference (% 2019 value)',xlabel=xlabel)\n",
    "    else:\n",
    "        ticks = a.get_xticks()\n",
    "        if len(ticks)>=heatmap_data.shape[1]:\n",
    "            a.set_xticks(ticks[::2])\n",
    "        a.set_xticklabels([i if i<1 else int(i) for i in heatmap_data.columns[::2]], rotation=0);\n",
    "        a.set(title=f'Impact of increasing recycling content on mine production\\n{collection_effort}',\n",
    "              xlabel=xlabel)\n",
    "    fig.set_dpi(400)\n",
    "    return fig, data\n",
    "    \n",
    "comms = many.results.index.get_level_values(0).unique()\n",
    "fig_list = []\n",
    "data_list = []\n",
    "for many in [many_d, many_s]:\n",
    "    for commodities in [['Al','Au','Cu'], ['Al'], comms]:\n",
    "        use_rirs = [False,True] if len(commodities)==3 else [False]\n",
    "        for use_rir in use_rirs:\n",
    "            fig,data = fruity_plots(many, commodities, dpi=400, use_rir=use_rir)\n",
    "            fig_list += [fig]\n",
    "            data_list += [data]\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99775fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_list[0].axes[0].legend(loc='lower left')\n",
    "fig_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab78c7f7",
   "metadata": {},
   "source": [
    "## Fruity feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d14583",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from skopt import Optimizer\n",
    "\n",
    "def get_case_study_data():\n",
    "    case_study = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx', index_col=0)\n",
    "    case_study = case_study[many.results.index.get_level_values(0).unique()]\n",
    "    precious = case_study.copy()[['Au','Ag']]\n",
    "    for i,j in zip(['industrial','transport','construction'],\n",
    "                   ['bar and coin','jewelry','industrial']):\n",
    "        cols = [k for k in precious.index if i in k]\n",
    "        rename_dict = dict(zip(cols, [k.replace(i,j) for k in cols]))\n",
    "        precious = precious.rename(rename_dict)\n",
    "    case_study = case_study.loc[:,~case_study.columns.isin(precious.columns)]\n",
    "    case_study = pd.concat([case_study, precious],axis=1)\n",
    "    case_vars = ['historical_growth_rate','china_fraction_demand','Recycling input rate, Global',\n",
    "                 'Secondary refinery fraction of recycled content, Global','primary_production_mean',\n",
    "                 'primary_ore_grade_mean','primary_commodity_price'\n",
    "                ] + [i for i in case_study.index if 'sector_dist' in i]\n",
    "    case_study = case_study.loc[case_vars].fillna(0)\n",
    "    return case_study\n",
    "\n",
    "def tune_ml_model(many, Regressor=None):\n",
    "    \"\"\"\n",
    "    Takes in Many object, and creates attributes:\n",
    "    - best_model: best-forming ML model\n",
    "    - fi_data: StandardScaler correction of feature importance data\n",
    "    - fi_data_orig: uncorrected feature importance data\n",
    "    - X_train_df: dataframe corresponding to training data\n",
    "    - X_validate_df: dataframe corresponding to validation data\n",
    "    - X_test_df: dataframe corresponding to testing data\n",
    "    \n",
    "    Returns best_model\n",
    "    \"\"\"\n",
    "    # getting data from fruity_plots function\n",
    "    comms = many.results.index.get_level_values(0).unique()\n",
    "    fig,data = fruity_plots(many, comms, dpi=10, use_rir=False)\n",
    "    plt.close()\n",
    "    fi_data = data.set_index(['Commodity','Scenario number'])\n",
    "\n",
    "    # getting additional parameter data\n",
    "    addl_params = many.rmse_df.copy()\n",
    "    addl_params = addl_params.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n",
    "    addl_params = addl_params.loc[(addl_params.apply(lambda x: x-x.mean(), axis=1)>1e-7).any(axis=1)]\n",
    "    addl_params = addl_params.loc[[i for i in addl_params.index if i[1]!='primary_commodity_price' and \n",
    "                                  'duration' not in i[1] and 'pct_change' not in i[1] and 'hyperparam' not in i[1]]]\n",
    "    addl_params = addl_params.stack().unstack(1)\n",
    "\n",
    "    # getting case study data\n",
    "    case_study = get_case_study_data()\n",
    "    many.case_study_data = case_study.copy()\n",
    "    case_study = case_study.T.stack()\n",
    "    case_study = pd.concat([case_study for _ in many.rmse_df.columns], axis=1).stack().unstack(1)\n",
    "\n",
    "    # concatenating\n",
    "    ind = np.intersect1d(addl_params.index, fi_data.index)\n",
    "    ind = np.intersect1d(ind, case_study.index)\n",
    "    fi_data = pd.concat([fi_data, addl_params.loc[ind], case_study.loc[ind]],axis=1).fillna(0)\n",
    "    \n",
    "    # removing outliers\n",
    "    fi_data = fi_data.loc[abs(fi_data['Mean change normed'])<80]\n",
    "    many.fi_data_orig = fi_data.copy()\n",
    "\n",
    "    # Logs:\n",
    "    if (fi_data['Scrap collected']==0).any():\n",
    "        logss = ['Scrap demand','Grade']\n",
    "    else:\n",
    "        logss = ['Scrap demand','Scrap collected','Grade']\n",
    "    fi_data[logss] = np.log(fi_data[logss])\n",
    "    \n",
    "    # applying standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    fi_std = scaler.fit_transform(fi_data.values)\n",
    "    fi_data = pd.DataFrame(fi_std,fi_data.index,fi_data.columns)\n",
    "    if 'region_specific_price_response' in fi_data.columns:\n",
    "        fi_data.drop(columns='region_specific_price_response',inplace=True)\n",
    "    many.fi_data = fi_data.copy()\n",
    "\n",
    "    # reformatting for split\n",
    "    X_df = fi_data[[i for i in fi_data.columns if i not in \n",
    "                    ['Mean change normed','Mean change','Scrap collected','RIR']]]\n",
    "    y_df = fi_data['Mean change normed']\n",
    "    X, y = X_df.values, y_df.values\n",
    "\n",
    "    # train/test/validate split\n",
    "    X_train, X_test, y_train, y_test, ind_train, ind_test = train_test_split(X, y, y_df.index,\n",
    "                                                                             test_size=0.333,\n",
    "                                                                             random_state=42)\n",
    "    X_validate, X_test, y_validate, y_test, ind_validate, ind_test = train_test_split(\n",
    "        X_test, y_test, ind_test,\n",
    "        test_size=0.5,\n",
    "        random_state=43)\n",
    "    many.X_train_df = pd.DataFrame(X_train, ind_train, X_df.columns)\n",
    "    many.X_test_df = pd.DataFrame(X_test, ind_test, X_df.columns)\n",
    "    many.X_validate_df = pd.DataFrame(X_validate, ind_validate, X_df.columns)\n",
    "\n",
    "    # Tuning regression hyperparameters:\n",
    "    if Regressor is None:\n",
    "        Regressor = [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor][1]\n",
    "    elif type(Regressor)==int:\n",
    "        Regressor = [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor][Regressor]\n",
    "    elif type(Regressor)==str:\n",
    "        Regressor_dict = dict(zip(\n",
    "            ['RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor'],\n",
    "            [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor]))\n",
    "        Regressor = Regressor_dict[Regressor]\n",
    "\n",
    "    variables = {'n_estimators':(5,800), 'min_samples_leaf': (0.00000001,0.5)}\n",
    "    opt = Optimizer([variables[i] for i in variables.keys()])\n",
    "\n",
    "    rmse_min = 1e4\n",
    "    best_params = None\n",
    "    best_mod = None\n",
    "    param_list = []\n",
    "    for i in range(20):\n",
    "        if i==0:\n",
    "            suggested = [800,1e-5]\n",
    "        else:\n",
    "            suggested = opt.ask()\n",
    "        regr = Regressor(random_state=0, n_estimators=suggested[0], min_samples_leaf=suggested[1], n_jobs=3)\n",
    "        regr.fit(X_train, y_train)\n",
    "        mse = mean_squared_error(regr.predict(X_validate), y_validate)**0.5\n",
    "        opt.tell(suggested, mse)\n",
    "        param_list += [suggested]\n",
    "        if mse<rmse_min:\n",
    "            rmse_min = mse\n",
    "            best_params = suggested\n",
    "            best_mod = regr\n",
    "        print('iteration:', i, suggested, mse)\n",
    "    print('Best params:', best_params)\n",
    "    print('Train R2:', r2_score(y_train, regr.predict(X_train)))\n",
    "    print('Validation R2:', r2_score(y_validate, regr.predict(X_validate)))\n",
    "    print('Test R2:', r2_score(y_test, regr.predict(X_test)))\n",
    "    many.best_mod = best_mod\n",
    "    return best_mod\n",
    "\n",
    "for many in [many_s, many_d]:\n",
    "    tune_ml_model(many)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7b3a7f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "case_study_only = False\n",
    "n_params = 15\n",
    "\n",
    "importances = pd.concat([\n",
    "    pd.Series(many_s.best_mod.feature_importances_, many.X_test_df.columns),\n",
    "    pd.Series(many_d.best_mod.feature_importances_, many.X_test_df.columns)\n",
    "], axis=1, keys=['Supply','Demand'])\n",
    "impor_heat = importances.copy()#.drop('Scrap demand')\n",
    "actual_sum = impor_heat.sum()\n",
    "ind = np.intersect1d(impor_heat.index, many_s.case_study_data.index)\n",
    "if case_study_only:\n",
    "    impor_heat = impor_heat.loc[ind]\n",
    "impor_heat = impor_heat.div(actual_sum,axis=1)\n",
    "impor_heat = impor_heat.sort_values(by='Supply',ascending=False).head(n_params)\n",
    "impor_heat = impor_heat.rename(index=make_parameter_names_nice(impor_heat.index))\n",
    "impor_heat = impor_heat.stack().reset_index()\n",
    "impor_heat = impor_heat.rename(columns={'level_0':'Parameter','level_1':'Scrap scenario',0:'Feature importance'})\n",
    "impor_heat = impor_heat.replace({'Supply':'Increasing demand + collection', 'Demand':'Increasing demand only'})\n",
    "# sns.heatmap(impor_heat, ax=ax,\n",
    "#             yticklabels=True,\n",
    "#            )\n",
    "orient = 'v'\n",
    "figsize = [8,5+impor_heat['Parameter'].unique().shape[0]*11/40]\n",
    "if orient=='h':\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    kwargs = {'y':'Parameter', 'x':'Feature importance', 'orient':orient}\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=figsize[::-1])\n",
    "    kwargs = {'x':'Parameter', 'y':'Feature importance', 'orient':orient}\n",
    "sns.barplot(impor_heat, \n",
    "            hue='Scrap scenario', ax=ax, **kwargs)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90);\n",
    "ax.legend(title='Scrap scenario', alignment='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fdd501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f6e905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_param_names(v):\n",
    "    demand = ['sector_specific_dematerialization_tech_growth', 'sector_specific_price_response',\n",
    "              'intensity_response_to_gdp', 'direct_melt_elas_scrap_spread']\n",
    "    mine_production = ['primary_oge_scale', 'mine_cu_margin_elas', 'mine_cost_og_elas',\n",
    "                       'mine_cost_change_per_year', 'mine_cost_price_elas']\n",
    "    incentive = ['initial_ore_grade_decline',\n",
    "                 'incentive_opening_probability', 'close_years_back', 'primary_price_resources_contained_elas',\n",
    "                 'reserves_ratio_price_lag', 'mine_cost_tech_improvements',\n",
    "                 'incentive_mine_cost_change_per_year']\n",
    "    price = ['primary_commodity_price_elas_sd', 'scrap_spread_elas_primary_commodity_price', \n",
    "             'scrap_spread_elas_sd', 'tcrc_elas_price','tcrc_elas_sd']\n",
    "    refinery = ['pri CU TCRC elas', 'pri CU price elas', 'refinery_capacity_fraction_increase_mining', \n",
    "                'sec CU TCRC elas', 'sec CU price elas', 'sec ratio TCRC elas', 'sec ratio scrap spread elas',]\n",
    "    scrap_avail = ['collection_elas_scrap_price']\n",
    "\n",
    "    if v in mine_production or v in incentive:\n",
    "        return ('Mining production', make_parameter_names_nice([v])[v])\n",
    "    elif v in demand:\n",
    "        return ('Demand response', make_parameter_names_nice([v])[v])\n",
    "    elif v in incentive:\n",
    "        return ('Reserve development', make_parameter_names_nice([v])[v])\n",
    "    elif v in price:\n",
    "        return ('Price formation', make_parameter_names_nice([v])[v])\n",
    "    elif v in refinery:\n",
    "        return ('Refinery operation', make_parameter_names_nice([v])[v])\n",
    "    elif v in scrap_avail:\n",
    "        return ('Secondary supply', make_parameter_names_nice([v])[v])\n",
    "    elif v == 'Scrap demand':\n",
    "        return ('Scrap demand', 'Scrap demand')\n",
    "    else:\n",
    "        return ('Input parameters', make_parameter_names_nice([v])[v])\n",
    "        \n",
    "grouped_import = importances.rename(\n",
    "    dict(zip(importances.index,[convert_param_names(i) for i in importances.index])))\n",
    "grouped_import.index = pd.MultiIndex.from_tuples(grouped_import.index)\n",
    "grouped_import = grouped_import.groupby(level=0).sum()\n",
    "plt.pie(grouped_import['Supply'], labels=grouped_import.index, autopct='%1.1f%%');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a65cb",
   "metadata": {
    "code_folding": [
     10,
     76,
     149
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ph = grouped_import['Demand']\n",
    "ph /= ph.sum()/100\n",
    "ph = ph.sort_values(ascending=False)\n",
    "tot_area = 100\n",
    "height, width = 10, 10\n",
    "debug=False\n",
    "first_height_fraction = 1\n",
    "figsize=(27,12)\n",
    "annotate=False # False, 's', 'n' (string, number)\n",
    "\n",
    "def get_width_height(area, n, rects):\n",
    "    repeat_flag=False\n",
    "    if area>5 and n==0:\n",
    "        h = height * first_height_fraction\n",
    "        w = area/h\n",
    "    elif n>0:\n",
    "        last_rect_end_x, last_rect_end_y = rects[n-1][1]\n",
    "        last_rect_start_x, last_rect_start_y = rects[n-1][0]\n",
    "        last_has_one_right = np.any([rects[i][0][0]>last_rect_start_x for i in rects if rects[i][0][1]==height])\n",
    "        \n",
    "        if last_rect_end_y>0 and last_rect_end_x<=rects[0][1][0]:\n",
    "            if debug: print('a')\n",
    "            h = last_rect_end_y\n",
    "            w = area/h\n",
    "        elif last_rect_end_y>0 and (last_rect_start_y-last_rect_end_y)*(width-last_rect_end_x)>area and \\\n",
    "         area/(last_rect_end_y)+last_rect_end_x < width:\n",
    "            if debug: print('b', last_rect_end_x, area/last_rect_end_y)\n",
    "            h = last_rect_end_y\n",
    "            w = area/h\n",
    "        elif width-last_rect_end_x<2 and width-last_rect_end_x>0 and \\\n",
    "         last_rect_start_y*(width-last_rect_end_x)>area and last_rect_start_y==height and not last_has_one_right:\n",
    "            if debug: print('c')\n",
    "            w = width-last_rect_end_x\n",
    "            h = area/w\n",
    "        elif last_rect_start_y<height and last_rect_start_y*(width-last_rect_end_x)>area:\n",
    "            if debug: print('d')\n",
    "            h = last_rect_start_y\n",
    "            w = area/h\n",
    "        elif area>10 and last_rect_end_x+area/(height*0.66) < width:\n",
    "            if debug: print('e')\n",
    "            h = height*0.66\n",
    "            w = area/h\n",
    "        else:\n",
    "            if debug: print('f')\n",
    "            farthest_x = [rects[i][1][0] for i in rects if rects[i][0][1]==height][-1]\n",
    "            bottoms = [rects[i][1][1] for i in rects if rects[i][0][1]==height]\n",
    "            bottoms_x = [rects[i][1][0] for i in rects if rects[i][1][1]==0]\n",
    "            \n",
    "            w = width-farthest_x\n",
    "            if w!=0:\n",
    "                h = area/w\n",
    "                \n",
    "            if farthest_x == width and bottoms[-1]==0:\n",
    "                if debug: print('g')\n",
    "                next_farthest_x = np.max([rects[i][1][0] for i in rects if rects[i][0][1]==height][:-1])\n",
    "                w = last_rect_start_x-next_farthest_x\n",
    "                h = area/w\n",
    "            elif (w==0 or h > height):\n",
    "                if debug: print('h')\n",
    "                h = np.min([b for b in bottoms if b!=0])\n",
    "#                 h = last_rect_end_y\n",
    "                w = area/h\n",
    "            elif height-h < np.max(bottoms):\n",
    "                if debug: print('i')\n",
    "                farthest_x = np.max([rects[i][1][0] for i in rects])\n",
    "                w = width-farthest_x\n",
    "                if w!=0:\n",
    "                    h = area/w\n",
    "                    if h>height:\n",
    "                        repeat_flag=True\n",
    "                else:\n",
    "                    w = last_rect_end_x-last_rect_start_x\n",
    "                    h = area/w\n",
    "        \n",
    "    return w, h, repeat_flag\n",
    "    \n",
    "def handle_repeat_flag(e, ph, colors, w, h, start=0):\n",
    "    end = 0\n",
    "    colors1 = colors[:e+1]\n",
    "    colors2 = colors[e:]\n",
    "    colors = colors1+colors2\n",
    "    ph1 = ph.iloc[:e+1]\n",
    "    ph2 = ph.iloc[e:]\n",
    "    old_area = h*w\n",
    "    if h>height:\n",
    "        h = height\n",
    "    else:\n",
    "        w = width-start[0]\n",
    "        end = (start[0]+w, start[1]-h)\n",
    "    new_area = h*w\n",
    "    ph1.iloc[e] = h*w\n",
    "    ph2.iloc[0] = old_area-new_area\n",
    "    ph = pd.concat([ph1,ph2])\n",
    "    \n",
    "    return w, h, colors, ph, start, end\n",
    "\n",
    "def get_rect_start_end(rects, w, h, n):\n",
    "    repeat_flag = False\n",
    "    if n!=0:\n",
    "        last_start = rects[n-1][0]\n",
    "        last_end = rects[n-1][1]\n",
    "    if n==0:\n",
    "        start = (0,10)\n",
    "    elif last_end[1]==0 and last_end[0]+w<width and last_start[1]-h>0:\n",
    "        if debug: print(1)\n",
    "        start = (last_end[0],last_start[1])\n",
    "    elif last_end[1]-h>=0 and n==1:\n",
    "        if debug: print(2)\n",
    "        x = [rects[i][1][0] for i in rects if rects[i][1][1]==0]\n",
    "        x = last_start[0] if len(x)==0 else x[-1]\n",
    "        start = (x,last_end[1])\n",
    "    elif last_end[0]+w==width and last_start[1]==height:\n",
    "        if debug: print(3)\n",
    "        start = (last_end[0],height)\n",
    "    elif last_end[0]+w==width:\n",
    "        if debug: print(4)\n",
    "        start = (last_end[0],last_end[1]+h)\n",
    "    else:\n",
    "        if debug: print(5)\n",
    "        farthest_x = [rects[i][1][0] for i in rects if rects[i][0][1]==height][-1]\n",
    "        if height-h>0 and farthest_x+w<width:\n",
    "            start = (farthest_x, height)\n",
    "        elif height-h>0:\n",
    "            farthest_x = [rects[i][1][0] for i in rects if rects[i][1][1]==0][-1]\n",
    "            start = (farthest_x, h)\n",
    "        if farthest_x == width:\n",
    "            if debug: print('5.1')\n",
    "            next_farthest_x = np.max([rects[i][1][0] for i in rects if rects[i][0][1]==height][:-1])\n",
    "            start = (next_farthest_x, height)\n",
    "        elif farthest_x+w > width:\n",
    "            if debug: print('5.2')\n",
    "            bottoms_of_tops = [rects[i][1][1] for i in rects if rects[i][0][1]==height and rects[i][1][1]!=0]\n",
    "            rights_of_tops = [rects[i][1][0] for i in rects if rects[i][0][1]==height and rects[i][1][1]!=0]\n",
    "            bottoms_of_tops, rights_of_tops = bottoms_of_tops[::-1], rights_of_tops[::-1] \n",
    "            start = (rights_of_tops[np.argmin(bottoms_of_tops)], np.max(bottoms_of_tops))\n",
    "            area = w*h\n",
    "            h = np.max(bottoms_of_tops)-np.min(bottoms_of_tops)\n",
    "            if h!=0:\n",
    "                w = area/h\n",
    "            else:\n",
    "                h = height*0.66\n",
    "                w = area/h\n",
    "            repeat_flag=True\n",
    "            if start[0]+w > width:\n",
    "                repeat_flag=True\n",
    "    if np.any([start[0]==rects[i][0][0] for i in rects]) and np.any([start[1]==rects[i][0][1] for i in rects]):\n",
    "        start = (last_start[0], last_end[1])\n",
    "    end = (start[0]+w, start[1]-h)\n",
    "    return start, end, repeat_flag\n",
    "    \n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('error')\n",
    "    colors = list(sns.color_palette('deep', n_colors=ph.shape[0]))\n",
    "    linecolor='k'\n",
    "    lw=2\n",
    "    fig,ax = plt.subplots(figsize=figsize)\n",
    "    rects = {}\n",
    "    e = 0\n",
    "    repeat_flag_tripped = False\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            repeat_flag_tripped = False\n",
    "            i = ph.index[e]\n",
    "            if debug: print(i)\n",
    "            w, h, repeat_flag = get_width_height(ph.iloc[e], e, rects)\n",
    "            if repeat_flag:\n",
    "                repeat_flag_tripped = True\n",
    "                w, h, colors, ph, start, end = handle_repeat_flag(e, ph, colors, w, h)\n",
    "            start, end, repeat_flag = get_rect_start_end(rects, w, h, e)\n",
    "            if repeat_flag:\n",
    "                repeat_flag_tripped=True\n",
    "                h = start[1]-end[1]\n",
    "                w = end[0]-start[0]\n",
    "                w, h, colors, ph, start, end = handle_repeat_flag(e, ph, colors, w, h, start)\n",
    "            rects[e] = (start, end, repeat_flag_tripped)\n",
    "            ax.fill_between((start[0],end[0]),(start[1],start[1]),(end[1],end[1]), color=colors[e])\n",
    "            if annotate:\n",
    "                if annotate=='n':\n",
    "                    ax.annotate(str(e), (np.mean([start[0],end[0]]), np.mean([start[1],end[1]])))\n",
    "                elif annotate=='s':\n",
    "                    ax.annotate(str(i), (np.mean([start[0],end[0]]), np.mean([start[1],end[1]])))\n",
    "            e += 1\n",
    "        except IndexError:\n",
    "            break\n",
    "    for rect in rects:\n",
    "        start, end, repeat_flag_tripped = rects[rect]\n",
    "        if not repeat_flag_tripped and (rect<1 or not rects[rect-1][2]):\n",
    "            ax.hlines([start[1],end[1]], start[0], end[0], color=linecolor, linewidth=lw)\n",
    "            ax.vlines([start[0],end[0]], start[1], end[1], color=linecolor, linewidth=lw)\n",
    "    ax.vlines([0,width], 0, height, color=linecolor, linewidth=lw)\n",
    "    ax.hlines([0,height], 0, width, color=linecolor, linewidth=lw)\n",
    "    ax.axis('off')\n",
    "    fig.set_dpi(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph, 100-ph[[i for i in ph.index if i!='Input parameters']].sum() # supply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ph # demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1270913",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "for many in [many_s, many_d]:\n",
    "    regr = many.best_mod\n",
    "    explainer = shap.TreeExplainer(regr)\n",
    "#     explainer_copy = deepcopy([explainer])[0]\n",
    "    shap_values_object = explainer(many.X_test_df.values)\n",
    "    shap_values = shap_values_object.values\n",
    "    shap_values_df = pd.DataFrame(shap_values, many.X_test_df.index, many.X_test_df.columns)\n",
    "    many.shap_values_df = shap_values_df.copy()\n",
    "# shap_interaction_values = explainer_copy.shap_interaction_values(data)\n",
    "# x = shap_interaction_values\n",
    "# data_ph = X_df.loc[ind_test].fillna(0).stack()\n",
    "# if self.commodity!=None:\n",
    "#     self.shap_interaction_df = pd.DataFrame(x.reshape(x.shape[0]*x.shape[1],x.shape[2]), data_ph.index, data.columns)\n",
    "# else:\n",
    "#     self.shap_interaction_df = pd.DataFrame(x.reshape(x.shape[0]*x.shape[1],x.shape[2]), data_ph.index, data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab47ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "many_s.fi_data = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_s_fi_data.csv', index_col=[0,1])\n",
    "many_s.fi_data_orig = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_s_fi_data_orig.csv', index_col=[0,1])\n",
    "many_s.shap_values_df = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_s_shap.csv', index_col=[0,1])\n",
    "\n",
    "many_d.fi_data = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_d_fi_data.csv', index_col=[0,1])\n",
    "many_d.fi_data_orig = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_d_fi_data_orig.csv', index_col=[0,1])\n",
    "many_d.shap_values_df = pd.read_csv(\n",
    "    'generalization/output_files/simulation_analysis/many_d_shap.csv', index_col=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e532c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_d.shap_values_df.groupby(level=0).mean().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6c3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP is explaing the Mean change normed column from fi_data, which represents the \n",
    "# mean mine production change from baseline, divided by the 2019 historical mine production value\n",
    "# and multiplied by 100 \n",
    "init_plot2(fontsize=24)\n",
    "many = many_d\n",
    "shap_vals = many.shap_values_df.groupby(level=0).median()\n",
    "shap_vals.loc['All'] = shap_vals.median()\n",
    "exclude = ['china_fraction_demand','sector_dist_electrical']\n",
    "shap_vals.drop(exclude,axis=1,inplace=True)\n",
    "ind = (shap_vals.max()-shap_vals.min()).sort_values()[:-1].tail(7).index[::-1]\n",
    "shaps = shap_vals[ind].drop('All')\n",
    "# shaps = shap_vals.sort_values(by='All',axis=1, ascending=False).drop('All').iloc[:,1:10]\n",
    "shaps = shaps.loc[out_table.index.sort_values()]\n",
    "# shaps = shaps[['scrap_spread_elas_sd','mine_cost_change_per_year','primary_price_resources_contained_elas']]\n",
    "vals = many.fi_data_orig.copy()\n",
    "vals = vals.groupby(level=0).median().loc[shaps.index, shaps.columns]\n",
    "\n",
    "if False:\n",
    "    fig,ax=plt.subplots(figsize=(8,7))\n",
    "    sns.heatmap(shaps.T.rename(fruity_rename), ax=ax, \n",
    "                cbar_kws={'label':'Larger mining reduction →'}, cmap='RdBu_r')\n",
    "    ax.tick_params(labelbottom=True,labeltop=True)\n",
    "    fig.set_dpi(50)\n",
    "    \n",
    "if True:\n",
    "    fig,ax=easy_subplots()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af00314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "many.fi_data_orig['Mean change normed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5116bcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "many = many_d\n",
    "import scipy.interpolate as interpolate\n",
    "\n",
    "cols = ['incentive_opening_probability', 'scrap_spread_elas_sd', \n",
    "        'tcrc_elas_sd','direct_melt_elas_scrap_spread']\n",
    "xlabels = ['Fraction', '% price change for 1% S-D ratio change', '% price change for 1% S-D ratio change',\n",
    "           '% use change for 1% scrap price change']\n",
    "# cols = many.shap_values_df.columns\n",
    "X_test_df_orig = many.fi_data_orig.loc[many.shap_values_df.index, cols]\n",
    "X_test_df = many.fi_data.loc[many.shap_values_df.index, cols]\n",
    "slopes = pd.Series(np.nan, cols)\n",
    "fig,ax = easy_subplots(cols,2, width_scale=1.1)\n",
    "for col,a,color,xlab in zip(cols,ax,mpl.color_sequences['Dark2'],xlabels):\n",
    "    x = X_test_df_orig[col]\n",
    "    y = many.shap_values_df[col].copy()\n",
    "    y *= many.fi_data_orig['Mean change normed'].std()\n",
    "    x_c = x.groupby(level=0).mean().loc[out_table.index]\n",
    "    y_c = y.groupby(level=0).mean().loc[out_table.index]\n",
    "    \n",
    "    l,h = 0.9, 1.1\n",
    "    x = x.loc[(x>x_c.min()*l) & (x<x_c.max()*h)]\n",
    "    y = y.loc[x.index]\n",
    "#     l,h = x_c.min()*0.9, x_c.max()*1.1\n",
    "    a.scatter(x=x, y=y, alpha=0.1, color=color)\n",
    "    \n",
    "#     a.scatter(x_c, y_c, color='k')\n",
    "    \n",
    "    ph = pd.Series(y.values,x.values).sort_index()\n",
    "    t, c, k = interpolate.splrep(ph.index, ph.values, s=4e6, k=4)\n",
    "    N = 100\n",
    "    xmin, xmax = x.min(), x.max()\n",
    "    xx = np.linspace(xmin, xmax, N)\n",
    "    spline = interpolate.BSpline(t, c, k, extrapolate=False)\n",
    "    a.plot(xx, spline(xx), color='k')\n",
    "    a.set(xlabel=xlab, ylabel='Larger mining reduction →', title=fruity_rename[col])\n",
    "    if col in ['intensity_response_to_gdp','sector_specific_price_response']:\n",
    "        a.set(ylim=(-.2,.2))\n",
    "    m = sm.GLS(many.shap_values_df[col], sm.add_constant(X_test_df[col])).fit(cov_type='HC3')\n",
    "    slopes.loc[col] = m.params[col]\n",
    "    \n",
    "    print('\\n',col)\n",
    "    print(x_c.sort_values().index)\n",
    "#     a.set_xlim(l,h)\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90da0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cba86b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990aad42",
   "metadata": {
    "code_folding": [
     28
    ]
   },
   "outputs": [],
   "source": [
    "commodities = ['Cu','Ta']\n",
    "which='Supply'\n",
    "annotate=False\n",
    "many=many_d if which=='Demand' else many_s\n",
    "fig, ax = easy_subplots(commodities, use_subplots=True, sharex=True, width_scale=1, height_scale=1.1)\n",
    "for commodity,a in zip(commodities, ax):\n",
    "    most_important_params = importances[which].sort_values(ascending=False).drop('Scrap demand').head(8).index\n",
    "    for_waterfall = many.shap_values_df.sort_index().loc[commodity].loc[:,most_important_params]\n",
    "    for_waterfall = for_waterfall.mean()*100\n",
    "    for_waterfall.rename(fruity_rename, inplace=True)\n",
    "    for_waterfall.rename(dict(zip(for_waterfall.index,\n",
    "                                  [i.replace('\\n',' ') for i in for_waterfall.index])),inplace=True)\n",
    "    for_waterfall = for_waterfall.reset_index()\n",
    "    for_waterfall['>0'] = for_waterfall[0]>0\n",
    "    important_hyperparam = many.fi_data_orig.sort_index().loc[\n",
    "        commodity].loc[:,most_important_params].astype(float)\n",
    "    important_hyperparam = important_hyperparam.mean()\n",
    "    \n",
    "    \n",
    "    sns.barplot(for_waterfall.reset_index(), ax=a, x=0, y='index', hue='>0', dodge=False, \n",
    "                palette=['blue','red'])\n",
    "    a.grid(axis='x')\n",
    "    a.set_xlim(-5.05,11 if which=='Supply' else 13)\n",
    "    a.legend('')\n",
    "    yvals = a.get_yticks()\n",
    "    xmin,xmax = a.get_xlim()\n",
    "    # Annotation:\n",
    "    if annotate:\n",
    "        for y,i,x in zip(yvals, important_hyperparam.round(2), for_waterfall[0]):\n",
    "            annot_color = 'k'\n",
    "    #         if x<xmax*0.4 and x>0: \n",
    "    #             x=x*2+xmax*0.4\n",
    "    #             annot_color='k'\n",
    "    #         elif x>xmin*0.4 and x<0: \n",
    "    #             x=x*2-xmax*0.4\n",
    "    #             annot_color='k'\n",
    "            if a==ax[0]:\n",
    "                x = 9 if which=='Supply' else 11\n",
    "            elif a==ax[1]:\n",
    "                x = -3\n",
    "            a.annotate(i, (x, y), horizontalalignment='center', verticalalignment='center', color=annot_color)\n",
    "\n",
    "    a.tick_params(axis='y', which='both', left=a==ax[0], right=True)\n",
    "    a.set(title=commodity.replace('Cu','Copper').replace('Ta','Tantalum'), \n",
    "          xlabel='Change in mine production\\ndue to parameter value (%)')\n",
    "    a.set_ylabel('')\n",
    "    a.set_xticks([-5,0,5,10])\n",
    "\n",
    "ax[1].set_yticklabels([])\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386eab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.tick_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb4b82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_pie_importance(which, importances):\n",
    "    pie_importance = importances[which].sort_values(ascending=False)\n",
    "    pie_importance = pie_importance.head(16)\n",
    "    if 'incentive_mine_cost_change_per_year' in pie_importance.index:\n",
    "        pie_importance.drop('incentive_mine_cost_change_per_year',inplace=True)\n",
    "    if 'initial_ore_grade_decline' in pie_importance.index:\n",
    "        pie_importance.drop('initial_ore_grade_decline',inplace=True)\n",
    "    pie_importance.loc['Other'] = 1-pie_importance.sum()\n",
    "    \n",
    "    pie_importance.rename(fruity_rename, inplace=True)\n",
    "    if which=='Demand':\n",
    "        pie_importance.rename({'Scrap demand':'Size of scrap\\ndemand increase'}, inplace=True)\n",
    "    else:\n",
    "        pie_importance.rename({'Scrap demand':'Size of scrap demand\\n& collection increase'}, inplace=True)\n",
    "    return pie_importance\n",
    "\n",
    "which = 'Demand'\n",
    "pie_importance = get_pie_importance(which, importances)\n",
    "fig,ax = plt.subplots()\n",
    "ax.pie(pie_importance, labels=pie_importance.index, rotatelabels=True, colors=sns.color_palette('Dark2',n_colors=9,desat=0.8),\n",
    "       startangle=-59 if which=='Demand' else -66, textprops={'fontname':'Hiragino Sans GB'});\n",
    "fig.set_dpi(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6b0b8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "which = 'Demand'\n",
    "percentage = 10\n",
    "pie_importance = get_pie_importance(which, importances)\n",
    "slopes_renamed = slopes.copy()\n",
    "if which=='Demand':\n",
    "    slopes_renamed.rename({'Scrap demand':'Size of scrap\\ndemand increase'}, inplace=True)\n",
    "else:\n",
    "    slopes_renamed.rename({'Scrap demand':'Size of scrap demand\\n& collection increase'}, inplace=True)\n",
    "slopes_renamed = slopes_renamed.rename(fruity_rename)\n",
    "slopes_and_importances = pd.concat([\n",
    "    slopes_renamed.loc[pie_importance.drop('Other').index],\n",
    "    pie_importance.drop('Other')],\n",
    "    axis=1, keys=['Slopes','Importance']\n",
    ")\n",
    "slopes_and_importances['Product'] = slopes_and_importances.product(axis=1)\n",
    "\n",
    "many = many_d if which=='Demand' else many_s\n",
    "addl_params = many.fi_data_orig.copy()\n",
    "addl_params = addl_params.rename(columns=fruity_rename).groupby(level=0).median()\n",
    "fig,data = fruity_plots(many, comms, dpi=10, use_rir=False)\n",
    "plt.close()\n",
    "response = data.groupby(['Commodity','Scrap demand']).median()[\n",
    "    'Mean change normed'].loc[idx[:,percentage]]\n",
    "\n",
    "cols = np.intersect1d(addl_params.columns, slopes_and_importances.index)\n",
    "addl_params = addl_params[cols]\n",
    "addl_params = addl_params.div(abs(addl_params).max(),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e55ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ac4c4-3d0f-431b-9297-a4947211e948",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def switch_to_rir(many):\n",
    "    \"\"\"\n",
    "    need Many instance to have diff_out object\n",
    "    \"\"\"\n",
    "    msrf = many.multi_scenario_results_formatted\n",
    "    commodities = [i.lower() for i in many.diff_out['Commodity'].unique()]\n",
    "    rir = (msrf['Scrap demand']/msrf['Total demand']).loc[idx[commodities,0,2020,1,:]]\n",
    "    if rir.index.nlevels>1:\n",
    "        rir = rir.droplevel([1,2,3])\n",
    "    rir = round(rir,4)\n",
    "    rir = rir[~rir.duplicated()]\n",
    "    alt_diff_out = many.diff_out.copy()\n",
    "    init_alt_diff_out = many.diff_out.copy()\n",
    "    if rir.index.nlevels==1:\n",
    "        for i in rir.index:\n",
    "            alt_diff_out['change'] = alt_diff_out['change'].replace(round((i-1)*100,5),rir[i])\n",
    "    else:\n",
    "        for c in rir.index.get_level_values(0).unique():\n",
    "            for i in rir.loc[c].index.unique():\n",
    "                alt_diff_out.loc[alt_diff_out['Commodity']==c.capitalize(),'change'] = alt_diff_out.loc[\n",
    "                    alt_diff_out['Commodity']==c.capitalize(),'change'].replace(round((i-1)*100,5),rir[c,i])\n",
    "\n",
    "    alt_diff_out = alt_diff_out.loc[[i not in init_alt_diff_out['change'].unique() \n",
    "                                     for i in alt_diff_out['change']]]\n",
    "    return alt_diff_out\n",
    "    \n",
    "def plot_rir_lineplot(alt_diff_out, plot_separate=False, \n",
    "                      kws={'x':'change', 'y':'diff_baseline', 'errorbar':('ci',ci)}):\n",
    "    \"\"\"\n",
    "    use function switch_to_rir to get alt_diff_out\n",
    "    \"\"\"\n",
    "    if not plot_separate:\n",
    "        fig,ax=easy_subplots(1,width_scale=3,use_subplots=True,sharex=True)\n",
    "        a=ax[0]\n",
    "        kws['hue']='Commodity'\n",
    "        kws['ax']=a\n",
    "        sns.lineplot(alt_diff_out, **kws)\n",
    "    else:\n",
    "        fig,ax=easy_subplots(len(alt_diff_out['Commodity'].unique()), ncol=1, width_scale=3, \n",
    "                             use_subplots=True, sharex=True)\n",
    "        for i,a in zip(alt_diff_out['Commodity'].unique(),ax):\n",
    "            kws['ax']=a\n",
    "            sns.lineplot(alt_diff_out.loc[alt_diff_out['Commodity']==i], **kws)\n",
    "            a.set(title=i)\n",
    "    return fig,ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49480f-9211-4900-b920-206cc999722a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_multi_scenario_response_all(many,param='Mine production', divisor='Total demand', \n",
    "                                     to_year=2040, ci=95, dpi=50):\n",
    "    ph = many.multi_scenario_results.copy()\n",
    "    commodities = ph.index.get_level_values(0).unique()\n",
    "    fig_list = []\n",
    "    ax_list = []\n",
    "    for commodity in commodities:\n",
    "        fig,ax = plot_multi_scenario_response(many,commodity,dpi=dpi,param=param,divisor=divisor,to_year=to_year, ci=ci)\n",
    "        title = ax.get_title()\n",
    "        # ax.set_title(f'{commodity.capitalize()}, {title.lower()}')\n",
    "        fig_list += [fig]\n",
    "        ax_list += [ax]\n",
    "    return fig_list, ax_list\n",
    "    \n",
    "def plot_multi_scenario_response(many=None, commodity=None, multi_scenario_results=None, \n",
    "                                 multi_scenario_hyperparam=None, param='Conc. supply', \n",
    "                                 divisor='Total demand', to_year=2040, \n",
    "                                 plot_rir=False, if_line_plot_separate=False,\n",
    "                                 ci=95, dpi=50):\n",
    "    \"\"\"\n",
    "    ci: float, percent value used for confidence interval on error bars/shading, default 95%\n",
    "    \"\"\"\n",
    "    if many is None and multi_scenario_results is None and multi_scenario_hyperparam is None:\n",
    "        raise ValueError('Need either many given or both multi_scenario_results and multi_scenario_hyperparam')\n",
    "    if type(commodity)==str:\n",
    "        commodities=[commodity]\n",
    "    else: commodities = commodity\n",
    "    diff_ph_out = pd.DataFrame()\n",
    "    for commodity in commodities:\n",
    "        if many is None:\n",
    "            multi_scenario_results = multi_scenario_results.copy()\n",
    "            multi_scenario_hyperparam = multi_scenario_hyperparam.copy()\n",
    "        else:\n",
    "            multi_scenario_results = many.multi_scenario_results.loc[commodity].copy()\n",
    "            multi_scenario_hyperparam = many.multi_scenario_hyperparam.loc[commodity].copy()\n",
    "        ph = multi_scenario_hyperparam.copy()\n",
    "        ph0 = multi_scenario_hyperparam[0]\n",
    "        ph = ph.loc[[type(ph0[i])!=np.ndarray for i in ph.index]].dropna()\n",
    "        duplicated = (ph[0]==ph[1]).all()\n",
    "        if 1 in multi_scenario_results.index.get_level_values(1) and duplicated: \n",
    "            multi_scenario_results.drop(1,level=1,inplace=True)\n",
    "        if 1 in multi_scenario_hyperparam.columns and duplicated: \n",
    "            multi_scenario_hyperparam.drop(1,axis=1,inplace=True)\n",
    "        if type(to_year)==int:\n",
    "            to_year=[to_year]\n",
    "        for to_year_i in to_year:\n",
    "            ph = multi_scenario_results[param].unstack(1)\n",
    "            diff_ph = ph.subtract(ph[0],axis=0).loc[idx[:,2019:to_year_i],:]\n",
    "            many.diff_ph1 = diff_ph.copy()\n",
    "            diff_ph = diff_ph.rename(columns=dict(zip(diff_ph.columns,\n",
    "                                                 pd.MultiIndex.from_tuples([(multi_scenario_hyperparam.loc[idx[0,'secondary_refined_duration'],i],round(multi_scenario_hyperparam.loc[idx[0,['secondary_refined_pct_change_tot','direct_melt_pct_change_tot']],i].sum()-1,4)) for i in multi_scenario_hyperparam.columns]))))\n",
    "            many.diff_ph2 = diff_ph.copy()\n",
    "            diff_ph.columns = pd.MultiIndex.from_tuples(diff_ph.columns)\n",
    "            diff_ph.columns = diff_ph.columns.set_names(['duration','change'])\n",
    "            diff_ph = diff_ph.groupby(level=0).mean()\n",
    "            many.diff_ph = diff_ph.copy()\n",
    "            # diff_ph = diff_ph.sort_index().drop_duplicates().T.sort_index().drop_duplicates().T\n",
    "            diff_ph = diff_ph.stack().stack().reset_index().rename(columns={'level_0':'hyperparam',0:'diff_baseline'})\n",
    "            diff_ph['Year evaluated'] = to_year_i\n",
    "            diff_ph['Commodity'] = commodity.capitalize()  \n",
    "            diff_ph['diff_baseline']/= many.multi_scenario_results[divisor].loc[commodity,:,0,2019].mean()/100\n",
    "            diff_ph_out = pd.concat([diff_ph_out,diff_ph])\n",
    "    diff_ph = diff_ph_out.copy()\n",
    "    many.diff_ph3 = diff_ph.copy()\n",
    "    diff_ph['change'] = round((diff_ph['change']-1)*100,2)\n",
    "    diff_ph = diff_ph.loc[diff_ph['duration']>0]\n",
    "    many.diff_out = diff_ph.copy()\n",
    "    kws = {'x':'change', 'y':'diff_baseline', 'palette':'Dark2', 'errorbar':('ci',ci)}\n",
    "    if len(to_year)==1 and len(commodities)==1:\n",
    "        pass\n",
    "    elif len(commodities)==1:\n",
    "        kws['hue'] = 'Year evaluated'\n",
    "    elif len(to_year)==1:\n",
    "        kws['hue'] = 'Commodity'\n",
    "    \n",
    "    if not plot_rir:\n",
    "        fig,a=plt.subplots(figsize=(-1+len(to_year)*15,5))\n",
    "        if len(to_year)!=1 or len(commodities)!=1:\n",
    "            kws['errwidth'] = 4\n",
    "        kws['ax']=a\n",
    "        sns.barplot(data=diff_ph, **kws)\n",
    "        p2 = param.replace('Conc.','mine').lower().replace('demand','consumption')\n",
    "        p3 = p2.replace('scrap','total scrap')\n",
    "        divisor_str = divisor.replace('Total ','').replace(param,'value').lower()\n",
    "        title_str = commodity.capitalize()+' i' if len(commodities)==1 else 'I'\n",
    "        a.set(\n",
    "            xlabel=f'Market fraction switching to recycled content (% 2019 {divisor_str})',\n",
    "            ylabel=f'Annual {p2}\\ndifference (% 2019 {divisor_str})',\n",
    "            title=f'{title_str}mpact of transitioning to recycled content on {p3}',\n",
    "            xticklabels=[str(round(i,4)) if i<1 else str(round(i,0)).split('.')[0] for i in sorted(diff_ph.change.unique())]\n",
    "        )\n",
    "    else:\n",
    "        fig,a = plot_rir_lineplot(switch_to_rir(many), \n",
    "                                   plot_separate=if_line_plot_separate, kws=kws)\n",
    "        a.set(title='Impact of increasing recycling content on mine production',\n",
    "           ylabel='Annual mine production\\ndifference (% 2019 value)',xlabel='Recycling input rate')\n",
    "    fig.set_dpi(dpi)\n",
    "    return fig,a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea533fd3-fb74-4cb5-b5d6-291ebd33bbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = 'Scrap demand'\n",
    "one_param = indiv.results.loc[idx[:,2019:],:][param].unstack(0)\n",
    "diff_baseline = one_param.subtract(one_param[0],axis=0)\n",
    "\n",
    "diff_baseline = diff_baseline.rename(columns=dict(zip(diff_baseline.columns,\n",
    "                                     pd.MultiIndex.from_tuples([(indiv.hyperparam.loc['secondary_refined_duration',i],round(indiv.hyperparam.loc[['secondary_refined_pct_change_tot','direct_melt_pct_change_tot'],i].sum()-1,3)) for i in indiv.hyperparam.columns]))))\n",
    "diff_baseline.columns = pd.MultiIndex.from_tuples(diff_baseline.columns)\n",
    "diff_baseline.drop(0,axis=1,inplace=True)\n",
    "scrap_demand = diff_baseline.copy().cumsum()\n",
    "\n",
    "param = 'Conc. demand'\n",
    "one_param = indiv.results.loc[idx[:,2019:],:][param].unstack(0)\n",
    "diff_baseline = one_param.subtract(one_param[0],axis=0)\n",
    "\n",
    "diff_baseline = diff_baseline.rename(columns=dict(zip(diff_baseline.columns,\n",
    "                                     pd.MultiIndex.from_tuples([(indiv.hyperparam.loc['secondary_refined_duration',i],round(indiv.hyperparam.loc[['secondary_refined_pct_change_tot','direct_melt_pct_change_tot'],i].sum()-1,3)) for i in indiv.hyperparam.columns]))))\n",
    "diff_baseline.columns = pd.MultiIndex.from_tuples(diff_baseline.columns)\n",
    "diff_baseline.drop(0,axis=1,inplace=True)\n",
    "conc_demand = diff_baseline.copy().cumsum()\n",
    "\n",
    "stacked_both = pd.concat([\n",
    "    scrap_demand,conc_demand],\n",
    "    keys=['Scrap demand','Mining demand']).stack().stack().reset_index()\n",
    "stacked_conc = conc_demand.stack(0).stack().reset_index()\n",
    "stacked_scrap = scrap_demand.stack(0).stack().reset_index()\n",
    "plt.figure(dpi=50,figsize=(7,5.5))\n",
    "fig,ax=easy_subplots(2,use_subplots=True, sharey=True)\n",
    "sns.lineplot(data=stacked_both,x='level_1',y=0,hue='level_0',errorbar=('ci',95),ax=ax[0])\n",
    "sns.lineplot(data=stacked_both,x='level_1',y=0,hue='level_0',errorbar=('sd',2),ax=ax[1])\n",
    "fig.suptitle('Mining demand response to\\nincreasing scrap demand',weight='bold',y=1.05)\n",
    "ax[0].set_xlabel('Year')\n",
    "ax[1].set_xlabel('Year')\n",
    "ax[0].set_ylabel('Change in demand (kt)')\n",
    "ax[1].set_ylabel('Change in demand (kt)')\n",
    "ax[0].set(title='95% confidence interval shaded')\n",
    "ax[1].set(title='2 std dev shaded')\n",
    "ax[0].legend(title=None)\n",
    "ax[1].legend(title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b089c98-17d9-40d4-8aef-58432ef87e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv.hyperparam.loc['Total production, Global',0],indiv.hyperparam.loc['initial_demand',0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['Total demand','Conc. demand', 'Conc. supply','Ref. demand','Ref. supply','Scrap demand','Scrap supply','Spread','TCRC','Refined price','Mean total minesite cost','Mean total cash margin','Mean mine grade','Conc. SD','Ref. SD','Scrap SD']\n",
    "for var in variables:\n",
    "    one_param = indiv.results.loc[idx[:,2019:],:][var].unstack(0)\n",
    "    diff_baseline = one_param.subtract(one_param[0],axis=0)\n",
    "\n",
    "    diff_baseline = diff_baseline.rename(columns=dict(zip(diff_baseline.columns,\n",
    "                                     pd.MultiIndex.from_tuples([(indiv.hyperparam.loc['secondary_refined_duration',i],round(indiv.hyperparam.loc[['secondary_refined_pct_change_tot','direct_melt_pct_change_tot'],i].sum()-1,3)) for i in indiv.hyperparam.columns]))))\n",
    "    diff_baseline.columns = pd.MultiIndex.from_tuples(diff_baseline.columns)\n",
    "    fig,ax = easy_subplots(3)\n",
    "    for i,a in zip(diff_baseline.columns.levels[0][1:],ax):\n",
    "        if 'supply' in var or 'demand' in var:\n",
    "            diff_baseline.loc[:2040,i].cumsum().plot(legend=True,ax=a)\n",
    "        else:\n",
    "            diff_baseline.loc[:2040,i].plot(legend=True,ax=a)\n",
    "    if 'supply' in var or 'demand' in var:\n",
    "        ax[1].set_title(var+' cumulative')\n",
    "    else:\n",
    "        ax[1].set_title(var)\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2096c0ff-d495-4381-b1a7-16b80c63b220",
   "metadata": {},
   "source": [
    "## Trying for hyperparam distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d60d9-0d69-4652-865c-c3f12191a962",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = Individual('Al',3,'data/aluminum_run_scenario_set0.pkl')\n",
    "pd.concat([indiv.results[['Additional direct melt','Additional secondary refined']].dropna().loc[idx[:,:],:].sum(axis=1).groupby(level=0).sum()/\\\n",
    "                indiv.results['Total demand'].loc[idx[:,2019]],\n",
    "           indiv.hyperparam.loc['direct_melt_pct_change_tot']-1],axis=1,keys=['Calculated for each scenario, fraction of total demand','From hyperparameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d1979-b40c-4b8c-881c-662559a6cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv = Individual('Al',3,'data/aluminum_run_scenario_set0.pkl')\n",
    "\n",
    "hyperparam = indiv.hyperparam.drop(0,axis=1)\n",
    "results = indiv.results.drop(0,level=0)\n",
    "[i for i in hyperparam.index if 'direct_melt' in i]\n",
    "hyperparam.loc[['direct_melt_pct_change_tot','direct_melt_duration']]\n",
    "hyperparam.loc['pct_change_tot',:] = hyperparam.loc[['direct_melt_pct_change_tot','secondary_refined_pct_change_tot']].sum()-1\n",
    "scenarios_per_hyperparam = len(hyperparam.loc['pct_change_tot'].unique())\n",
    "n=-1\n",
    "for i in hyperparam.columns:\n",
    "    if hyperparam.loc['direct_melt_pct_change_tot',i]==1:\n",
    "        n+=1\n",
    "    hyperparam.loc['hyperparameter_set',i]=n\n",
    "    hyperparam.loc['scenario_number',i]=i-n*scenarios_per_hyperparam-1\n",
    "    \n",
    "results.loc[:,'Hyperparameter set'] = results.apply(lambda x: hyperparam[x.name[0]]['hyperparameter_set'],axis=1)\n",
    "results.loc[:,'Scenario number'] = results.apply(lambda x: x.name[0]-hyperparam[x.name[0]]['hyperparameter_set']*scenarios_per_hyperparam-1,axis=1)\n",
    "results = results.reset_index().set_index(['Hyperparameter set','Scenario number','level_1']).drop(columns='level_0')\n",
    "hyperparam = hyperparam.T.set_index(['hyperparameter_set','scenario_number']).T\n",
    "results = results.rename(dict(zip(results.index.get_level_values(1).unique(),[round(hyperparam.loc['pct_change_tot',idx[:,i]].unique()[0],4) for i in results.index.get_level_values(1).unique()])),level=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0319e3f-0115-47a4-93b6-c1893689e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "param='Conc. supply'\n",
    "ph = results[param].unstack(1).loc[idx[:,2019:],:]\n",
    "diff_ph = ph.subtract(ph[1],axis=0).drop(1,axis=1)\n",
    "diff_ph = diff_ph.groupby(level=0).median()\n",
    "diff_ph = diff_ph.apply(lambda x: x.loc[abs(x)<abs(x.mean())+3*x.std()],axis=0)\n",
    "\n",
    "diff_ph1 = diff_ph.copy()\n",
    "diff_ph = diff_ph.stack().reset_index().rename(columns={'Hyperparameter set':'hyperparam',0:'diff_baseline','Scenario number':'change'})\n",
    "diff_ph['diff_baseline']/=indiv.results['Total demand'].unstack(0).loc[2019]/100\n",
    "diff_ph['change'] = round((diff_ph['change']-1)*100,1)\n",
    "plt.figure(figsize=(8,5),dpi=50)\n",
    "sns.barplot(data=diff_ph, x='change', y='diff_baseline',\n",
    "            palette='Dark2')\n",
    "plt.xlabel('Market fraction switching to\\nrecycled content (% total demand)')\n",
    "p2 = param.replace('Conc.','mine').lower().replace('demand','consumption')\n",
    "plt.ylabel(f'Mean {p2}\\nchange (% total {param.split()[1]})')\n",
    "p2 = p2.replace('scrap','the rest of market\\'s scrap')\n",
    "plt.title(f'Impact of transitioning to recycled content\\non {p2}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f3909c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-09T20:05:29.701773Z",
     "start_time": "2022-06-09T20:05:29.654871Z"
    }
   },
   "source": [
    "# Big sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab206cb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-24T02:37:08.408969Z",
     "start_time": "2022-06-16T17:28:21.207580Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scenarios = ['ss_nono_5yr_0%tot_0%inc']+\\\n",
    "    ['ss_no_'+str(yr)+'yr_'+str(pct)+'%tot_0%inc' for yr in np.arange(5,21,5) for pct in np.arange(5,21,5)]\n",
    "#                          'ss_pr_3yr_2%tot_0%inc','ss_no_3yr_2%tot_0%inc',\n",
    "#                          'sd_pr_3yr_2%tot_0%inc','sd_no_3yr_2%tot_0%inc',\n",
    "#                          'ss_pr_10yr_2%tot_']\n",
    "filename = 'data/big_sensitivity_ss_no.pkl'\n",
    "n_scen = 50\n",
    "OVERWRITE = True\n",
    "\n",
    "for i in np.arange(7,n_scen):\n",
    "    print('Running outside scenario {}/{}, {:.1f}% complete'.format(i+1,n_scen,i/n_scen*100))\n",
    "    ci = generate_commodity_inputs(commodity_inputs, i)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('error')\n",
    "        OVERWRITE = OVERWRITE if i==0 else False\n",
    "        s = Sensitivity(filename,ci,OVERWRITE=OVERWRITE,notes='Big sensitivity, scrap supply shock, no collection rate price response',scenarios=scenarios,verbosity=0)\n",
    "        s.run_monte_carlo(n_scenarios=2,random_state=220615+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2c739b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-15T17:38:43.989713Z",
     "start_time": "2022-06-15T17:38:43.918902Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_df = pd.read_pickle(filename)\n",
    "pd.concat([big_df.loc['hyperparam',i]['Value'] for i in big_df.columns],keys=big_df.columns,axis=1).head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d4fe4f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68787a4e-66c3-4525-807e-75323df73e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ed9185",
   "metadata": {},
   "source": [
    "# Paper figures - main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357c1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_sg=Many()\n",
    "many_sg.load_data('2023-01-17 15_28_13_0_split_grades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66aad7",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_given_columns_for_paper(many, commodity, columns, column_name=None, \n",
    "                                 ax=None, column_subset=None, start_year=None, end_year=2019, \n",
    "                                 show_all_lines=False, r2_on_own_line=True, plot_actual_price=False, \n",
    "                                 dpi=50):\n",
    "    \"\"\"\n",
    "    Plots historical vs simulated demand, mining,\n",
    "    and primary commodity price for a given commodity\n",
    "    and whichever hyperparameter sets given as the\n",
    "    `columns` variable. Can get the n_best_scenarios\n",
    "    using the get_best_columns function, or more manually\n",
    "    by running get_train_test_scores on an rmse_df (or\n",
    "    commodity subset), then running its output through\n",
    "    get_commodity_scores, e.g.\n",
    "\n",
    "    rr = get_train_test_scores(many_test.integ.rmse_df.loc['silver'])\n",
    "    s1 = get_commodity_scores(rr,None)\n",
    "    then passing s1.loc[s1.selection2].index as the columns input.\n",
    "\n",
    "    -------------\n",
    "    many: Many() object, needs to have results object of its own,\n",
    "        so if you ran get_multiple to load many, you should pass\n",
    "        many.integ to this function\n",
    "    commodity: str, commodity name in lowercase form\n",
    "    columns: list, int, or None. If list, columns from rmse_df to \n",
    "        plot. If int, will selected the int lowest-score (RMSE)\n",
    "        columns. If None, will select the 25 lowest-score columns.\n",
    "        Will highlight the lowest-score one if no column_subset is \n",
    "        passed.\n",
    "    column_name: str, gets included in the plot title if not None\n",
    "    ax: matplotlib axes object, can leave out and this will\n",
    "        create its own plot for you.\n",
    "    column_subset: list, allows you to select a subset of the\n",
    "        passed columns to highlight, or to just plot two groups\n",
    "        of parameter sets simultaneously, since column_subset\n",
    "        and columns do not have to intersect. Pass a list or\n",
    "        array of numbers corresponding to rmse_df columns.\n",
    "    plot_actual_price: bool, whether to include the unadjusted \n",
    "        historical price in the plot\n",
    "    dpi: dots per inch, controls resolution. Only functions if\n",
    "        the ax input is None.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig,ax=easy_subplots(3, dpi=dpi)\n",
    "    else:\n",
    "        fig = 0\n",
    "    if columns is None:\n",
    "        columns = many.rmse_df.loc[commodity].sort_values(by='score',axis=1).iloc[:,:25].columns\n",
    "    elif type(columns)==int:\n",
    "        columns = many.rmse_df.loc[commodity].sort_values(by='score',axis=1).iloc[:,:columns].columns\n",
    "    objective_results_map = {'Total demand':'Total demand','Primary commodity price':'Refined price',\n",
    "                                 'Primary demand':'Conc. demand','Primary supply':'Mine production',\n",
    "                                'Conc. SD':'Conc. SD','Scrap SD':'Scrap SD','Ref. SD':'Ref. SD'}\n",
    "    for i,a in zip(['Total demand','Primary commodity price','Primary supply'], ax):\n",
    "        results = many.results.copy()[objective_results_map[i]].sort_index()\\\n",
    "            .loc[idx[commodity,:,2001:end_year]].droplevel(0).unstack(0)\n",
    "        if 'SD' not in i:\n",
    "            historical_data = many.historical_data.copy()[i].loc[commodity].loc[:2019]\n",
    "            if i=='Primary commodity price':\n",
    "                original_price = many.historical_data.copy()[\n",
    "                    'Original primary commodity price'].loc[commodity].loc[:2019]\n",
    "            if start_year is not None:\n",
    "                historical_data = historical_data.loc[start_year:]\n",
    "                if i=='Primary commodity price':\n",
    "                    original_price = original_price.loc[start_year:]\n",
    "        else:\n",
    "            historical_data = pd.Series(results.min(),[0])\n",
    "        results_ph = results.copy()\n",
    "        results = results[columns]\n",
    "\n",
    "        diction = get_unit(results, historical_data, i)\n",
    "        results, historical_data, unit = [diction[i] for i in ['simulated','historical','unit']]\n",
    "        results_ph *= results[columns[0]].mean()/results_ph[columns[0]].mean()\n",
    "        if show_all_lines:\n",
    "            sim_line = a.plot(results,linewidth=1,color='gray',alpha=0.3,label=results.columns)\n",
    "            if column_subset is None:\n",
    "                best_line= a.plot(results[columns[0]],linewidth=6,label='Simulated',color='tab:blue')\n",
    "            else:\n",
    "                best_line= a.plot(results_ph[column_subset],linewidth=1,label='Simulated',color='tab:blue')\n",
    "        else:\n",
    "            sns_results = results.stack().reset_index().rename(columns={\n",
    "                'level_0':'Year','level_1':'Scenario',0:'Value'\n",
    "            })\n",
    "            sim_line = sns.lineplot(data=sns_results, x='Year', y='Value', ax=a)\n",
    "            sim_line = sim_line.get_lines()\n",
    "        mins = min(historical_data.min(),results[columns[0]].min())*0.95\n",
    "        maxs = max(historical_data.max(),results[columns[0]].max())*1.1\n",
    "        extra_label=''\n",
    "        if i=='Primary commodity price' and plot_actual_price:\n",
    "            if original_price.isna().all():\n",
    "                comm_not_element = many.element_commodity_map[commodity]\n",
    "                original_price = pd.read_csv(\n",
    "                    'generalization/input_files/user_defined/price adjustment results.csv',\n",
    "                    index_col=0)[f'log({comm_not_element})'].sort_index().loc[original_price.index]\n",
    "            orig_hist_line = a.plot(original_price, label='Historical', color='lightgray', linewidth=6,\n",
    "                                    linestyle=':')\n",
    "            extra_label=', rolling mean'\n",
    "        hist_line = a.plot(historical_data,label='Historical'+extra_label,color='k',linewidth=6)\n",
    "        inter = np.intersect1d(results.index,historical_data.index)\n",
    "        if show_all_lines:\n",
    "            m = sm.GLS(historical_data.loc[inter], \n",
    "                       sm.add_constant(results[columns[0]].loc[inter])\n",
    "                      ).fit(cov_type='HC3')\n",
    "        else:\n",
    "            m = sm.GLS(historical_data.loc[inter], \n",
    "                       sm.add_constant(sim_line[0].get_data()[1])\n",
    "                      ).fit(cov_type='HC3')\n",
    "        mse = round(m.mse_resid**0.5,2)\n",
    "        mse = round(m.rsquared,2)\n",
    "        if column_name is not None:\n",
    "            title=f'{i}, {column_name} {commodity},\\n'+r'$R^2$'+f'={mse}, scenario {columns[0]}'\n",
    "        else:\n",
    "            title=f'{i}, {commodity}'\n",
    "        if i=='Primary commodity price':\n",
    "            maxs *= 1.1\n",
    "            if show_all_lines:\n",
    "                maxs *= 1.1\n",
    "            if plot_actual_price:\n",
    "                maxs *= 1.15\n",
    "                if commodity=='Sn':\n",
    "                    maxs *= 1\n",
    "                    mins *= 0.85\n",
    "                elif commodity=='Cu':\n",
    "                    mins *= 0.9\n",
    "                    maxs*=1.15\n",
    "#                 if commodity=='Ag':\n",
    "#                     maxs *= 1.1\n",
    "            title = title.replace('Primary commodity','Refined metal')\n",
    "        elif i=='Primary supply' and show_all_lines:\n",
    "            maxs *= 1.1\n",
    "#         elif i=='Total demand' and commodity=='Ag':\n",
    "#             maxs *= 0.6\n",
    "#         elif i=='Primary supply' and commodity=='Ag':\n",
    "#             mins *= 0.8\n",
    "#             maxs *= 0.8\n",
    "        elif i=='Primary supply' and commodity=='Cu':\n",
    "            mins *=0.9\n",
    "        title = title.replace('Primary supply','Mine production')\n",
    "        a.set(title=title,\n",
    "              ylabel=i+' ('+unit+')',xlabel='Year',ylim=(mins,maxs))\n",
    "#         a.text(0.05,0.9, r'$R^2$'+f'={mse}', transform=a.transAxes)\n",
    "        \n",
    "        if len(sim_line)<10 and show_all_lines:\n",
    "            a.legend()\n",
    "        else:\n",
    "            handles = []\n",
    "            labels = []\n",
    "            simulated_string = 'Simulated, best'\n",
    "            r2_string = r'$R^2$'+f'={mse}'\n",
    "            r2_handle = Line2D([0],[0], color='w', linewidth=0)\n",
    "            if plot_actual_price and i=='Primary commodity price':\n",
    "                handles += [orig_hist_line[0]]\n",
    "                labels += ['Historical']\n",
    "            handles += [hist_line[0]]\n",
    "            labels += ['Historical'+extra_label]\n",
    "            if len(sim_line)!=1:\n",
    "                handles += [best_line[0], sim_line[0]]\n",
    "                if r2_on_own_line:\n",
    "                    labels += [simulated_string, 'Simulated, other', r2_string]\n",
    "                    handles += [r2_handle]\n",
    "                else:\n",
    "                    labels += [' '.join([simulated_string, r2_string]), 'Simulated, other']\n",
    "                    a.set(title=title+', '+r2_string)\n",
    "            else:\n",
    "                handles += [sim_line[0]]\n",
    "                if r2_on_own_line:\n",
    "                    labels += [simulated_string, r2_string]\n",
    "                    handles += [r2_handle]\n",
    "                else:\n",
    "                    labels += [' '.join([simulated_string, r2_string])]\n",
    "                    a.set(title=title+', '+r2_string)\n",
    "            a.legend(handles, labels, loc='upper left')\n",
    "    return fig,ax\n",
    "\n",
    "def plot_best_fits(many, show_all_lines=False, plot_actual_price=False, commodities=None, dpi=300, \n",
    "                   start_year=None, end_year=2019):\n",
    "    if commodities is None:\n",
    "    #     commodities=['Cu','Ni','Pb','Zn','Au']\n",
    "    #     commodities=['Ag','Sn','Al','Steel']\n",
    "    #     commodities=['Ag']\n",
    "        commodities = many.results.index.get_level_values(0).unique()\n",
    "    #     commodities=[many.element_commodity_map[i].lower() for i in commodities]\n",
    "    fig, ax = easy_subplots(len(commodities)*3,height_scale=1.1)\n",
    "    for e,comm in enumerate(commodities):\n",
    "        a = ax[3*e:3*(e+1)]\n",
    "        best = many.rmse_df.loc[comm].sort_values(by='score',axis=1).iloc[:,:25].columns\n",
    "        fig1,a=plot_given_columns_for_paper(many,comm,columns=best, show_all_lines=show_all_lines, \n",
    "                                            plot_actual_price=plot_actual_price, \n",
    "                                            start_year=start_year, end_year=end_year,\n",
    "                                            r2_on_own_line=True, ax=a)\n",
    "#         a[1].set_ylim(10500,29000)\n",
    "    fig.tight_layout()\n",
    "    fig.set_dpi(dpi)\n",
    "    return fig\n",
    "#     return sim_line\n",
    "\n",
    "fig = plot_best_fits(many_sg, show_all_lines=False, plot_actual_price=True, commodities=['Cu','Ni','Pb','Zn','Au']);\n",
    "# fig.savefig('figures/tuning_results1.pdf')\n",
    "# fig = plot_best_fits(many_sg, show_all_lines=False, plot_actual_price=True, commodities=['Ag','Sn','Al','Steel']);\n",
    "# fig.savefig('figures/tuning_results2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b19fc",
   "metadata": {
    "code_folding": [
     93
    ]
   },
   "outputs": [],
   "source": [
    "# def pval_to_qval(pv, m=None, verbose=False, lowmem=False, pi0=None):\n",
    "#     \"\"\"\n",
    "#     Estimates q-values from p-values\n",
    "#     Args\n",
    "#     =====\n",
    "#     m: number of tests. If not specified m = pv.size\n",
    "#     verbose: print verbose messages? (default False)\n",
    "#     lowmem: use memory-efficient in-place algorithm\n",
    "#     pi0: if None, it's estimated as suggested in Storey and Tibshirani, 2003.\n",
    "#          For most GWAS this is not necessary, since pi0 is extremely likely to be\n",
    "#          1\n",
    "#     From: https://github.com/nfusi/qvalue\n",
    "#     \"\"\"\n",
    "#     if type(pv)==list:\n",
    "#         pv = np.array(pv)\n",
    "#     assert(pv.min() >= 0 and pv.max() <= 1), \"p-values should be between 0 and 1\"\n",
    "\n",
    "#     original_shape = pv.shape\n",
    "#     pv = pv.ravel()  # flattens the array in place, more efficient than flatten()\n",
    "\n",
    "#     if m is None:\n",
    "#         m = float(len(pv))\n",
    "#     else:\n",
    "#         # the user has supplied an m\n",
    "#         m *= 1.0\n",
    "\n",
    "#     # if the number of hypotheses is small, just set pi0 to 1\n",
    "#     if len(pv) < 100 and pi0 is None:\n",
    "#         pi0 = 1.0\n",
    "#     elif pi0 is not None:\n",
    "#         pi0 = pi0\n",
    "#     else:\n",
    "#         # evaluate pi0 for different lambdas\n",
    "#         pi0 = []\n",
    "#         lam = np.arange(0, 0.90, 0.01)\n",
    "#         counts = np.array([(pv > i).sum() for i in np.arange(0, 0.9, 0.01)])\n",
    "#         for l in range(len(lam)):\n",
    "#             pi0.append(counts[l]/(m*(1-lam[l])))\n",
    "\n",
    "#         pi0 = np.array(pi0)\n",
    "\n",
    "#         # fit natural cubic spline\n",
    "#         tck = interpolate.splrep(lam, pi0, k=3)\n",
    "#         pi0 = interpolate.splev(lam[-1], tck)\n",
    "#         if verbose:\n",
    "#             print(\"qvalues pi0=%.3f, estimated proportion of null features \" % pi0)\n",
    "\n",
    "#         if pi0 > 1:\n",
    "#             if verbose:\n",
    "#                 print(\"got pi0 > 1 (%.3f) while estimating qvalues, setting it to 1\" % pi0)\n",
    "#             pi0 = 1.0\n",
    "\n",
    "#     assert(pi0 >= 0 and pi0 <= 1), \"pi0 is not between 0 and 1: %f\" % pi0\n",
    "\n",
    "#     if lowmem:\n",
    "#         # low memory version, only uses 1 pv and 1 qv matrices\n",
    "#         qv = np.zeros((len(pv),))\n",
    "#         last_pv = pv.argmax()\n",
    "#         qv[last_pv] = (pi0*pv[last_pv]*m)/float(m)\n",
    "#         pv[last_pv] = -np.inf\n",
    "#         prev_qv = last_pv\n",
    "#         for i in range(int(len(pv))-2, -1, -1):\n",
    "#             cur_max = pv.argmax()\n",
    "#             qv_i = (pi0*m*pv[cur_max]/float(i+1))\n",
    "#             pv[cur_max] = -np.inf\n",
    "#             qv_i1 = prev_qv\n",
    "#             qv[cur_max] = min(qv_i, qv_i1)\n",
    "#             prev_qv = qv[cur_max]\n",
    "\n",
    "#     else:\n",
    "#         p_ordered = np.argsort(pv)\n",
    "#         pv = pv[p_ordered]\n",
    "#         qv = pi0 * m/len(pv) * pv\n",
    "#         qv[-1] = min(qv[-1], 1.0)\n",
    "\n",
    "#         for i in range(len(pv)-2, -1, -1):\n",
    "#             qv[i] = min(pi0*m*pv[i]/(i+1.0), qv[i+1])\n",
    "\n",
    "#         # reorder qvalues\n",
    "#         qv_temp = qv.copy()\n",
    "#         qv = np.zeros_like(qv)\n",
    "#         qv[p_ordered] = qv_temp\n",
    "\n",
    "#     # reshape qvalues\n",
    "#     qv = qv.reshape(original_shape)\n",
    "\n",
    "#     return qv\n",
    "\n",
    "# import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import minimize_scalar\n",
    "from scipy.interpolate import splrep, splev, UnivariateSpline\n",
    "\n",
    "def pi0est(p, lambda_seq=np.arange(0.05, 0.96, 0.05), pi0_method='smoother',\n",
    "           smooth_df=3, smooth_log_pi0=False, plot=False):\n",
    "    \"\"\"\n",
    "    Adapted from https://github.com/StoreyLab/qvalue, see folder R, filename\n",
    "    pi0est.R. Part of the port from R was done using translation by chatGPT. \n",
    "    \n",
    "    Estimates the proportion of true null p-values, \n",
    "    i.e., those following the Uniform(0,1) distribution.\n",
    "    \n",
    "    Storey JD, Bass AJ, Dabney A, Robinson D (2022). qvalue: Q-value estimation \n",
    "    for false discovery rate control. R package version 2.30.0, \n",
    "    http://github.com/jdstorey/qvalue.\n",
    "    \n",
    "    Originally described in:\n",
    "    https://www.ncbi.nlm.nih.gov/pmc/articles/PMC170937/\n",
    "    Storey JD, Tibshirani R. Statistical significance for genomewide studies. \n",
    "    Proc Natl Acad Sci USA. 2003 Aug 5;100(16):9440-5. \n",
    "    doi: 10.1073/pnas.1530509100. Epub 2003 Jul 25. PMID: 12883005; PMCID: PMC170937.\n",
    "    \n",
    "    Full documentation at: \n",
    "    https://www.bioconductor.org/packages/release/bioc/html/qvalue.html\n",
    "    \n",
    "    Additional nice explanation here:\n",
    "    http://varianceexplained.org/statistics/interpreting-pvalue-histogram/\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check input arguments\n",
    "    p = np.array(p)\n",
    "    p = p[~np.isnan(p)]\n",
    "    m = len(p)\n",
    "    lambda_seq = np.sort(lambda_seq)  # guard against user input\n",
    "\n",
    "    ll = len(lambda_seq)\n",
    "\n",
    "    # Determines pi0\n",
    "    if ll == 1:\n",
    "        pi0 = np.mean(p >= lambda_seq) / (1 - lambda_seq)\n",
    "        pi0_lambda = pi0\n",
    "        pi0 = min(pi0, 1)\n",
    "        pi0Smooth = None\n",
    "    else:\n",
    "#         print(np.searchsorted(lambda_seq,p))\n",
    "#         print(np.bincount(np.searchsorted(lambda_seq,p))[::-1][:-1])\n",
    "#         print((m * (1 - lambda_seq[::-1])))\n",
    "#         pi0 = np.cumsum(np.digitize(x=np.searchsorted(lambda_seq,p), \n",
    "#                                     bins=lambda_seq)[::-1]) / (m * (1 - lambda_seq[::-1]))\n",
    "        pi0 = np.cumsum(np.bincount(np.searchsorted(lambda_seq,p),minlength=len(lambda_seq)+1)[::-1][:-1]) / (m * (1 - lambda_seq[::-1]))\n",
    "        pi0 = pi0[::-1]\n",
    "        pi0_lambda = pi0\n",
    "        # Smoother method approximation\n",
    "        if pi0_method == 'smoother':\n",
    "            if smooth_log_pi0:\n",
    "                pi0 = np.log(pi0)\n",
    "                spl = splrep(lambda_seq, pi0, k=smooth_df)\n",
    "                pi0Smooth = np.exp(splev(lambda_seq, spl))\n",
    "                pi0 = min(pi0Smooth[-1], 1)\n",
    "            else:\n",
    "                spl = UnivariateSpline(lambda_seq, pi0, k=smooth_df)\n",
    "#                 spl.set_smoothing_factor(10)\n",
    "                pi0Smooth = spl(lambda_seq)\n",
    "#                 print(pi0Smooth)\n",
    "                if plot:\n",
    "                    plt.figure()\n",
    "                    plt.scatter(lambda_seq, pi0)\n",
    "                    plt.plot(lambda_seq, pi0Smooth)\n",
    "                    plt.show()\n",
    "                r_version = [0.86135727, 0.80143108, 0.74204924, 0.68383418, 0.62729461, 0.57278862, 0.52043101, 0.47048902, 0.42318349, 0.37826941, 0.33546801, 0.29498387, 0.25722865, 0.22212778, 0.18938777, 0.15892771, 0.13070403, 0.10428954, 0.07883342,]\n",
    "#                 plt.plot(lambda_seq, r_version)\n",
    "#                 spi0 = splrep(lambda_seq, pi0, k=smooth_df, s=len(lambda_seq)-10)\n",
    "#                 pi0Smooth = splev(lambda_seq, spi0)\n",
    "                pi0 = min(max(pi0Smooth[-1],0), 1)\n",
    "        elif pi0_method == 'bootstrap':\n",
    "            # Bootstrap method closed form solution by David Robinson\n",
    "            minpi0 = np.percentile(pi0, q=10)\n",
    "            W = np.array([np.sum(p >= l) for l in lambda_seq])\n",
    "            mse = (W / (m ** 2 * (1 - lambda_seq) ** 2)) * (1 - W / m) + (pi0 - minpi0) ** 2\n",
    "            pi0 = min(pi0[np.argmin(mse)], 1)\n",
    "            pi0Smooth = None\n",
    "        if pi0 < 0:\n",
    "            raise ValueError(\"The estimated pi0 < 0. Setting the pi0 estimate to be 1. Check that you have valid p-values or use a different range of lambda.\")\n",
    "            pi0 = 1\n",
    "            pi0_lambda = 1\n",
    "            pi0Smooth = 0\n",
    "            lambda_seq = 0\n",
    "        return {'pi0': pi0, 'pi0.lambda': pi0_lambda, 'lambda': lambda_seq, 'pi0.smooth': pi0Smooth}\n",
    "\n",
    "# def FixedB(p, B):\n",
    "#     \"\"\"    \n",
    "#     Input:\n",
    "#      p: a vector of p-values\n",
    "#      B: an integer, the interval [0,1] is divided into B equal-length intervals\n",
    " \n",
    "#     Output:\n",
    "#      pi0: an estimate of the proportion of true null hypotheses\n",
    "     \n",
    "#     From: \n",
    "#     - https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2623313/#b11-cin-6-0025\n",
    "#     - https://www.stat.purdue.edu/~doerge/software/AverageEstimate_Rcode\n",
    "#     \"\"\"\n",
    "#     m = len(p)\n",
    "#     t = np.linspace(0, 1, num=B+1)\n",
    "    \n",
    "#     NB = [0] * B\n",
    "#     NBaverage = [0] * B\n",
    "#     NS = [0] * B\n",
    "#     pi = [0] * B\n",
    "    \n",
    "#     for i in range(B):\n",
    "#         NB[i] = len([x for x in p if x >= t[i]])\n",
    "#         NBaverage[i] = NB[i] / (B - i)\n",
    "#         NS[i] = len([x for x in p if x >= t[i] and x < t[i+1]])\n",
    "#         pi[i] = NB[i] / (1 - t[i]) / m\n",
    "#     i = min([i for i in range(B) if NS[i] <= NBaverage[i]])\n",
    "#     i = i-1 if i>0 else i\n",
    "#     pi0 = min(1, np.mean(pi[i:B]))\n",
    "#     return pi0\n",
    "\n",
    "# import numpy as np\n",
    "# from random import choices\n",
    "\n",
    "# def AverageEstimate(p=None, Bvector=[5, 10, 20, 50, 100], alpha=0.05):\n",
    "#     if p is None:\n",
    "#         p = []\n",
    "    \n",
    "#     # check if the p-values are valid\n",
    "#     if min(p) < 0 or max(p) > 1:\n",
    "#         print(\"Error: p-values are not in the interval of [0,1]\")\n",
    "#     m = len(p) # Total number p-values\n",
    "    \n",
    "#     Bvector = [int(b) for b in Bvector]  # Make sure Bvector is a vector of integers\n",
    "    \n",
    "#     # Bvector has to be bigger than 1\n",
    "#     if min(Bvector) <= 1:\n",
    "#         print(\"Error: B has to be bigger than 1\")\n",
    "        \n",
    "#     # Estimate pi0\n",
    "#     if len(Bvector) == 1:  # fixed number of numbers, i.e., B is fixed\n",
    "#         pi0 = AverageEstimateFixedB(p, Bvector)\n",
    "#     else:\n",
    "#         Numboot = 100\n",
    "#         OrigPi0Est = np.zeros(len(Bvector))\n",
    "#         for Bloop in range(len(Bvector)):\n",
    "#             OrigPi0Est[Bloop] = FixedB(p, Bvector[Bloop])\n",
    "        \n",
    "#         BootResult = np.zeros((Numboot, len(Bvector)))  # Contains the bootstrap results\n",
    "        \n",
    "#         for k in range(Numboot):\n",
    "#             p_boot = choices(p, k=m)  # bootstrap sample\n",
    "#             for Bloop in range(len(Bvector)):\n",
    "#                 BootResult[k, Bloop] = FixedB(p_boot, Bvector[Bloop])\n",
    "        \n",
    "#         MeanPi0Est = np.mean(OrigPi0Est)  # average of pi0 estimates over the range of Bvector\n",
    "#         MSEestimate = np.zeros(len(Bvector))  # compute mean-squared error\n",
    "#         for i in range(len(Bvector)):\n",
    "#             MSEestimate[i] = (OrigPi0Est[i]- MeanPi0Est)**2\n",
    "#             for k in range(Numboot):\n",
    "#                 MSEestimate[i] += 1/Numboot * (BootResult[k, i] - OrigPi0Est[i])**2\n",
    "        \n",
    "#         pi0 = OrigPi0Est[np.argmin(MSEestimate)]\n",
    "    \n",
    "#     # Apply the adaptive FDR controlling procedure\n",
    "#     sorted_p = np.sort(p)  # sorted p-values\n",
    "#     order_p = np.argsort(p)  # order of the p-values\n",
    "#     m0 = pi0 * m  # estimate of the number of true null\n",
    "#     i = m\n",
    "#     crit = i / m0 * alpha\n",
    "#     while sorted_p[i-1] > crit:\n",
    "#         i -= 1\n",
    "#         crit = i / m0 * alpha\n",
    "#         if i == 1:\n",
    "#             break\n",
    "#     K = i\n",
    "#     if K == 1 and sorted_p[K-1] <= 1/m0 * alpha:\n",
    "#         K = 1\n",
    "#     if K == 1 and sorted_p[K-1] > 1/m0 * alpha:\n",
    "#         K = 0\n",
    "    \n",
    "#     significant = np.zeros(m)  # indicator of significance of the p-values\n",
    "#     if K>0:\n",
    "#         significant[order_p[0:K-1]] = 1\n",
    "#     result = {'pi0':pi0, 'significant':significant}\n",
    "#     return result\n",
    "\n",
    "# pi0est(np.array(pvals))['pi0'], AverageEstimate(pvals)['pi0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c05ab",
   "metadata": {
    "code_folding": [
     3,
     12,
     117,
     139,
     178,
     223
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "from scipy import interpolate\n",
    "\n",
    "def pval_to_star(pval, no_star_cut=0.1, period_cut=0.05, one_star_cut=0.01, two_star_cut=0.001):\n",
    "    \"\"\"\n",
    "    Converts a value from its numerical value to a string where:\n",
    "    *** < 0.001 < ** < 0.01 < * < 0.05 < (.) < 0.1\n",
    "    \"\"\"\n",
    "    pval_str = '***' if pval < two_star_cut else '**' if pval < one_star_cut else '*' if \\\n",
    "        pval < period_cut else '(.)' if pval < no_star_cut else ''\n",
    "    return pval_str\n",
    "\n",
    "def make_parameter_mean_std_table(many, n_best, value_in_parentheses='standard error', stars='ttest',\n",
    "                                  rand_size=1000):\n",
    "    \"\"\"\n",
    "    many: Many instance, from tuning, either the integ object or full thing is has integ\n",
    "        object\n",
    "    n_best: int, number of best scenarios to use in the calculation of mean and std error/\n",
    "        std dev / variance\n",
    "    value_in_parentheses: str, can be `standard error`, `standard deviation`, or `variance`\n",
    "    \"\"\"\n",
    "    if hasattr(many, 'integ'):\n",
    "        rmse_df = many.integ.rmse_df_sorted.copy()\n",
    "    else:\n",
    "        rmse_df = many.rmse_df_sorted.copy()\n",
    "    r = [i for i in rmse_df.index.get_level_values(1).unique()\n",
    "         if np.any([j in i for j in ['score', 'R2', 'RMSE', 'region_specific_price_response']])]\n",
    "    rmse_df.drop(r, inplace=True, level=1)\n",
    "    best_n = rmse_df.loc[:, :n_best]\n",
    "    means = best_n.mean(axis=1).unstack(0).fillna('')\n",
    "    if value_in_parentheses == 'standard error':\n",
    "        stds = best_n.sem(axis=1).unstack(0).fillna('')\n",
    "    elif value_in_parentheses == 'standard deviation':\n",
    "        stds = best_n.std(axis=1).unstack(0).fillna('')\n",
    "    elif value_in_parentheses == 'variance':\n",
    "        stds = best_n.var(axis=1).unstack(0).fillna('')\n",
    "    elif value_in_parentheses in ['None',None]:\n",
    "        stds = best_n.var(axis=1).unstack(0)\n",
    "        stds.loc[:] = ''\n",
    "    \n",
    "    if stars=='ttest':\n",
    "        pvals = best_n.apply(lambda x: stats.ttest_1samp(x,popmean=0)[1], axis=1)\n",
    "        display(pvals)\n",
    "    elif stars=='uniform':\n",
    "        pvals = get_difference_from_uniform(best_n, rand_size=rand_size)\n",
    "    elif stars=='both':\n",
    "        pvals = best_n.apply(lambda x: stats.ttest_1samp(x,popmean=0)[1], axis=1)\n",
    "        pvals_u = get_difference_from_uniform(best_n, rand_size=rand_size)\n",
    "        stars_u = pvals_u.apply(pval_to_star)\n",
    "    else:\n",
    "        raise ValueError('stars input must be either ttest or uniform')\n",
    "    stars_df = pvals.apply(pval_to_star)\n",
    "\n",
    "    demand = ['sector_specific_dematerialization_tech_growth', 'sector_specific_price_response',\n",
    "              'intensity_response_to_gdp', 'direct_melt_elas_scrap_spread']\n",
    "    mine_production = ['primary_oge_scale', 'mine_cu_margin_elas', 'mine_cost_og_elas',\n",
    "                       'mine_cost_change_per_year', 'mine_cost_price_elas']\n",
    "    incentive = ['initial_ore_grade_decline',\n",
    "                 'incentive_opening_probability', 'close_years_back', 'primary_price_resources_contained_elas',\n",
    "                 'reserves_ratio_price_lag', 'mine_cost_tech_improvements', 'incentive_mine_cost_change_per_year']\n",
    "    price = ['primary_commodity_price_elas_sd', 'scrap_spread_elas_primary_commodity_price', \n",
    "             'scrap_spread_elas_sd', 'tcrc_elas_price','tcrc_elas_sd']\n",
    "    refinery = ['pri CU TCRC elas', 'pri CU price elas', 'refinery_capacity_fraction_increase_mining', \n",
    "                'sec CU TCRC elas', 'sec CU price elas', 'sec ratio TCRC elas', 'sec ratio scrap spread elas',]\n",
    "    scrap_avail = ['collection_elas_scrap_price']\n",
    "    \n",
    "    mean_std = pd.DataFrame(np.nan, means.index, means.columns)\n",
    "    for i in mean_std.index:\n",
    "        for c in mean_std.columns:\n",
    "            if means[c][i] != '':\n",
    "                if stars != 'both' and value_in_parentheses in ['None',None]:\n",
    "                    mean_std.loc[i, c] = '{:.3f}{:s}'.format(means[c][i], stars_df[c][i])\n",
    "                elif stars != 'both':\n",
    "                    mean_std.loc[i, c] = '{:.3f}{:s} ({:.3f})'.format(means[c][i], stars_df[c][i], stds[c][i])\n",
    "                elif value_in_parentheses in ['None',None]:\n",
    "                    mean_std.loc[i, c] = r'{:.3f}{:s}/{:s}'.format(means[c][i], stars_df[c][i], stars_u[c][i])\n",
    "                else:\n",
    "                    mean_std.loc[i, c] = r'{:.3f}{:s}/{:s} ({:.3f})'.format(means[c][i], stars_df[c][i], stars_u[c][i], stds[c][i])\n",
    "            else:\n",
    "                mean_std.loc[i, c] = ' '\n",
    "    mean_std = mean_std.T\n",
    "    params_nice = make_parameter_names_nice(mean_std.columns)\n",
    "\n",
    "    def convert_param_names(v):\n",
    "        if v in mine_production:\n",
    "            return ('Mining production', params_nice[v])\n",
    "        elif v in demand:\n",
    "            return ('Demand response', params_nice[v])\n",
    "        elif v in incentive:\n",
    "            return ('Reserve development', params_nice[v])\n",
    "        elif v in price:\n",
    "            return ('Price formation', params_nice[v])\n",
    "        elif v in refinery:\n",
    "            return ('Refinery operation', params_nice[v])\n",
    "        elif v in scrap_avail:\n",
    "            return ('Secondary supply', params_nice[v])\n",
    "        else:\n",
    "            print(v)\n",
    "            return ('Integration parameters', params_nice[v])\n",
    "\n",
    "    mean_std = mean_std.rename(columns=dict(zip(mean_std.columns,\n",
    "                                                [convert_param_names(i) for i in mean_std.columns])))\n",
    "    pvals = pvals.unstack()\n",
    "    pvals = pvals.rename(columns=dict(zip(pvals.columns,\n",
    "                                                [convert_param_names(i) for i in pvals.columns])))\n",
    "    pvals.columns = pd.MultiIndex.from_tuples(pvals.columns)\n",
    "#     mean_std = mean_std.rename(dict(zip(mean_std.index, [i\n",
    "#                                                          for i in mean_std.index])))\n",
    "    mean_std.columns = pd.MultiIndex.from_tuples(mean_std.columns)\n",
    "    means = means.T.replace('',np.nan)\n",
    "    means = means.rename(columns=dict(zip(means.columns,\n",
    "                                                [convert_param_names(i) for i in means.columns])))\n",
    "#     means = means.rename(dict(zip(means.index, [many.commodity_element_map[i.capitalize()]\n",
    "#                                                          for i in means.index])))\n",
    "    means.columns = pd.MultiIndex.from_tuples(means.columns)\n",
    "    return mean_std.T.sort_index().T, means.sort_index().T.sort_index().T, pvals.sort_index().T.sort_index().T\n",
    "\n",
    "def get_correct_loc_scale(parameter):\n",
    "    loc = 0\n",
    "    scale = 1\n",
    "    if parameter=='incentive_opening_probability':\n",
    "        scale = 0.5\n",
    "    elif parameter in ['mine_cost_change_per_year','incentive_mine_cost_change_per_year']:\n",
    "        loc = -5\n",
    "        scale = 10\n",
    "    elif parameter=='sector_specific_dematerialization_tech_growth':\n",
    "        loc = -0.1\n",
    "        scale = 0.2\n",
    "    elif parameter=='intensity_response_to_gdp':\n",
    "        loc = -0.5\n",
    "        scale = 1.5\n",
    "    elif parameter in ['sector_specific_price_response','region_specific_price_response']:\n",
    "        loc = -0.6\n",
    "        scale = 0.6\n",
    "    elif parameter in ['sec ratio TCRC elas','mine_cost_og_elas',\n",
    "                       'primary_commodity_price_elas_sd','initial_ore_grade_decline']:\n",
    "        loc = -1\n",
    "    return loc, scale\n",
    "\n",
    "def get_difference_from_uniform(rmse_df, rand_size=25):\n",
    "    for_test = rmse_df.copy()\n",
    "    \n",
    "    uniform_rvs = pd.DataFrame()\n",
    "    loc = 0\n",
    "    scale = 1\n",
    "    for rs in np.arange(0,100):\n",
    "        uniform_rvs[rs] = stats.uniform.rvs(loc=loc,scale=scale,size=25, random_state=rs)\n",
    "\n",
    "    def ks100_uniform(array, size=25):\n",
    "        \"\"\"\n",
    "        Size is the size of the random variable generation from the uniform distribution\n",
    "        \"\"\"\n",
    "        ks_pvals = []\n",
    "        name = array.name[1]\n",
    "        loc, scale = get_correct_loc_scale(name)\n",
    "        if size is not None:\n",
    "            for rs in np.arange(0,100):\n",
    "                if True:\n",
    "                    randoms = stats.uniform.rvs(loc=loc, scale=scale, size=size, random_state=rs)\n",
    "                    val = stats.kstest(array, randoms)[1]\n",
    "                else:\n",
    "                    val = stats.kstest(array, uniform_rvs[rs])[1]\n",
    "                ks_pvals += [val]\n",
    "#             return np.median(ks_pvals)\n",
    "            return pi0est(np.array(ks_pvals))['pi0']\n",
    "#             if array.name[0]=='Au':\n",
    "#                 if array.name[1]=='sector_specific_price_response':\n",
    "#                     print(np.median(ks_pvals), np.max(pval_to_qval(ks_pvals)))\n",
    "#             return np.median(pval_to_qval(ks_pvals))\n",
    "        else: \n",
    "            randoms = stats.uniform.rvs(loc=loc, scale=scale, size=10000, random_state=0)\n",
    "            val = stats.kstest(array, randoms)[1]\n",
    "#             val = stats.kstest(array, 'uniform')[1]\n",
    "            return val\n",
    "    \n",
    "    applied = for_test.apply(lambda x: ks100_uniform(x, size=rand_size), axis=1)\n",
    "    return applied\n",
    "\n",
    "def plot_colorful_table(many, stars='ttest', value_in_parentheses='standard error', \n",
    "                        no_color_for_insignificant=True,\n",
    "                        rand_size=1000, dpi=50):\n",
    "    \"\"\"\n",
    "    Creates the table showing parameter values, stars for statistical significance, and\n",
    "    standard error/other metric in parentheses. \n",
    "    \n",
    "    many: Many object\n",
    "    stars: str, how to calculate pvalues for the stars. Can be `ttest`, `uniform`, or `both`. \n",
    "        The ttest version uses a simple one-sample t-test for difference from zero. The \n",
    "        uniform version checks for whether the distribution is different from a uniform\n",
    "        distribution with the corresponding bounds, performing 100 Kolmogorov-Smirnov tests\n",
    "        and taking the mean. Uniform takes forever.\n",
    "    value_in_parentheses: str, can be `standard error`, `standard deviation`, or `variance`\n",
    "    dpi: int, controls resolution\n",
    "    \"\"\"\n",
    "    table,means,pvals = make_parameter_mean_std_table(many,25, stars=stars, \n",
    "                                                value_in_parentheses=value_in_parentheses, rand_size=rand_size)\n",
    "    alt_table = table.replace(dict(zip(table.values.flatten(),\n",
    "                                       [i.replace(' (','\\n(') for i in table.values.flatten()])))\\\n",
    "        .sort_index().T.sort_index().droplevel(0)\n",
    "    alt_means = means.div(abs(means).max()).T.droplevel(0)\n",
    "    alt_pvals = pvals.T.droplevel(0)\n",
    "    if no_color_for_insignificant:\n",
    "        alt_means[alt_pvals>0.1] = 0\n",
    "    fig,ax = plt.subplots()\n",
    "    sns.heatmap(alt_means,\n",
    "                ax=ax,\n",
    "                annot=alt_table,\n",
    "                fmt='s',\n",
    "                annot_kws={'fontsize':16},\n",
    "                xticklabels=True,\n",
    "                yticklabels=True,\n",
    "                cmap='vlag',\n",
    "                cbar=False,\n",
    "               )\n",
    "    # ax.set_yticks(np.arange(ax.get_yticks()[0],ax.get_yticks()[-1]+1,1))\n",
    "    ax.tick_params(labelbottom=True, labeltop=True, labelsize=24)\n",
    "    if stars != 'both':\n",
    "        fig.set_size_inches(12,16)\n",
    "    else:\n",
    "        fig.set_size_inches(16,16)\n",
    "    fig.set_dpi(dpi)\n",
    "    return table, means, pvals, alt_means, fig\n",
    "\n",
    "def plot_colorful_table2(many, stars='ttest', value_in_parentheses='standard error', \n",
    "                        no_color_for_insignificant=True,\n",
    "                        rand_size=1000, dpi=50):\n",
    "    \"\"\"\n",
    "    Creates the table showing parameter values, stars for statistical significance, and\n",
    "    standard error/other metric in parentheses. \n",
    "    \n",
    "    many: Many object\n",
    "    stars: str, how to calculate pvalues for the stars. Can be `ttest`, `uniform`, or `both`. \n",
    "        The ttest version uses a simple one-sample t-test for difference from zero. The \n",
    "        uniform version checks for whether the distribution is different from a uniform\n",
    "        distribution with the corresponding bounds, performing 100 Kolmogorov-Smirnov tests\n",
    "        and taking the mean. Uniform takes forever.\n",
    "    value_in_parentheses: str, can be `standard error`, `standard deviation`, or `variance`\n",
    "    dpi: int, controls resolution\n",
    "    \"\"\"\n",
    "    table,means,pvals = make_parameter_mean_std_table(many,25, stars=stars, \n",
    "                                                value_in_parentheses=value_in_parentheses, rand_size=rand_size)\n",
    "    alt_table = table.replace(dict(zip(table.values.flatten(),\n",
    "                                       [i.replace(' (','\\n(') for i in table.values.flatten()])))\\\n",
    "        .sort_index().T.sort_index()\n",
    "    alt_means = means.div(abs(means).max()).T\n",
    "    alt_pvals = pvals.T\n",
    "    if no_color_for_insignificant:\n",
    "        alt_means[alt_pvals>0.1] = 0\n",
    "    \n",
    "    plots = alt_means.index.get_level_values(0).unique()\n",
    "    height_ratios = [alt_means.loc[i].shape[0] for i in plots]\n",
    "    fig,axes = easy_subplots(plots,1, height_ratios=height_ratios, use_subplots=True, sharex=True)\n",
    "    for i,ax in zip(plots, axes):\n",
    "        sub_means = alt_means.loc[i]\n",
    "        sub_table = alt_table.loc[i]\n",
    "        sns.heatmap(sub_means,\n",
    "                    ax=ax,\n",
    "                    annot=sub_table,\n",
    "                    fmt='s',\n",
    "                    annot_kws={'fontsize':18},\n",
    "                    xticklabels=True,\n",
    "                    yticklabels=True,\n",
    "                    cmap='vlag',\n",
    "                    cbar=False,\n",
    "                    vmin=-1,\n",
    "                    vmax=1,\n",
    "                   )\n",
    "        # ax.set_yticks(np.arange(ax.get_yticks()[0],ax.get_yticks()[-1]+1,1))\n",
    "        ax.tick_params(labelbottom= i==plots[-1], labeltop= i==plots[0], labelsize=20, color='white')\n",
    "        ax.tick_params(axis='y',rotation=0)\n",
    "        ax.set_ylabel(i,rotation='horizontal',horizontalalignment='left', labelpad=170, fontsize=22)\n",
    "    if stars != 'both':\n",
    "        fig.set_size_inches(19,14)\n",
    "    else:\n",
    "        fig.set_size_inches(19,16)\n",
    "    fig.set_dpi(dpi)\n",
    "    fig.align_ylabels(axes)\n",
    "    fig.tight_layout()\n",
    "    return table, means, pvals, alt_means, fig\n",
    "\n",
    "rand_size=25\n",
    "table, means, pvals, alt_means, fig = plot_colorful_table2(many_sg, stars='uniform', dpi=250, \n",
    "                                        value_in_parentheses='None',#'standard error'\n",
    "                                        rand_size=rand_size);\n",
    "# if rand_size is None:\n",
    "#     fig.axes[0].set_title('single test, size=10,000')\n",
    "# else:\n",
    "#     fig.axes[0].set(title=f'pi0, size={rand_size}')\n",
    "# fig.savefig('figures/compare_elasticities_table.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87beae8",
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_to_observe = [\n",
    "    ('Ag','sector_specific_price_response'),('Ag','sec CU TCRC elas'),\n",
    "    ('Cu','collection_elas_scrap_price'),('Ni','collection_elas_scrap_price'),\n",
    "    ('Steel','collection_elas_scrap_price'),('Pb','collection_elas_scrap_price'),\n",
    "                    ]\n",
    "\n",
    "for which in np.arange(0,len(params_to_observe)):\n",
    "    commodity = params_to_observe[which][0]\n",
    "    name = params_to_observe[which][1]\n",
    "    actual = many_sg.rmse_df_sorted.loc[:,:25].loc[idx[commodity,name],:]\n",
    "    loc, scale = get_correct_loc_scale(name)\n",
    "    n_samples = pd.DataFrame(np.nan, index=[25], \n",
    "                             columns=['median pval','max qval','pi0'], \n",
    "                             dtype=float)\n",
    "    for n in n_samples.index:\n",
    "        pvals = []\n",
    "        for rs in np.arange(0,100):\n",
    "        #     random = stats.uniform.rvs(loc=0, scale=1, size=100, random_state=rs)\n",
    "            random = stats.uniform.rvs(loc=loc, scale=scale, size=n, random_state=rs)\n",
    "            pval = stats.kstest(actual,random)[1]\n",
    "            pvals += [pval]\n",
    "        n_samples.loc[n] = [np.median(pvals), np.max(pval_to_qval(pvals)), \n",
    "                            pi0est(np.array(pvals))['pi0']]\n",
    "\n",
    "    # plt.hist(random, alpha=0.4, label='random')\n",
    "    plt.figure()\n",
    "    plt.hist(pvals)\n",
    "    pvals = np.array(pvals)\n",
    "    plt.title(commodity+' '+name)\n",
    "    print(pvals[pvals<0.05].shape[0]/pvals.shape[0])\n",
    "    fig,ax = easy_subplots(2,2)\n",
    "    off = 0.05\n",
    "    ax[0].hist(actual, alpha=0.4, label='actual')\n",
    "    ax[0].set(title=f'{commodity}, {name}', ylabel='Count', xlabel='Value', xlim=(loc-off, loc+scale+off))\n",
    "    n_samples.plot(ax=ax[1], xlabel='number of samples', ylabel='pvalue', title=f'{commodity}, {name}',\n",
    "                   logx=True)\n",
    "    ax[1].hlines(0.1, n_samples.index.min(), n_samples.index.max(), color='k', linestyle='--')\n",
    "    # np.quantile(pvals,q=0.25), np.median(pvals), pi0est(np.array(pvals))['pi0'], \\\n",
    "    #     np.max(pval_to_qval(pvals)), np.mean(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589e71c",
   "metadata": {
    "code_folding": [
     8
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params_to_observe = [\n",
    "    ('Ag','sector_specific_price_response'),('Ag','sec CU TCRC elas'),\n",
    "    ('Cu','collection_elas_scrap_price'),('Ni','collection_elas_scrap_price'),\n",
    "    ('Steel','collection_elas_scrap_price'),('Pb','collection_elas_scrap_price'),\n",
    "    ('Ag','tcrc_elas_sd')\n",
    "                    ]\n",
    "# for commodity,name in params_to_observe:\n",
    "commodity, name = params_to_observe[-2]\n",
    "if True:\n",
    "    variances = pd.Series(np.nan,[25,100,1000,10000,1e5,1e6])\n",
    "    pvals_df = pd.DataFrame(np.nan, np.arange(0,100), variances.index)\n",
    "    actual = many_sg.rmse_df_sorted.loc[:,:25].loc[idx[commodity,name],:]\n",
    "    loc, scale = get_correct_loc_scale(name)\n",
    "    for size in variances.index:\n",
    "        pvals = []\n",
    "        for rs in np.arange(0,100):\n",
    "            pval = stats.kstest(actual, stats.uniform.rvs(loc=loc, scale=scale, size=int(size), random_state=rs))[1]\n",
    "            pvals += [pval]\n",
    "        pvals_df.loc[:,size] = pvals\n",
    "        variances.loc[size] = np.std(pvals)\n",
    "\n",
    "    # Plotting\n",
    "    fig,ax = easy_subplots(2,2)\n",
    "    ax=ax[::-1]\n",
    "    variances.plot(logx=True, ax=ax[0],\n",
    "                  title='Standard deviation of p-values for\\nKolmogorov-Smirnov test, 100 random states',\n",
    "                  xlabel='Number of samples in uniform distribution',\n",
    "                  ylabel='Standard deviation of p-value').grid(axis='x')\n",
    "\n",
    "    pvals_sns = pvals_df.stack().reset_index().rename(\n",
    "        columns={'level_0':'Random state','level_1':'Sample size',0:'p-value'})\n",
    "    pvals_sns.loc[:,'log(Sample size)'] = np.log10(pvals_sns['Sample size'])\n",
    "    sns.lineplot(pvals_sns, x='log(Sample size)', y='p-value', ax=ax[1], errorbar=('ci',100))\n",
    "    xlim = ax[1].get_xlim()\n",
    "    ax[1].set_xticks([2,3,4,5,6])\n",
    "    ax[1].set_xlim(xlim)\n",
    "    ax[1].set_xticklabels([r'$10^2$',r'$10^3$', r'$10^4$',r'$10^5$', r'$10^6$'])\n",
    "    ax[1].set(title='P-value distribution for Kolmogorov-Smirnov\\ntest, 100 random states',\n",
    "              xlabel='Number of samples in uniform distribution',\n",
    "              ylabel='P-value')\n",
    "    print(commodity, name)\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_reformat(x):\n",
    "    pv = pvals[means==x].dropna(how='all').dropna(axis=1).values.flatten()[0]\n",
    "    return '{:.3f}{:s}'.format(x, pval_to_star(pv))\n",
    "    \n",
    "def make_pretty(styler):\n",
    "    styler.background_gradient(axis=1, cmap=\"vlag\")\n",
    "    styler.format(formatter=string_reformat, na_rep='')\n",
    "    headers = [{'selector': 'th.ind_heading', 'props': 'text-align: top;'},\n",
    "        {'selector': 'th.ind_heading.level0', 'props': 'font-size: 1.5em;'},\n",
    "        {'selector': 'td', 'props': 'text-align: center; font-weight: bold;'}]\n",
    "    styler.set_table_styles(headers)\n",
    "    for l0 in means.T.index.get_level_values(0).unique()[1:]:\n",
    "        df = means.T.loc[l0]\n",
    "        \n",
    "        styler.set_table_styles({(l0, df.index[0]): [{'selector': '', 'props': 'border-top: 3px solid black;'}],\n",
    "                            },\n",
    "                          overwrite=False, axis=1)\n",
    "    return styler\n",
    "\n",
    "means.T.style.pipe(make_pretty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461e6115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb6e21a",
   "metadata": {
    "code_folding": [
     6
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_offset = 0.1\n",
    "ub = 0.35\n",
    "min_dist = 0.3\n",
    "value_scale = 1\n",
    "x_scale = 1\n",
    "\n",
    "def categorical_scatter_jitter(df):\n",
    "    \"\"\"\n",
    "    Takes in dataframe where Value is in columns, and the index has two levels:\n",
    "    parameter name and commodity (like seaborn format but retaining the index). \n",
    "    Adds a column called x that gives the offset from the Value axis such that\n",
    "    each point will overlap less when plotted in a categorical scatter plot.\n",
    "    \n",
    "    Can then plot the returned dataframe values using:\n",
    "        plt.scatter(df.Value, df.x) # if horizontally-oriented plot\n",
    "    \"\"\"\n",
    "    ph = df.copy()\n",
    "    vals = ph.index.get_level_values(0).unique()\n",
    "    x_dict = dict([i[::-1] for i in list(enumerate(vals))])\n",
    "    ph.loc[:,'x'] = [x_dict[i] for i in ph.index.get_level_values(0)]\n",
    "    ph.loc[:,'x'] *= -1 # h.x.max()\n",
    "    md = pd.DataFrame()\n",
    "\n",
    "    def get_distance(x, pph):\n",
    "        val_dist = (x['Value']-pph['Value'].drop(x.name))\n",
    "        x_dist = (x['x']-pph['x'].drop(x.name))*0.5\n",
    "        tot_dist = (val_dist**2 + x_dist**2)**0.5\n",
    "        return tot_dist.min()\n",
    "\n",
    "    for v in ph.index.get_level_values(0).unique():\n",
    "        offset = set_offset\n",
    "        pph = ph.copy().loc[v]\n",
    "        init_val = pph['x'].unique()[0]\n",
    "\n",
    "        adj = pph.apply(lambda x: get_distance(x,pph), axis=1)\n",
    "        pph.loc[adj.duplicated(keep='last'),'x']+=offset\n",
    "        pph.loc[adj.duplicated(keep='first'),'x']-=offset\n",
    "        for it in range(0,50):\n",
    "            adj = pph.apply(lambda x: get_distance(x,pph), axis=1)\n",
    "            if it==0:\n",
    "                pph.loc[adj.duplicated(keep='last'),'x']+=offset\n",
    "                pph.loc[adj.duplicated(keep='first'),'x']-=offset\n",
    "            else:\n",
    "                if it % 10 == 0:\n",
    "                    offset /= 1.1\n",
    "                for k in adj[adj.duplicated()].values:\n",
    "                    if k<0.25:\n",
    "                        ind = adj[adj==k].index\n",
    "                        pph.loc[pph['x'][ind].idxmax(),'x'] += (1-k)/20\n",
    "                        pph.loc[pph['x'][ind].idxmin(),'x'] -= (1-k)/20\n",
    "                pph.loc[pph['x']>init_val+ub,'x'] = init_val+ub\n",
    "                pph.loc[pph['x']<init_val-ub,'x'] = init_val-ub\n",
    "\n",
    "        pph = pd.concat([pph],keys=[v])\n",
    "        md = pd.concat([md, pph])\n",
    "    return md, x_dict\n",
    "   \n",
    "def plot_var_category_values_text(df, ax=0, outer_df=0):\n",
    "    md, x_dict = categorical_scatter_jitter(df)\n",
    "    if type(ax)==int:\n",
    "        fig, ax = plt.subplots()\n",
    "    a = ax\n",
    "    x, y, z = md.Value, md.x, md.Commodity.replace('Steel','St')\n",
    "\n",
    "    a.scatter(x, y, alpha=0)\n",
    "    \n",
    "    # Adding shading for seeing the categories more clearly\n",
    "    if len(x_dict)>1:\n",
    "        val = [-i+0.5 for i in x_dict.values()]\n",
    "        val += [min(val)-1]\n",
    "        for v1,v2 in zip(val[::2], val[1::2]):\n",
    "            a.fill_between([-2,2],[v1,v1], [v2,v2], zorder=1, alpha=0.07, color='gray')\n",
    "    offset_lim = 0.25 \n",
    "    offset_lim_x = offset_lim\n",
    "    if type(outer_df)==0:\n",
    "        a.set_xlim(min(x)-offset_lim, max(x)+ offset_lim)\n",
    "        a.set_ylim(min(y)-offset_lim, max(y)+ offset_lim)\n",
    "    else:\n",
    "        outer_df=outer_df['Value']\n",
    "        if min(outer_df)>=0:\n",
    "            offset_lim_x = 0.15\n",
    "        a.set_xlim(min(outer_df)-offset_lim_x, max(outer_df)+ offset_lim_x)\n",
    "        a.set_ylim(min(y)-offset_lim, max(y)+ offset_lim)\n",
    "        \n",
    "\n",
    "    colors = dict(zip(z.unique(), sns.color_palette('dark', 9, 0.6)))\n",
    "    # below from https://stackoverflow.com/questions/27577668/looking-for-marker-text-option-for-pyplot-plot, \n",
    "    # user: SteveWithamDuplicate\n",
    "    for i,j,s in zip(x,y,z):\n",
    "        a.annotate(str(s),  xy=(i, j), color=colors[s],\n",
    "                    fontsize=\"large\", weight='heavy',\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center')\n",
    "\n",
    "    yticks = x_dict.values()\n",
    "    a.set_yticks([q*-1 for q in yticks])\n",
    "    a.set_yticklabels(list(x_dict.keys()));\n",
    "    \n",
    "def plot_category_values_text(means, pvals, scale_option=0, dpi=50):\n",
    "    \"\"\"\n",
    "    scale_option: must be str in [`divide_by_abs_max`,`btw_zero_one`]\n",
    "        or int index of same\n",
    "    \"\"\"\n",
    "    scale_options = ['divide_by_abs_max','btw_zero_one']\n",
    "    mm = means.copy()\n",
    "    mm[pvals>0.1] = np.nan\n",
    "    mm = mm.dropna(how='all',axis=1)\n",
    "    mm = mm.T\n",
    "#     display(mm)\n",
    "    mm = mm.loc[mm.notna().sum(axis=1)>1]\n",
    "    # mm = mm.subtract(mm.min(axis=1), axis=0)\n",
    "    if type(scale_option)==int:\n",
    "        scale_option = scale_options[scale_option]\n",
    "    if scale_option=='divide_by_abs_max':\n",
    "        mm = mm.div(abs(mm).max(axis=1),axis=0)\n",
    "    elif scale_option=='btw_zero_one':\n",
    "        mm = mm.apply(lambda x: (x-x.min())/(x.max()-x.min()), axis=1)\n",
    "    else:\n",
    "        raise ValueError(f'scale_option is currently {scale_option}, must be in'\n",
    "                         '[`divide_by_abs_max`,`btw_zero_one`]')\n",
    "    plots = mm.index.get_level_values(0).unique()\n",
    "    height_ratios = [mm.loc[i].shape[0] for i in plots]\n",
    "    mm = pd.DataFrame(mm.stack())\n",
    "    mm.rename(columns={mm.columns[0]:'Value'}, inplace=True)\n",
    "    mm.loc[:,'Parameter'] = mm.index.get_level_values(1)\n",
    "    mm.loc[:,'Commodity'] = mm.index.get_level_values(2)\n",
    "    fig,ax = easy_subplots(plots, 1, height_ratios=height_ratios, sharex=True)\n",
    "    for i,a in zip(plots, ax):\n",
    "        plot_var_category_values_text(mm.loc[i], a, mm);            \n",
    "        a.tick_params(top=True, labelbottom= i==plots[-1], labeltop= i==plots[0], labelsize=20)\n",
    "        a.tick_params(axis='y',rotation=0)\n",
    "        a.set_xlabel(None if i!=plots[-1] else \n",
    "                     'Value normalized by parameter absolute maximum' if scale_option=='divide_by_abs_max' else\n",
    "                     'Value normalized to be in (0,1)' if scale_option=='btw_zero_one' else 'Value')\n",
    "        a.set_ylabel(i,rotation='horizontal',horizontalalignment='left', labelpad=170, fontsize=22)\n",
    "        ylim = a.get_ylim()\n",
    "        a.vlines(0,-10,10, color='gray',alpha=0.3,linestyle='--', linewidth=1,zorder=0)\n",
    "        a.set_ylim(ylim)\n",
    "    fig.align_ylabels()\n",
    "    fig.set_size_inches(18,20)\n",
    "    fig.tight_layout(pad=0.5)\n",
    "    fig.set_dpi(dpi)\n",
    "    fig.savefig('figures/compare_elasticities_element_labels.pdf')\n",
    "    return fig\n",
    "\n",
    "fig = plot_category_values_text(means, pvals, scale_option=0, dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0279aa",
   "metadata": {
    "code_folding": [
     57,
     84,
     122,
     153,
     174,
     207,
     216,
     241,
     296
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleanup_whole(source):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        for i in np.arange(0,source.shape[0]):\n",
    "            for j in np.arange(0,source.shape[1]):\n",
    "                col = source.columns[j]\n",
    "                ix = source[col].iloc[i]\n",
    "                if type(ix)==str:\n",
    "                    ind = source.index[i]\n",
    "                    if ' ' in ix:\n",
    "                        source.loc[(ind[0],ind[1]+', a',ind[2]),col]=float(ix.split(' ')[0])\n",
    "                        try:\n",
    "                            source.loc[(ind[0],ind[1]+', b',ind[2]),col]=float(ix.split(' ')[-1])\n",
    "                        except:\n",
    "                            print(ix)\n",
    "                            raise ValueError\n",
    "                    elif '-' in ix:\n",
    "                        source.loc[(ind[0],ind[1]+', a',ind[2]),col]=float(ix.split('-')[0])\n",
    "                        source.loc[(ind[0],ind[1]+', b',ind[2]),col]=float(ix.split('-')[-1])\n",
    "                    source.iloc[i,j]=np.nan\n",
    "        source = source.dropna(how='all')\n",
    "        return source               \n",
    "\n",
    "def load_sources(many):\n",
    "    sources = pd.read_excel('Sources for elasticities.xlsx',index_col=[0,1,2,3])\n",
    "    if 'Li' in sources.columns:\n",
    "        sources.drop(columns=['Li'],inplace=True)\n",
    "    sources = sources.loc[:,'Steel':'Zn'].dropna(how='all')\n",
    "    sources.rename(dict(zip(sources.index.get_level_values(0).unique(),\n",
    "                            [i.replace('supply-demand imbalance','SD') for i in sources.index.get_level_values(0).unique()])),level=0,inplace=True)\n",
    "    \n",
    "    sources_to_rmse = {\n",
    "        'Scrap supply elasticity to secondary price':'Collection elasticity to scrap price',\n",
    "        'Scrap demand elasticity to scrap spread':'Direct melt fraction elasticity to scrap spread',\n",
    "        'Price elasticity to supply-demand imbalance':'Refined price elasticity to SD',\n",
    "        'Ore treated elasticity to total cash margin':'Mine CU elasticity to TCM',\n",
    "        'Primary supply elasticity to price':'Mine CU elasticity to TCM',\n",
    "        'Primary demand elasticity to price':'Demand elasticity to price',\n",
    "        'Ore grade decline per year':'Ore grade elasticity to COT distribution mean',\n",
    "                      }\n",
    "    sources.rename(sources_to_rmse,level=0,inplace=True)\n",
    "#     sources = sources.loc[idx[:,:,['N','From Dahl'],:],:]\n",
    "    sources = sources.droplevel(2)\n",
    "    sources = cleanup_whole(sources)\n",
    "    new_ind = []\n",
    "    for e,i in enumerate(sources.index):\n",
    "        if 'Demand' in i[0]:\n",
    "            new_ind += [(i[0].replace('Demand','Intensity'), i[1]+' - demand', i[2])]\n",
    "            if 'GDP' in i[0]:\n",
    "                sources.iloc[e] = sources.iloc[e] - 1\n",
    "        elif i[0]=='Scrap price elasticity to primary price':\n",
    "            new_ind += [('Scrap spread elasticity to price', i[1]+' - scrap price', i[2])]\n",
    "            sources.iloc[e] = 1-sources.iloc[e]\n",
    "        else:\n",
    "            new_ind += [i]\n",
    "    sources.index = pd.MultiIndex.from_tuples(new_ind)\n",
    "    many.sources_methods = sources.copy()\n",
    "    many.sources = sources.copy().droplevel(2)\n",
    "\n",
    "def get_distribution_differences(sources, this_study):\n",
    "    \"\"\"\n",
    "    Returns an n by 2 dataframe with a 2-level index \n",
    "    (parameter name, commodity) and columns being the\n",
    "    pvalues from the Kruskal-Wallis H-test and \n",
    "    Kolmogorov-Smirnov tests.\n",
    "    \"\"\"\n",
    "    dist_test_pvals = pd.DataFrame()\n",
    "    for i in sources.index.get_level_values(0).unique():\n",
    "        dist_test_ph = pd.DataFrame()\n",
    "        for c in sources.columns:\n",
    "            if c in sources.columns:\n",
    "                sources_vals = sources.loc[i,c].dropna()\n",
    "            else: \n",
    "                sources_vals = pd.DataFrame()\n",
    "            if c in this_study.columns:\n",
    "                model_vals = this_study.sort_index().loc[i,c].dropna()\n",
    "            else: \n",
    "                model_vals = pd.DataFrame()\n",
    "\n",
    "            if model_vals.shape[0]>0 and sources_vals.shape[0]>0:\n",
    "                dist_test_ph.loc[c,'KW test pval'] = stats.kruskal(model_vals, sources_vals)[1]\n",
    "                dist_test_ph.loc[c,'KS test pval'] = stats.kstest(model_vals, sources_vals)[1]\n",
    "        dist_test_ph = pd.concat([dist_test_ph],keys=[i])\n",
    "        dist_test_pvals = pd.concat([dist_test_pvals,dist_test_ph])\n",
    "    return dist_test_pvals\n",
    "    \n",
    "def get_sources_this_study(many):\n",
    "    load_sources(many)\n",
    "    sources = many.sources.copy()\n",
    "    many.rmse_df_nice = many.rmse_df_sorted.loc[:,:25].rename(\n",
    "        make_parameter_names_nice(many.rmse_df_sorted.index.get_level_values(1)), level=1\n",
    "    )\n",
    "    this_study = many.rmse_df_nice.stack().unstack(0)\n",
    "    ind = np.intersect1d(sources.index.get_level_values(0).unique(), \n",
    "                         this_study.index.get_level_values(0).unique())\n",
    "    sources = sources.loc[ind]\n",
    "    this_study = this_study.loc[ind]\n",
    "    this_study['Source'] = this_study.index.get_level_values(1)\n",
    "    sources['Source'] = sources.index.get_level_values(1)\n",
    "    this_study = this_study.reset_index().set_index(['level_0','level_1','Source'])\n",
    "    sources = sources.reset_index().set_index(['level_0','level_1','Source'])\n",
    "    this_study = this_study.rename(\n",
    "        columns=dict(zip(this_study.columns,[i.capitalize() for i in this_study.columns])))\n",
    "    this_study = this_study.rename(\n",
    "        dict(zip(np.arange(0,26),[f'This study {i}' for i in np.arange(0,26)]),level=1))\n",
    "    sources = sources.rename(dict(zip(sources.index.get_level_values(1),\n",
    "                            ['Literature' if '- demand' not in i else 'Literature' for i in\n",
    "                             sources.index.get_level_values(1)\n",
    "                            ]\n",
    "                           )),level=1)\n",
    "    sources_both = pd.concat([\n",
    "        this_study,\n",
    "        sources]).dropna(how='all',axis=1)\n",
    "    sources_plt = sources_both.stack().reset_index().rename(columns={'level_0':'Parameter',\n",
    "                                                            'level_1':'General source',\n",
    "                                                            'level_3':'Commodity',\n",
    "                                                            0: 'Value'\n",
    "                                                           })\n",
    "    sources_plt.loc[['This study' in i for i in sources_plt['General source']],'General source'] = 'This study'\n",
    "    sources_plt['Commodity, source'] = [', '.join([i,j.lower()]) for i,j in zip(sources_plt['Commodity'],\n",
    "                                                                    sources_plt['General source'])]\n",
    "    sources_plt = sources_plt.astype({'Value':float})\n",
    "    return sources, this_study, sources_plt\n",
    "\n",
    "def update_ore_grade_annual_to_COT(many, sources_plt):\n",
    "    # setting per year changes in ore grade to be in terms of cumu. ore treated\n",
    "    # done by multiplying per year value by the average percent change in cumu. OT per year\n",
    "    intermediate = sources_plt.loc[\n",
    "        (sources_plt['Parameter']=='Ore grade elasticity to COT distribution mean') &\n",
    "        (sources_plt['General source']!='This study') &\n",
    "        (sources_plt['Source']!='This study, c')\n",
    "    ]\n",
    "    initial_ind = intermediate.index\n",
    "    intermediate = intermediate.set_index(['Source','Commodity'])\n",
    "\n",
    "    mean_pct_change = many.sources.loc['Mean percent change per year in cumulative ore treated']\n",
    "    intermediate.loc[:,'New value'] = intermediate.apply(lambda x: mean_pct_change[x.name[1]]*x['Value']/100,axis=1)\n",
    "    intermediate['Value'] = intermediate['New value']\n",
    "    intermediate = intermediate.drop(columns='New value')\n",
    "    intermediate = intermediate.reset_index()\n",
    "    intermediate.index = initial_ind\n",
    "\n",
    "    sources_plt.loc[\n",
    "        (sources_plt['Parameter']=='Ore grade elasticity to COT distribution mean') &\n",
    "        (sources_plt['General source']!='This study') &\n",
    "        (sources_plt['Source']!='This study, c')\n",
    "    ] = intermediate\n",
    "\n",
    "    sources_plt.loc[\n",
    "        (sources_plt['Parameter']=='Ore grade elasticity to COT distribution mean') &\n",
    "        (sources_plt['General source']!='This study') &\n",
    "        (sources_plt['Source']!='This study, c')\n",
    "    ]\n",
    "    return sources_plt\n",
    "    \n",
    "def update_sources_sub_parameters(sources_sub):\n",
    "    sources_sub.loc[sources_sub.Parameter=='Mine cost change per year','Value']/=10\n",
    "    sources_sub.loc[sources_sub.Parameter==\n",
    "                    'Mine cost change per year','Parameter'\n",
    "                   ] = r'$\\frac{1}{10} x$ Mine cost change per year'\n",
    "    sources_sub.loc[sources_sub.Parameter=='Intensity elasticity to time','Value']*=10\n",
    "    sources_sub.loc[sources_sub.Parameter==\n",
    "                    'Intensity elasticity to time','Parameter'\n",
    "                   ] = '10 x Intensity elasticity to time'\n",
    "#         sources_sub.loc[sources_sub.Parameter=='Ore grade elasticity to COT distribution mean','Value']/=-5\n",
    "#         sources_sub.loc[sources_sub.Parameter==\n",
    "#                         'Ore grade elasticity to COT distribution mean','Parameter'\n",
    "#                        ] = r'$\\frac{1}{5} x$ Ore grade elasticity to COT'\n",
    "\n",
    "    param_list = []\n",
    "    for i in sources_sub['Parameter'].unique():\n",
    "        if len(sources_sub.loc[sources_sub['Parameter']==i]['General source'].unique())>1:\n",
    "            param_list += [i]\n",
    "    sources_sub = sources_sub.loc[sources_sub['Parameter'].apply(lambda x: x in param_list)]\n",
    "    return sources_sub, param_list\n",
    "\n",
    "def update_sources_sub_stars(comm, sources_sub, dist_test_pvals, param_list, include_stars,\n",
    "                             stars_overlapping, stars_overlapping90, stars_not_overlapping):\n",
    "    if include_stars:\n",
    "        for x in param_list:\n",
    "            dist_param = [i for i in dist_test_pvals.index.get_level_values(0).unique()\n",
    "                          if i in x.replace('\\n',' ')][0]\n",
    "            star = '***' if (dist_test_pvals.loc[dist_param].loc[comm]<0.001).any() else '**' if \\\n",
    "                            (dist_test_pvals.loc[dist_param].loc[comm]<0.01).any() else '*' if \\\n",
    "                            (dist_test_pvals.loc[dist_param].loc[comm]<0.05).any() else '.' if \\\n",
    "                            (dist_test_pvals.loc[dist_param].loc[comm]<0.1).any() else ' '\n",
    "            if (dist_test_pvals.loc[dist_param].loc[comm]<0.05).any():\n",
    "                sub_sub = sources_sub.copy().loc[sources_sub['Parameter']==x]\n",
    "                lit_values = sub_sub.loc[sub_sub['General source']=='Literature','Value']\n",
    "                our_values = sub_sub.loc[sub_sub['General source']=='This study','Value']\n",
    "                lit_values75 = [lit_values.quantile(0.25), lit_values.quantile(0.75)]\n",
    "                our_values75 = [our_values.quantile(0.25), our_values.quantile(0.75)]\n",
    "                if (np.min(lit_values75)<np.max(our_values75) and np.min(lit_values75)>np.min(our_values75)):\n",
    "                    stars_overlapping += [(comm,dist_param)]\n",
    "                elif (np.min(our_values75)<np.max(lit_values75) and np.min(our_values75)>np.min(lit_values75)):\n",
    "                    stars_overlapping += [(comm,dist_param)]\n",
    "                else:\n",
    "                    stars_not_overlapping += [(comm,dist_param)]\n",
    "                lit_values90 = [lit_values.quantile(0.1), lit_values.quantile(0.9)]\n",
    "                our_values90 = [our_values.quantile(0.1), our_values.quantile(0.9)]\n",
    "                if (np.min(lit_values90)<np.max(our_values90) and np.min(lit_values90)>np.min(our_values90)):\n",
    "                    stars_overlapping90 += [(comm,dist_param)]\n",
    "                elif (np.min(our_values90)<np.max(lit_values90) and np.min(our_values90)>np.min(lit_values90)):\n",
    "                    stars_overlapping90 += [(comm,dist_param)]  \n",
    "            star = f' ({star})'\n",
    "            sources_sub.loc[sources_sub['Parameter']==x,'Parameter'] = x+star\n",
    "        param_list = sources_sub['Parameter'].unique()\n",
    "    return sources_sub, param_list, stars_overlapping, stars_overlapping90, stars_not_overlapping\n",
    "            \n",
    "def update_sources_sub_linelength(sources_sub, param_list):\n",
    "    for x in [i for i in param_list if len(i)>len('Mine cost elasticity to ')]:\n",
    "        split = x.split(' ')\n",
    "        num_words = 3 if ' x$ ' not in x else 4\n",
    "        sources_sub.loc[sources_sub.Parameter==\n",
    "                    x,'Parameter'\n",
    "                   ] = ' '.join(split[:num_words])+'\\n'+' '.join(split[num_words:])\n",
    "    return sources_sub\n",
    "\n",
    "def initialize_big_plot():\n",
    "    shapex, shapey = 20,3\n",
    "    cu_span = 15\n",
    "    al_span = 8\n",
    "    ni_span = 7\n",
    "    pb_span = 6\n",
    "    au_span = 4\n",
    "    ag_span = 4\n",
    "    fig, ax = plt.subplot_mosaic('A'*cu_span + 'B'*(shapex-cu_span)+';'+\n",
    "                                 'C'*al_span + 'D'*ni_span + 'E'*(shapex-al_span-ni_span)+';'+\n",
    "                                 'F'*pb_span + 'G'*au_span + 'H'*ag_span + 'I'*(shapex-pb_span-au_span-ag_span),\n",
    "                                 figsize=(30,28)\n",
    "                                )\n",
    "    axes = {}\n",
    "    axes['Cu'] = ax['A']\n",
    "    axes['Steel'] = ax['B']\n",
    "    axes['Al'] = ax['C']\n",
    "    axes['Ni'] = ax['D']\n",
    "    axes['Sn'] = ax['E']\n",
    "    axes['Pb'] = ax['F']\n",
    "    axes['Au'] = ax['G']\n",
    "    axes['Ag'] = ax['H']\n",
    "    axes['Zn'] = ax['I']\n",
    "    return fig, axes\n",
    "            \n",
    "def plot_violin_ax(sources_sub, ax, comm):\n",
    "    sns.violinplot(ax=ax, data=sources_sub, x='Parameter', y='Value', hue='General source',\n",
    "                   linewidth=4, cut=0, width=0.95, inner='box', palette=['#666666','#66a61e'],\n",
    "                  )\n",
    "    fontsize=22\n",
    "    if fontsize is None:\n",
    "        ax.tick_params(axis='x',rotation=90)\n",
    "        ax.tick_params(axis='y')\n",
    "        ax.set_title(comm)\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend(title=None, loc='lower right')\n",
    "    else:\n",
    "        ax.tick_params(axis='x',rotation=90,labelsize=fontsize)\n",
    "        ax.tick_params(axis='y',labelsize=fontsize)\n",
    "        ax.set_title(comm,fontsize=fontsize*1.1)\n",
    "        ax.set_ylabel('Value',fontsize=fontsize)\n",
    "        ax.legend(title=None, fontsize=fontsize, loc='lower right')\n",
    "    if comm not in ['Copper','Zinc']:\n",
    "        ax.get_legend().remove()\n",
    "    return ax\n",
    "            \n",
    "def print_paper_info(many, sources_plt, dist_test_pvals,\n",
    "                     stars_overlapping, stars_overlapping90, stars_not_overlapping):\n",
    "    print('Number of unique publications (excluding our own work):', \n",
    "          np.unique([i.split(',')[0].split(' -')[0].split(')b')[0].split(')a')[0] \n",
    "               for i in many.sources.index.get_level_values(1) if 'This study' not in i]).shape[0])\n",
    "    print('Number of unique regressions (excluding our own work):', \n",
    "          np.unique([i\n",
    "               for i in many.sources.index.get_level_values(1) if 'This study' not in i]).shape[0])\n",
    "    print('Number of unique methods (excluding our own work and treating long- and short-run as same):',\n",
    "         np.unique([i[2].split(',')[0].split(' short')[0].split(\n",
    "             ' long')[0].replace('OLS','Ordinary least squares')\n",
    "           for i in many.sources_methods.index if 'This study' not in i[1]]).shape[0]\n",
    "         )\n",
    "    print('Number of unique values (excluding our own work):',\n",
    "          many_sg.sources.stack().loc[idx[:,\n",
    "                [i for i in many.sources.index.get_level_values(1) if 'This study' not in i],\n",
    "                :]].shape[0]\n",
    "         )\n",
    "    print('Number of unique parameters:', sources_plt['Parameter'].unique().shape[0])\n",
    "    print('Percent of distributions significantly different by either test (KW or KS), 95% confidence:',\n",
    "          round(100*(dist_test_pvals<0.05).any(axis=1).sum()/dist_test_pvals.shape[0],3),\n",
    "          f'({(dist_test_pvals<0.05).any(axis=1).sum()}/{dist_test_pvals.shape[0]})'\n",
    "         )\n",
    "    print('Percent of distributions significantly different by either test (KW or KS), 99% confidence:',\n",
    "          round(100*(dist_test_pvals<0.01).any(axis=1).sum()/dist_test_pvals.shape[0],3),\n",
    "          f'({(dist_test_pvals<0.01).any(axis=1).sum()}/{dist_test_pvals.shape[0]})'\n",
    "         )\n",
    "    print('Number of significantly different (95%) dists that overlap their 25-75% percentiles (their boxes):',\n",
    "          f'{len(stars_overlapping)}/{len(stars_overlapping)+len(stars_not_overlapping)}'\n",
    "         )\n",
    "    print('Number of significantly different (95%) dists that overlap their 10-90% percentiles:',\n",
    "          f'{len(stars_overlapping90)}/{len(stars_overlapping)+len(stars_not_overlapping)}'\n",
    "         )\n",
    "\n",
    "def plot_comparative_violins(many, dpi=100):\n",
    "    with warnings.catch_warnings():\n",
    "        include_stars = True\n",
    "        warnings.simplefilter('error')\n",
    "\n",
    "        sources, this_study, sources_plt = get_sources_this_study(many)\n",
    "\n",
    "        sources_plt = update_ore_grade_annual_to_COT(many, sources_plt)\n",
    "\n",
    "        dist_test_pvals = get_distribution_differences(sources, this_study)\n",
    "\n",
    "        fig, axes = initialize_big_plot()\n",
    "\n",
    "        stars_overlapping = []\n",
    "        stars_overlapping90 = []\n",
    "        stars_not_overlapping = []\n",
    "        for comm in sources_plt.Commodity.unique():\n",
    "            ax = axes[comm]\n",
    "            sources_sub = sources_plt.loc[sources_plt['Commodity']==comm]\n",
    "            sources_sub, param_list = \\\n",
    "                update_sources_sub_parameters(sources_sub)\n",
    "\n",
    "            sources_sub, param_list, stars_overlapping, stars_overlapping90, stars_not_overlapping = \\\n",
    "                update_sources_sub_stars(comm, sources_sub, dist_test_pvals,\n",
    "                                         param_list, include_stars,\n",
    "                                         stars_overlapping, stars_overlapping90, stars_not_overlapping)\n",
    "\n",
    "            sources_sub = \\\n",
    "                update_sources_sub_linelength(sources_sub, param_list)\n",
    "\n",
    "            ax = plot_violin_ax(sources_sub, ax, comm)\n",
    "            # if comm == sources_plt.Commodity.unique()[0]:\n",
    "            #     ax.legend()\n",
    "\n",
    "        fig.tight_layout(pad=0.3)\n",
    "        fig.set_dpi(dpi)\n",
    "        fig.axes[0].legend()\n",
    "        plt.show()\n",
    "        plt.savefig('figures/literature_comparison.pdf')\n",
    "        plt.close()\n",
    "\n",
    "        print_paper_info(many, sources_plt, dist_test_pvals,\n",
    "                         stars_overlapping, stars_overlapping90, stars_not_overlapping)\n",
    "        return sources, this_study, sources_plt, fig\n",
    "\n",
    "sources, this_study, sources_plt, fig = plot_comparative_violins(many_sg, dpi=200);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9348b07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_plot2(fontsize=20,figsize=(8,5.5),font='Arial',font_family='sans-serif',linewidth=4,font_style='bold',have_axes=True,dpi=50,marker=None,markersize=12,markeredgewidth=1.0,markeredgecolor=None,markerfacecolor=None, markercycler=False, cmap='Dark2', n_colors=8, **kwargs):\n",
    "    '''Sets default plot formats.\n",
    "    Potential inputs: fontsize, figsize, font,\n",
    "    font_family, font_style, linewidth, have_axes,\n",
    "    dpi, marker, markersize, markeredgewidth,\n",
    "    markeredgecolor, markerfacecolor.\n",
    "    have_axes: determines whether there is a border\n",
    "    on the plot. Also has **kwargs so that any other\n",
    "    arguments that can be passed to mpl.rcParams.update\n",
    "    that were not listed above.\n",
    "\n",
    "    cmap can take any matplotlib colormap string.'''\n",
    "    import matplotlib as mpl\n",
    "    params = {\n",
    "        'axes.labelsize': fontsize,\n",
    "        'font.size': fontsize,\n",
    "        'axes.titlesize':fontsize+1,\n",
    "        'axes.titleweight':font_style,\n",
    "        'figure.titlesize':fontsize+1,\n",
    "        'legend.fontsize': fontsize,\n",
    "        'xtick.labelsize': fontsize,\n",
    "        'ytick.labelsize': fontsize,\n",
    "        'axes.titlesize': fontsize+2,\n",
    "        'figure.titlesize': fontsize+2,\n",
    "        'text.usetex': False,\n",
    "        'figure.figsize': figsize,\n",
    "        'lines.linewidth': linewidth,\n",
    "        'lines.solid_capstyle': 'round',\n",
    "        'legend.framealpha': 1,\n",
    "        'legend.frameon': False,\n",
    "        'mathtext.default': 'regular',\n",
    "        'axes.linewidth': 2/3*linewidth,\n",
    "        'xtick.direction': 'in', # in, out, inout\n",
    "        'ytick.direction': 'in', # in, out, inout\n",
    "        'xtick.major.size': 7,\n",
    "        'xtick.major.width': 2,\n",
    "        'xtick.major.pad': 3.5,\n",
    "        'ytick.major.size': 7,\n",
    "        'ytick.major.width': 2,\n",
    "        'ytick.major.pad': 3.5,\n",
    "        'font.family': font_family,\n",
    "        'font.'+font_family: font,\n",
    "        'figure.dpi': dpi,\n",
    "        'lines.marker': marker,\n",
    "        'lines.markersize':markersize,\n",
    "        'lines.markeredgewidth':markeredgewidth,\n",
    "        'pdf.fonttype': 42,\n",
    "        'ps.fonttype': 42,\n",
    "        'svg.fonttype': 'none',\n",
    "        'savefig.transparent': True,\n",
    "        'savefig.bbox': 'tight',\n",
    "        'axes.facecolor': 'white',\n",
    "        'axes.edgecolor':'k',\n",
    "        }\n",
    "\n",
    "    mpl.rcParams.update(params)\n",
    "    mpl.rcParams.update(**kwargs)\n",
    "    mpl.rcParams['axes.spines.left'] = have_axes\n",
    "    mpl.rcParams['axes.spines.right'] = have_axes\n",
    "    mpl.rcParams['axes.spines.top'] = have_axes\n",
    "    mpl.rcParams['axes.spines.bottom'] = have_axes\n",
    "    mpl.rcParams['axes.axisbelow'] = True\n",
    "    mpl.rcParams['axes.grid'] = True\n",
    "    mpl.rcParams['axes.grid.axis'] = 'y'\n",
    "    mpl.rcParams['grid.color'] = '0.9'\n",
    "    mpl.rcParams['grid.linewidth'] = 1\n",
    "    mpl.rcParams['grid.linestyle'] = '-'\n",
    "    mpl.rcParams['axes.labelweight'] = font_style\n",
    "    mpl.rcParams['font.weight'] = font_style\n",
    "\n",
    "    sns.set_palette(cmap)\n",
    "\n",
    "    if markeredgecolor != None:\n",
    "        mpl.rcParams['lines.markeredgecolor'] = markeredgecolor\n",
    "    if markerfacecolor != None:\n",
    "        mpl.rcParams['lines.markerfacecolor'] = markerfacecolor\n",
    "    if markercycler:\n",
    "        cmap_colors = mpl.cm.get_cmap(cmap)\n",
    "        colors = [cmap_colors(i) for i in np.linspace(0,1,n_colors)]\n",
    "        markers = [\"o\",\"s\",\"^\",\"v\",\"p\",\"8\",\"P\",\"X\"]\n",
    "        if n_colors>8: markers = np.tile(markers,int(np.floor(n_colors/8)+1))\n",
    "        markers = markers[:n_colors]\n",
    "        mpl.rcParams['axes.prop_cycle'] =  cycler('color',colors) + cycler('marker',markers)\n",
    "\n",
    "init_plot2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09493de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pp.loc[idx['Demand elasticity to GDP',:,:,:,:, 'Cu' ,:,:],:].astype(float).plot.hist()\n",
    "pp.index.get_level_values(0).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53cf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "meds.Ref.unique().reshape(-1,1)\n",
    "styled_df['Publication'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51086672",
   "metadata": {
    "code_folding": [
     0,
     89,
     218,
     251
    ],
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def commodity_level_feature_importance_heatmap(self,dpi=50,recalculate=True,objective=None, only_plot_names=False, normalize=False, importances_df_column='Mean no dummies'):\n",
    "    \"\"\"\n",
    "    Creates a plot showing commodity level feature importances in heatmap form.\n",
    "\n",
    "    dpi: int, dots per inch, controls figure resolution.\n",
    "    recalculate: bool, whether or not feature importance gets recalculated,\n",
    "      should keep as True unless you have just run this function with\n",
    "      recalculate=True and just want to update dpi or other plotting components.\n",
    "    objective: None or str, string must correspond with a column in the results\n",
    "      dataframe\n",
    "    only_plot_names: bool, default False, restricts the variables shown to be\n",
    "      those defined in the names variable within this function. Does not seem\n",
    "      useful anymore\n",
    "    normalize: bool, default False. If True, divides all feature importance\n",
    "      values in each commodity by the commodity maximum; gives relative importance\n",
    "      where most important parameter has the value 1, rather than all summing\n",
    "      to 1\n",
    "    \"\"\"\n",
    "    names = ['Intensity elasticity to GDP',\n",
    "                 'Intensity decline per year',\n",
    "                 'Intensity elasticity to price',\n",
    "                 'Mine CU elasticity to TCM',\n",
    "                 'Incentive pool opening probability',\n",
    "                 'Ore grade elasticity distribution mean',\n",
    "                 'Mine cost reduction per year',\n",
    "                 'TCRC elasticity to SD',\n",
    "                 'Refinery SR elasticity to scrap spread',\n",
    "                 'Refinery SR elasticity to TCRC',\n",
    "                 'Secondary refinery CU elasticity to price',\n",
    "                 'Primary refinery CU elasticity to price',\n",
    "                 'Scrap spread elasticity to SD',\n",
    "                 'Ref. cap. growth frac. from mine prod. growth',\n",
    "                 'TCRC elasticity to price',\n",
    "                 'Collection elasticity to scrap price',\n",
    "                 'Direct melt fraction elasticity to scrap spread']\n",
    "    if not hasattr(self,'importances_df_reformed') or recalculate:\n",
    "        importances_df = pd.DataFrame()\n",
    "        importances_predict = pd.DataFrame()\n",
    "        importance_r2 = pd.DataFrame()\n",
    "        if hasattr(self,'rmse_df'):\n",
    "            outer = self.rmse_df.copy()\n",
    "        else:\n",
    "            outer = self.multi_scenario_results.copy()\n",
    "        for comm in list(outer.index.get_level_values(0).unique())+[None]:\n",
    "            feature_importance(self,commodity=comm,recalculate=True,plot=False,objective=objective)\n",
    "            ph = self.importances_df[importances_df_column]\n",
    "            ph.name=comm if comm!=None else 'None'\n",
    "            importances_df = pd.concat([importances_df,ph],axis=1)\n",
    "            ph = pd.concat([self.importance_test],keys=[str(comm)])\n",
    "            importances_predict = pd.concat([importances_predict, ph])\n",
    "            ph = pd.concat([self.importance_r2],keys=[str(comm)])\n",
    "            importance_r2 = pd.concat([importance_r2, ph])\n",
    "            \n",
    "        self.importances_df_reformed = importances_df.rename(\n",
    "            make_parameter_names_nice(importances_df.index)).rename(\n",
    "            columns=dict(zip(importances_df.columns,[i.capitalize().replace('None','All') for i in importances_df.columns])))\n",
    "        self.importances_predict = importances_predict.copy()\n",
    "        self.importance_r2_all = importance_r2.copy()\n",
    "        if only_plot_names:\n",
    "            self.importances_df_reformed = self.importances_df_reformed.loc[[i for i in names if i in self.importances_df_reformed.index]]\n",
    "\n",
    "    fig,ax = easy_subplots(1,height_scale=1.1*self.importances_df.shape[0]/len(names))\n",
    "    a = ax[0]\n",
    "    if normalize:\n",
    "        self.importances_df_reformed = self.importances_df_reformed.div(self.importances_df_reformed.max())\n",
    "        cbar_kws = {'label': 'Relative feature importance'}\n",
    "    else:\n",
    "        cbar_kws = {'label': 'Feature importance'}\n",
    "    self.importances_df_reformed.rename(columns=self.commodity_element_map, inplace=True)\n",
    "    self.importances_df_reformed = self.importances_df_reformed.sort_values(by='All',ascending=False)[[\n",
    "        'Ag','Al','Au','Cu','Ni','Pb','Sn','Steel','Zn','All']]\n",
    "    if not normalize:\n",
    "        cbar_kws['ticks']=[0, 0.01, 0.05, 0.1, 0.2, 0.4]\n",
    "        cbar_kws['format'] = '%.2f'\n",
    "    additional_kws = {}\n",
    "    if not normalize:\n",
    "        additional_kws['annot'] = self.importances_df_reformed \n",
    "        additional_kws['vmax'] = 0.1\n",
    "        additional_kws['annot_kws'] = {'fontsize':10}\n",
    "        additional_kws['norm'] = mpl.colors.LogNorm()\n",
    "    sns.heatmap(self.importances_df_reformed,\n",
    "                xticklabels=True,yticklabels=True,ax=a,cbar_kws=cbar_kws,cmap='OrRd',\n",
    "                **additional_kws, \n",
    "               )\n",
    "    fig.set_dpi(dpi)\n",
    "    a.tick_params(axis='x', labelbottom=True, labeltop=True, labelrotation=90)\n",
    "    a.set_title('Commodity level\\nfeature importance',weight='bold')\n",
    "    return fig,a\n",
    "\n",
    "def feature_importance(self,plot=None,recalculate=False, standard_scaler=True, \n",
    "                       plot_commodity_importances=False, commodity=None, objective=None, \n",
    "                       processing_option=None, dpi=50):\n",
    "    '''\n",
    "    Calculates feature importances using three different tree-based machine\n",
    "    learning algorithms: RandomForestRegressor, ExtraTreesRegressor, and\n",
    "    GradientBoostingRegressor. Takes the mean of all three. Creates new\n",
    "    variable in self called importances_df that includes the outcomes of this\n",
    "    method.\n",
    "\n",
    "    self: Many() instance with rmse_df object or\n",
    "        multi_scenario_results_formatted object if objective!=None.\n",
    "    plot: None, bool, or str. if True or None, plots using the\n",
    "        plot_feature_importances method. If str `both`, plots\n",
    "        using plot_train_test and plot_feature_importances variables True.\n",
    "        Should have made these into separate methods but here we are.\n",
    "        False means no plotting. (recalculate has to be True for\n",
    "        plot_train_test to work)\n",
    "        - plot_train_test plots the predicted vs actual scores for the test\n",
    "          set, for each ML tree method, and shows the associated regression\n",
    "          characteristics.\n",
    "        - plot_feature_importances plots the bar plot of feature importances\n",
    "          with bars for each regression method, with and without dummy variables\n",
    "          included (dummy variables should only make a difference if we are\n",
    "          doing all commodities simultaneously)\n",
    "    recalculate: bool, whether to recalculate feature importance, only set\n",
    "        False if this has already been run and you want to plot things.\n",
    "    standard_scaler: bool, whether to use standard scaler to rescale data so it\n",
    "        is N(0,1), which we should probably always do.\n",
    "    plot_commodity_importances: bool, whether to include the importance of each\n",
    "        commodity in the `with dummies` subplot for plot_feature_importances\n",
    "    commodity: None or str. If str, has to be lowercase commodity name, in which\n",
    "        case this gets the right commodity from the corresponding dataframe. If\n",
    "        None, uses the full dataframe, going across all commodities\n",
    "    objective: None or str. If str, has to be one of the columns in\n",
    "        multi_scenario_results_formatted, otherwise will be using `score` or\n",
    "        `RMSE` from rmse_df dataframe depending on whether using and Integration\n",
    "        or demandModel formulation\n",
    "    processing_option: None or str, used to determine which method we use to\n",
    "        set up y_df in get_X_df_y_df function, particularly for criticality\n",
    "        evaluation. Options are:\n",
    "        - `2040 ratio 2018 hist`: y_df will contain the ratio between the\n",
    "           simulated 2040 value and the historical 2018 value, for the given\n",
    "           objective, provided the objective has a corresponding historical\n",
    "           value\n",
    "    dpi: float, dots per inch, controls figure resolution.\n",
    "    '''\n",
    "\n",
    "    if processing_option is not None:\n",
    "        self.processing_option = processing_option\n",
    "\n",
    "    if hasattr(self,'objective'):\n",
    "        objective=self.objective\n",
    "    elif objective is None:\n",
    "        objective='score'\n",
    "    self.objective = objective\n",
    "\n",
    "    split_frac = 0.5\n",
    "\n",
    "    if plot==None or (type(plot)==bool and plot):\n",
    "        plot_train_test=False\n",
    "        plot_feature_importances=True\n",
    "    elif type(plot)==bool and not plot:\n",
    "        plot_train_test=False\n",
    "        plot_feature_importances=False\n",
    "    elif type(plot)==str and plot=='both':\n",
    "        plot_train_test=True\n",
    "        plot_feature_importances=True\n",
    "\n",
    "    get_X_df_y_df(self, commodity=commodity, objective=objective, standard_scaler=standard_scaler)\n",
    "\n",
    "    if not hasattr(self,'importances_df') or recalculate or plot_commodity_importances:\n",
    "        self.importance_test = None\n",
    "        importances_df = pd.DataFrame()\n",
    "        importance_r2 = pd.DataFrame()\n",
    "        for Regressor, name in zip([RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor],['RandomForest','ExtraTrees','GradientBoosting']):\n",
    "            if plot_train_test: fig2,ax2 = easy_subplots(2,dpi=dpi)\n",
    "            for e,dummies in enumerate([True,False]):\n",
    "                for quantile in [0.975, 1]:\n",
    "                    X_df, y_df = self.X_df.copy().fillna(0), self.y_df.copy()\n",
    "\n",
    "                    if dummies:\n",
    "                        X_df.loc[:,'commodity =']=X_df.index.get_level_values(0)\n",
    "                        X_df = pd.get_dummies(X_df,columns=['commodity ='])\n",
    "\n",
    "                    y_df = y_df.loc[y_df['score'] < y_df['score'].quantile(quantile)]\n",
    "                    y_df = y_df.loc[y_df['score'] > y_df['score'].quantile(1-quantile)]\n",
    "                    X_df = X_df.loc[y_df.index]\n",
    "\n",
    "\n",
    "                    X = X_df.reset_index(drop=True).values\n",
    "                    y = y_df.values.flatten()\n",
    "\n",
    "                    X_train, X_test, y_train, y_test, ind_train, ind_test = train_test_split(X,\n",
    "                                                                                             y,\n",
    "                                                                                             y_df.index,\n",
    "                                                                                             test_size=0.33,\n",
    "                                                                                             random_state=42)\n",
    "                    X_df_test = X_df.loc[ind_test]\n",
    "                    y_df_test = y_df.loc[ind_test]\n",
    "\n",
    "                    regr = Regressor(random_state=0, n_estimators=200)\n",
    "                    regr.fit(X_train, y_train)\n",
    "\n",
    "                    test = pd.concat([X_df_test,y_df_test],axis=1)\n",
    "                    if self.importance_test is None:\n",
    "                        self.importance_test = test.copy()\n",
    "                    test.loc[:,'predicted '+objective] = regr.predict(X_test)\n",
    "                    dumm_name = 'w/ dummies' if dummies else 'no dummies'\n",
    "                    self.importance_test.loc[:,'predicted '+objective+' '+name+' '+dumm_name] = test['predicted '+objective]\n",
    "\n",
    "                    if not plot_commodity_importances:\n",
    "                        importances = pd.Series(regr.feature_importances_, X_df.columns).drop([i for i in X_df.columns if 'commodity =' in i]).sort_values(ascending=False)\n",
    "                    else:\n",
    "                        importances = pd.Series(regr.feature_importances_, X_df.columns).sort_values(ascending=False)\n",
    "                    importances.name =  name + (' w/ dummies' if dummies else ' no dummies') + f', {quantile} quantile'\n",
    "\n",
    "                    r2_name = importances.name.replace(f', {quantile} quantile','')\n",
    "                    importance_r2.loc[r2_name, quantile] = r2_score(y_test, regr.predict(X_test))\n",
    "\n",
    "                    if dummies:\n",
    "                        importances /= importances.sum()\n",
    "                        self.X_df_dummies = X_df.copy()\n",
    "                        self.y_df_dummies = y_df.copy()\n",
    "                    else:\n",
    "                        self.X_df_no_dummies = X_df.copy()\n",
    "                        self.y_df_no_dummies = y_df.copy()\n",
    "                    importances_df = pd.concat([importances_df, importances],axis=1)\n",
    "\n",
    "                    if plot_train_test:\n",
    "                        if objective is None and 'RMSE' in test.columns:\n",
    "                            target='RMSE'\n",
    "                        elif objective is None:\n",
    "                            target = 'score'\n",
    "                        else: target = objective\n",
    "                        do_a_regress(test[target], test['predicted '+objective],ax=ax2[e])\n",
    "                        ax2[e].set(title=importances.name.replace(' w/','\\nw/').replace(' no','\\nno'))\n",
    "\n",
    "        # adding columns to importances_df that don't have the quantile label, taking best of quantiles\n",
    "        for dummies in [True, False]:\n",
    "            for name in importance_r2.index:\n",
    "                quantile = importance_r2.loc[name].idxmax()\n",
    "                quantile = quantile if quantile!=1 else int(quantile)\n",
    "                df_name = name + f', {quantile} quantile'\n",
    "                importances_df.loc[:, name] = importances_df[df_name]\n",
    "        \n",
    "        dummy_cols = [i for i in importances_df.columns if 'w/ dummies' in i and 'quantile' not in i]\n",
    "        no_dummy_cols=[i for i in importances_df.columns if 'no dummies' in i and 'quantile' not in i]\n",
    "        importances_df.loc[:,'Mean w/ dummies'] = importances_df[dummy_cols].mean(axis=1)\n",
    "        importances_df.loc[:,'Mean w/ dummies'] /= importances_df['Mean w/ dummies'].sum()\n",
    "        importances_df.loc[:,'Mean no dummies'] = importances_df[no_dummy_cols].mean(axis=1)\n",
    "        importances_df.loc[:,'Mean no dummies'] /= importances_df['Mean no dummies'].sum()\n",
    "        dummy_cols += ['Mean w/ dummies']\n",
    "        no_dummy_cols += ['Mean no dummies']\n",
    "        \n",
    "        importances_df.loc[:, 'Best test R2'] = importances_df[importance_r2.max(axis=1).idxmax()]\n",
    "        self.importances_df = importances_df.copy()\n",
    "        self.importance_r2 = importance_r2.copy()\n",
    "\n",
    "    dummy_cols = [i for i in self.importances_df.columns if 'w/ dummies' in i]\n",
    "    no_dummy_cols=[i for i in self.importances_df.columns if 'no dummies' in i]\n",
    "\n",
    "    if plot_feature_importances:\n",
    "        to_plot_du = self.importances_df.loc[:,dummy_cols].sort_values(by='Mean w/ dummies',ascending=False)\n",
    "        to_plot_du.rename(columns=dict(zip(dummy_cols,[i.split(' w/ dummies')[0] for i in dummy_cols])),inplace=True)\n",
    "        to_plot_no = self.importances_df.loc[:,no_dummy_cols].sort_values(by='Mean no dummies',ascending=False).dropna()\n",
    "        to_plot_no.rename(columns=dict(zip(no_dummy_cols,[i.split(' no dummies')[0] for i in no_dummy_cols])),inplace=True)\n",
    "        to_plot_du.rename(make_parameter_names_nice(to_plot_du.index),inplace=True)\n",
    "        to_plot_no.rename(make_parameter_names_nice(to_plot_no.index),inplace=True)\n",
    "        height_scale = 2 if plot_commodity_importances and 'Mine cost reduction per year' in to_plot_no.index else 1.7 if plot_commodity_importances else 1.5\n",
    "        fig1,ax1 = easy_subplots(2, height_scale=height_scale, width_scale=9/len(self.importances_df.columns), dpi=dpi,\n",
    "                            width_ratios=(to_plot_du.shape[0],to_plot_no.shape[0]))\n",
    "        to_plot_du.plot.bar(ax=ax1[0],ylabel='Feature importance').grid(axis='x')\n",
    "        to_plot_no.plot.bar(ax=ax1[1],ylabel='Feature importance',legend=not plot_commodity_importances).grid(axis='x')\n",
    "        ax1[0].set_title('With commodity dummies',weight='bold')\n",
    "        ax1[1].set_title('No dummies',weight='bold')\n",
    "        y1a, y1b = ax1[0].get_ylim()\n",
    "        y2a, y2b = ax1[1].get_ylim()\n",
    "        ya = min(y1a,y2a)\n",
    "        yb = max(y1b,y2b)\n",
    "        ax1[0].set(ylim=(ya,yb))\n",
    "        ax1[1].set(ylim=(ya,yb))\n",
    "        if plot_commodity_importances:\n",
    "            ax1[1].set_yticklabels([])\n",
    "            ax1[1].set_ylabel(None)\n",
    "            ax1[1].yaxis.set_tick_params(width=0)\n",
    "            fig1.tight_layout(pad=0.5)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        return fig1, ax1\n",
    "\n",
    "many = many_sg\n",
    "# many = many_b\n",
    "fig,ax = commodity_level_feature_importance_heatmap(many, \n",
    "                                                    recalculate = True,\n",
    "                                                    objective = None, \n",
    "                                                    normalize = False, \n",
    "                                                    importances_df_column = 'Mean no dummies',\n",
    "                                                    dpi = 200)\n",
    "plt.show()\n",
    "plt.close()\n",
    "display(pd.concat([\n",
    "    many.importance_r2_all.rename({np.nan:'All'}).groupby(level=0).max().idxmax(axis=1),\n",
    "    many.importance_r2_all.rename({np.nan:'All'}).max(axis=1).groupby(level=0).max()\n",
    "], axis=1, \n",
    "    keys=['Score quantiles used for each commodity',\n",
    "          'Best-performing ML model R2 value for test set prediction vs actual']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c111b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_st.integ.results_sorted.loc[idx[:,:25,2001:2019],'Collection rate construction':'Collection rate transport']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89bb91f",
   "metadata": {},
   "source": [
    "# Paper figures - SI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a6d7c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_future_for_variable(many, commodity='copper', var='Mean total cash margin', n=50, \n",
    "                color='#1b9e77', dpi=50, end_year=2040, ax=0):\n",
    "    if type(ax)==int:\n",
    "        fig, ax = plt.subplots()\n",
    "    else: fig=0\n",
    "    data = many.results_sorted[var].loc[commodity].loc[idx[:n,2001:end_year]]\n",
    "    if 'cost' not in var and 'margin' not in var and 'grade' not in var and 'TCRC' not in var:\n",
    "        _ = get_unit(data,data,var)\n",
    "        data = _['simulated']\n",
    "        label= _['unit']\n",
    "    elif 'grade' in var:\n",
    "        label = '%'\n",
    "    else:\n",
    "        label = 'USD/t'\n",
    "    data = data.reset_index()\\\n",
    "        .rename(columns={'level_1':'Year'})\n",
    "    sns.lineplot(data=data, x='Year',y=var, color=color, ax=ax, label='Simulated')\n",
    "    title=var.replace('Spread','Scrap spread').replace('Refined price','Cathode price')\\\n",
    "              .replace('Mean ','').replace('mine grade','ore grade')\n",
    "    if title!='TCRC': title=title.capitalize()\n",
    "    ax.set(ylabel=var+f' ({label})', title=commodity.capitalize())\n",
    "    return fig, ax\n",
    "\n",
    "def plot_actual_variables(many, variable='Mean mine grade', dpi=50):\n",
    "    \"\"\"\n",
    "    varible: str, options are: `Mean mine grade`, \n",
    "      `Mean total minesite cost`, `Mean total cash margin`, \n",
    "      `TCRC`\n",
    "    \"\"\"\n",
    "    colors = ['#1b9e77','#d95f02','#7570b3','#e7298a','#66a61e','#e6ab02','#a6761d','#666666']\n",
    "    commodities = many.rmse_df.index.get_level_values(0).unique()\n",
    "#     mean_grade_actual = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx',\n",
    "#                                       sheet_name='Ore grade', index_col=0)\n",
    "    mean_grade_actual = pd.read_excel(\n",
    "        'generalization/input_files/static/SP Global weighted mean mine parameters update.xlsx',\n",
    "        index_col=0, header=[0,1])\n",
    "    variable_map = {'Mean mine grade':'Grade (%)', \n",
    "                 'TCRC':'Inflation-adjusted TCRC (USD/t)',\n",
    "                 'Mean total minesite cost':'Inflation-adjusted minesite cost (USD/t)', \n",
    "                 'Mean total cash margin':'Inflation-adjusted total cash margin (USD/t)'\n",
    "                }\n",
    "    actual_variable = variable_map[variable]\n",
    "    mean_grade_actual = mean_grade_actual.loc[:2019,idx[:,actual_variable]].droplevel(1,axis=1)\n",
    "    fig,ax = easy_subplots(commodities, dpi=dpi, height_scale=0.8)\n",
    "    for comm,color,a in zip(commodities, np.tile(colors,(2)),ax):\n",
    "        # comm='gold'\n",
    "        ax = plot_future_for_variable(many, var=variable, ax=a,\n",
    "                                        color=color, dpi=50, n=20, commodity=comm, end_year=2019)[1]\n",
    "        if comm.capitalize() in mean_grade_actual.columns:\n",
    "            ax.plot(mean_grade_actual.loc[2001:, comm.capitalize()], color='k', label='Historical')\n",
    "            ax.legend()\n",
    "        else:\n",
    "            ax.get_legend().remove()\n",
    "    fig.tight_layout()\n",
    "    fig.set_dpi(150)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "plot_actual_variables(many_sg, variable='Mean mine grade')\n",
    "plot_actual_variables(many_sg, variable='TCRC')\n",
    "plot_actual_variables(many_sg, variable='Mean total cash margin')\n",
    "plot_actual_variables(many_sg, variable='Mean total minesite cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59aa2b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# many_b = Many()\n",
    "# many_b.load_data('2023-02-07 14_01_51_2_run_scenario_CHECK')\n",
    "demand_bool=False # demand_bool=true means plot demand, false means plot scrap collected\n",
    "years = np.arange(2001,2020)\n",
    "if demand_bool:\n",
    "    demand_cols = [i for i in many_b.results.columns if 'demand, global' in i]\n",
    "    demand = many_b.results[demand_cols].groupby(level=[0,2]).mean()\n",
    "    demand = demand.rename(columns=dict(zip(demand.columns,[i.split(' ')[0] for i in demand.columns])))\n",
    "else:\n",
    "    demand_cols = [i for i in many_b.results.columns if 'Old scrap' in i and 'collection' not in i]\n",
    "    demand = many_b.results[demand_cols].groupby(level=[0,2]).mean()\n",
    "    demand = demand.rename(columns=dict(zip(demand.columns,[i.split(' ')[-1].capitalize() for i in demand.columns])))\n",
    "\n",
    "comms = demand.index.get_level_values(0).unique()\n",
    "fig,ax=easy_subplots(comms)\n",
    "colors = dict(zip(\n",
    "    ['Bar and coin','Construction','Electrical','Industrial','Jewelry','Transport','Other'],\n",
    "    list(sns.color_palette('deep',n_colors=7))))\n",
    "\n",
    "# fig,a = plt.subplots()\n",
    "for comm,a in zip(comms,ax):\n",
    "    demand_ph = demand.copy().loc[comm].loc[years]\n",
    "    if comm in ['Ag','Au']:\n",
    "        demand_ph.rename(columns={'Transport':'Bar and coin','Industrial':'Jewelry'},inplace=True)\n",
    "    dicty = get_unit(demand_ph,demand_ph,'Demand (kt)')\n",
    "    demand_ph = dicty['simulated']\n",
    "    unit = dicty['unit']\n",
    "    stacks = a.stackplot(demand_ph.index, demand_ph.T, labels=demand_ph.columns,\n",
    "               colors=[colors[q] for q in demand_ph.columns])\n",
    "\n",
    "    hatches=[\"\\\\\", \"//\",\"+\",'x','o']\n",
    "    for stack, hatch in zip(stacks, hatches):\n",
    "        stack.set_hatch(hatch)\n",
    "    label = 'Demand' if demand_bool else 'Scrap collected'\n",
    "    a.set(title=comm, xlabel='Year', ylabel=f'{label} ({unit})',\n",
    "         )\n",
    "    a.legend()\n",
    "    h,l=a.get_legend_handles_labels()\n",
    "    a.legend(h[::-1],l[::-1], loc=(0.1,0.45), frameon=True, framealpha=0.5)\n",
    "# fig.set_dpi(150)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = np.arange(2001,2020)\n",
    "demand_cols = [i for i in many_b.results.columns if 'demand, global' in i]\n",
    "demand = many_b.results[demand_cols].groupby(level=[0,2]).mean()\n",
    "demand = demand.rename(columns=dict(zip(demand.columns,[i.split(' ')[0] for i in demand.columns])))\n",
    "\n",
    "demand_cols_cn = [i for i in many_b.results.columns if 'demand, China' in i]\n",
    "demand_cn = many_b.results[demand_cols_cn].groupby(level=[0,2]).mean()\n",
    "demand_cn = demand_cn.rename(columns=dict(zip(demand_cn.columns,[i.split(' ')[0] for i in demand_cn.columns])))\n",
    "\n",
    "china_demand_fraction = demand_cn.sum(axis=1)/demand.sum(axis=1)\n",
    "china_demand_fraction = pd.DataFrame(china_demand_fraction)\n",
    "china_demand_fraction.loc[:,'Rest of World'] = 1-china_demand_fraction.iloc[:,0]\n",
    "china_demand_fraction.rename(columns={china_demand_fraction.columns[0]:'China'},inplace=True)\n",
    "comms = china_demand_fraction.index.get_level_values(0).unique()\n",
    "fig,ax=easy_subplots(comms)\n",
    "colors = dict(zip(\n",
    "    ['China','Rest of World'],\n",
    "    list(sns.color_palette('deep',n_colors=7))))\n",
    "\n",
    "# fig,a = plt.subplots()\n",
    "for comm,a in zip(comms,ax):\n",
    "    china_demand_fraction_ph = china_demand_fraction.copy().loc[comm].loc[years]\n",
    "    stacks = a.stackplot(china_demand_fraction_ph.index, china_demand_fraction_ph.T, \n",
    "                         labels=china_demand_fraction_ph.columns,\n",
    "                         colors=[colors[q] for q in china_demand_fraction_ph.columns])\n",
    "\n",
    "    hatches=[\"\\\\\", \"//\",\"+\",'x','o']\n",
    "    for stack, hatch in zip(stacks, hatches):\n",
    "        stack.set_hatch(hatch)\n",
    "    label = 'Demand distribution'\n",
    "    a.set(title=comm, xlabel='Year', ylabel=label)\n",
    "    a.legend()\n",
    "    h,l=a.get_legend_handles_labels()\n",
    "    a.legend(h[::-1],l[::-1], loc='upper left', frameon=True, framealpha=0.5)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f24c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lifetimes = {'Transport': (15,4.5),\n",
    "            'Other': (10,2),\n",
    "            'Industrial': (20,6),\n",
    "            'Electrical': (30,9),\n",
    "            'Construction': (40,12)}\n",
    "yrs = np.linspace(0,100,200)\n",
    "pdfs = pd.DataFrame(np.nan, yrs, lifetimes.keys())\n",
    "for sec in lifetimes.keys():\n",
    "    pdf = pd.Series(stats.norm.pdf(yrs,loc=lifetimes[sec][0], scale=lifetimes[sec][1]), yrs)\n",
    "    pdfs.loc[yrs,sec] = pdf\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "pdfs.plot(ax=ax)\n",
    "ax.set(title='Lifetime distributions',xlabel='Years after initial demand', ylabel='Density')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f67310",
   "metadata": {
    "code_folding": [
     10
    ]
   },
   "outputs": [],
   "source": [
    "split_on_china = True\n",
    "pct_change = False\n",
    "include_global = True\n",
    "norm = True\n",
    "norm_year = 2019\n",
    "end_year = 2019\n",
    "dpi = 250\n",
    "\n",
    "xl = 'generalization/input_files/static/Demand prediction data-copper.xlsx'\n",
    "volumes = pd.read_excel(xl, sheet_name='All sectors', header=[0,1], \n",
    "              index_col=0).sort_index().sort_index(axis=1).stack(0).unstack()\n",
    "\n",
    "def get_jewelry_bar_coin():\n",
    "    gold_rolling_window=5\n",
    "    static_data_folder = 'generalization/input_files/static'\n",
    "    gold_vols = pd.read_excel(f'{static_data_folder}/Gold demand volume indicators.xlsx',sheet_name='Volume drivers',index_col=0).loc[2001:]\n",
    "    gold_vols1 = gold_vols.loc[2001:2019].rolling(gold_rolling_window,min_periods=1,center=True).mean()\n",
    "    gold_vols1 = pd.concat([gold_vols1,gold_vols.rolling(5,min_periods=1,center=True).mean().loc[2020:]])\n",
    "    gold_vols2 = gold_vols.rolling(gold_rolling_window+2,min_periods=1,center=True).mean()\n",
    "    global_cash_reserves = volumes.loc[:,idx[:,'Industrial']].apply(lambda x: x/x.sum(), axis=1).apply(lambda x: x*gold_vols2['Global cash reserves (USD$2021)'])#['US circulating coin production (million coins)'])\n",
    "    diamond_demand = volumes.loc[:,idx[:,'Transport']].apply(lambda x: x/x.sum(), axis=1).apply(lambda x: x*gold_vols1['Diamond demand ($B)'])\n",
    "    diamond_demand = diamond_demand.rename(columns={'Transport':'Jewelry'},level=1)\n",
    "    global_cash_reserves = global_cash_reserves.rename(columns={'Industrial':'Bar and coin'},level=1)\n",
    "    return diamond_demand, global_cash_reserves\n",
    "\n",
    "jewelry, bar_and_coin = get_jewelry_bar_coin()\n",
    "volumes = pd.concat([volumes, jewelry, bar_and_coin], axis=1)\n",
    "sector_units = {\n",
    "    'Transport':'Vehicle sales (million vehicles/year)',\n",
    "    'Construction':'Value added in construction (2010 USD)',\n",
    "    'Electrical':'Total grid power demand (GW)',\n",
    "    'Industrial':'Value added in manufacturing (2010 USD)',\n",
    "    'Other':'Proxy: GDP (2010 USD)',\n",
    "    'Jewelry':'Diamond demand (billion USD)',\n",
    "    'Bar and coin':'Global cash reserves (2021 USD)'\n",
    "}\n",
    "sectors = volumes.columns.get_level_values(1).unique()\n",
    "non_china = ['EU', 'Japan', 'NAM', 'ROW']\n",
    "if split_on_china:\n",
    "    ph = pd.concat([volumes.loc[:,idx[non_china,:]].groupby(axis=1,level=1).sum()],keys=['RoW'],axis=1)\n",
    "    volumes = pd.concat([volumes, ph],axis=1).drop(non_china,axis=1,level=0)\n",
    "if include_global:\n",
    "    ph = pd.concat([volumes.groupby(axis=1,level=1).sum()],keys=['Global'],axis=1)\n",
    "    volumes = pd.concat([volumes, ph], axis=1)\n",
    "    volumes = volumes.replace(0,np.nan)\n",
    "if pct_change:\n",
    "    volumes = volumes.pct_change().dropna()\n",
    "if norm:\n",
    "    volumes = volumes.apply(lambda x: x/volumes.loc[norm_year], axis=1)\n",
    "volumes = volumes.loc[:end_year]\n",
    "fig,ax = easy_subplots(sectors, 4)\n",
    "for sector, a in zip(sectors,ax):\n",
    "    volumes.loc[:,idx[:,sector]].droplevel(1,axis=1).plot(\n",
    "        ax=a, title=sector, xlabel='Year', ylabel=sector_units[sector]\n",
    "    )\n",
    "    a.legend(title=None)\n",
    "\n",
    "which = 'Percent per year' if pct_change else 'Actual'\n",
    "cols_to_use = {'Percent per year':'A:F', 'Actual':'H:M'}\n",
    "if split_on_china: \n",
    "    cols_to_use['Actual'] = 'W:AB'\n",
    "cols = cols_to_use[which]\n",
    "gdp = pd.read_excel(xl, sheet_name='GDP growth', header=[0], \n",
    "              index_col=0, usecols=cols).sort_index().sort_index(axis=1).dropna()\n",
    "if include_global:\n",
    "    ph = pd.DataFrame(gdp.sum(axis=1))\n",
    "    gdp = pd.concat([gdp, ph.rename(columns={ph.columns[0]:'Global'})], axis=1)\n",
    "if split_on_china:\n",
    "    pop = pd.read_excel(xl, sheet_name='GDP growth', header=[0], \n",
    "              index_col=0, usecols='O:T').sort_index().sort_index(axis=1).dropna()\n",
    "    if include_global:\n",
    "        ph = pd.DataFrame(pop.sum(axis=1))\n",
    "        pop = pd.concat([pop, ph.rename(columns={ph.columns[0]:'Global'})], axis=1)\n",
    "if which=='Percent per year':\n",
    "    gdp = gdp.mul(100)\n",
    "    sector = 'GDP change per year'\n",
    "    ylabel = 'GDP change per year (%)'\n",
    "else:\n",
    "    sector = 'GDP per capita'\n",
    "    ylabel = 'GDP (2018 USD) per capita'\n",
    "    gdp = gdp.rename(columns=dict(zip(gdp.columns, [i.split('.')[0] for i in gdp.columns])))\n",
    "    if split_on_china:\n",
    "        pop = pop.rename(columns=dict(zip(pop.columns, [i.split('.')[0] for i in pop.columns])))\n",
    "        gdp.loc[:,'RoW'] = gdp[non_china].sum(axis=1)\n",
    "        pop.loc[:,'RoW'] = pop[non_china].sum(axis=1)\n",
    "        gdp = gdp/pop/1000\n",
    "        gdp.drop(columns=non_china, inplace=True)\n",
    "    if norm:\n",
    "        gdp = gdp.apply(lambda x: x/gdp.loc[norm_year], axis=1)\n",
    "gdp = gdp.loc[:end_year]\n",
    "gdp.plot(ax=ax[-1], title=sector, ylabel=ylabel, xlabel='Year')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeaa37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "log = True\n",
    "static_lifetimes=pd.read_excel('generalization/input_files/static/Generalization drive file.xlsx',\n",
    "                               sheet_name='Static lifetimes', index_col=0,header=[0,1]).loc[2019].unstack()\n",
    "static_lifetimes.loc[:,'Static lifetime'] = static_lifetimes['Reserves']/static_lifetimes['Mine production']\n",
    "static_lifetime = static_lifetimes['Static lifetime']\n",
    "opening_probs = many_sg.rmse_df_sorted.loc[idx[:,'incentive_opening_probability'],\n",
    "                                           :25].astype(float).mean(axis=1).droplevel(1)\n",
    "price = many_sg.historical_data.loc[idx[:,2019],'Primary commodity price'].droplevel(1)\n",
    "both = pd.concat([static_lifetime, opening_probs, price],axis=1,\n",
    "                 keys=['Static lifetime','Opening probability','Price'])\n",
    "\n",
    "def plot_two_params_commodity(both, x, y, ax=0, log=True, **plt_kwargs):\n",
    "    if x=='Price':\n",
    "        x = np.log10(both[x])\n",
    "    else:\n",
    "        x = both[x]\n",
    "    if log:\n",
    "        y = np.log10(both[y].astype(float))\n",
    "    else:\n",
    "        y = both[y]\n",
    "    z = both.index\n",
    "    \n",
    "    colors = dict(zip(z,\n",
    "        list(sns.color_palette('deep',n_colors=9))))\n",
    "\n",
    "    if type(ax)==int:\n",
    "        fig,a = plt.subplots()\n",
    "    else:\n",
    "        a = ax\n",
    "    a.scatter(x,y,alpha=0)\n",
    "#     m = sm.GLS(y.astype(float), (x.astype(float))).fit(cov_type='HC3')\n",
    "#     a.plot(a.get_xlim(), [a.get_ylim()[0], a.get_xlim()[1]*m.coef[x.name]], color='k', linestyle='--')\n",
    "#     a.plot(a.get_xlim(), [a.get_ylim()[0], a.get_xlim()[1]*2.6], color='k', linestyle='--')\n",
    "#     a.plot(a.get_xlim(), a.get_xlim(), color='k', linestyle='--')\n",
    "    \n",
    "    for i,j,s in zip(x,y,z):\n",
    "        a.annotate(str(s),  xy=(i, j), color=colors[s],\n",
    "                    fontsize=\"large\", weight='heavy',\n",
    "                    horizontalalignment='center',\n",
    "                    verticalalignment='center')\n",
    "    initial_xlim, initial_ylim = a.get_xlim(), a.get_ylim()\n",
    "    offset_fraction = 10\n",
    "    offset_x = (initial_xlim[1]-initial_xlim[0])/offset_fraction\n",
    "    offset_y = (initial_ylim[1]-initial_ylim[0])/offset_fraction\n",
    "    a.set_xlim(initial_xlim[0]-offset_x, initial_xlim[1]+offset_x)\n",
    "    a.set_ylim(initial_ylim[0]-offset_y, initial_ylim[1]+offset_y)\n",
    "    a.set(**plt_kwargs)\n",
    "    if log and y.name=='Static lifetime':\n",
    "        yticks = a.get_yticks()\n",
    "        unlogged_ticks = np.arange(15,91,15)\n",
    "        a.set_yticks([np.log10(i) for i in unlogged_ticks])\n",
    "        a.set_yticklabels(unlogged_ticks)\n",
    "    if x.name=='Price':\n",
    "        logged_ticks = np.arange(3,9,1)\n",
    "        a.set_xticks(logged_ticks)\n",
    "        a.set_xticklabels([f'$10^{{{i}}}$' for i in logged_ticks])\n",
    "    if y.name=='Price':\n",
    "        logged_ticks = np.arange(3,9,1)\n",
    "        a.set_yticks(logged_ticks)\n",
    "        a.set_yticklabels([f'$10^{{{i}}}$' for i in logged_ticks])\n",
    "    \n",
    "    \n",
    "fig,ax = easy_subplots(3,3)\n",
    "plt_kwargs = {'title':'Static lifetime vs\\nfraction of viable mines that open', \n",
    "              'xlabel':'Fraction of viable mines that open', 'ylabel':'Static lifetime (years)'}\n",
    "plot_two_params_commodity(both, 'Opening probability', 'Static lifetime', ax[0], False, **plt_kwargs)\n",
    "\n",
    "plt_kwargs = {'title':'Price vs\\nfraction of viable mines that open', \n",
    "              'xlabel':'Fraction of viable mines that open', 'ylabel':'Price (USD/t)'}\n",
    "plot_two_params_commodity(both, 'Opening probability', 'Price', ax[1], **plt_kwargs)\n",
    "\n",
    "plt_kwargs = {'title':'Static lifetime vs\\nprice', \n",
    "              'xlabel':'Price (USD/t)', 'ylabel':'Static lifetime (years)'}\n",
    "plot_two_params_commodity(both, 'Price', 'Static lifetime', ax[2], False, **plt_kwargs)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1deb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_sg.historical_data.sort_index().loc[idx[['Al','Steel'],2001:2019],'Primary supply'].unstack(0).pct_change().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56040b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lifetime_over_prob = both['Static lifetime']*both['Opening probability']\n",
    "both['Static lifetime']['Ni']/both['Static lifetime']['Zn'], \\\n",
    "    both['Opening probability']['Zn']/both['Opening probability']['Ni']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0b5771",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "manies = [many_sg, many_17, many_16, many_15]\n",
    "many_names = ['2001-2019', '2001-2016', '2001-2015', '2001-2014']\n",
    "fig, ax=easy_subplots(manies, 2, use_subplots=True, sharey=True, height_scale=0.9)\n",
    "for many,a,name in zip(manies, ax, many_names):\n",
    "    cummin_score = many.rmse_df.loc[idx[:,'score'],:].cummin(axis=1).droplevel(1).T\n",
    "#     cummin_score = many.rmse_df.loc[idx[:,'score'],:].droplevel(1).T.rolling(20).mean()\n",
    "    ind = many.rmse_df.loc[idx[:,'score'],:].min(axis=1).droplevel(1).sort_values(ascending=False).index\n",
    "    cummin_score = cummin_score.loc[:,ind]\n",
    "    cummin_score.plot(ax=a, style=['-','--','-.',':','-','--','-.',':','-']).grid(axis='x')\n",
    "    # ax.legend(loc=(1.05,0))\n",
    "    a.legend(ncols=2)\n",
    "    a.set(title=f'Learning curves\\ntuning {name}', xlabel='Number of iterations', \n",
    "           ylabel='Cumulative min. training score')\n",
    "fig.tight_layout()\n",
    "fig.set_dpi(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9df15b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "years = np.arange(2001,2020)\n",
    "rmse_test = pd.DataFrame()\n",
    "for h,r in zip(['Total demand','Primary supply','Primary commodity price'],\n",
    "               ['Total demand','Mine production','Refined price']):\n",
    "    hist = many_16.historical_data[h].loc[idx[:,years]]\n",
    "    simu = many_16.results[r].loc[idx[:,:,years]]\n",
    "    rmse = simu.unstack().apply(lambda x: mean_squared_error(hist.loc[x.name[0]], x)**0.5\n",
    "                         /hist.loc[idx[x.name[0],2001]],axis=1)\n",
    "    rmse = pd.DataFrame(rmse)\n",
    "    rmse = rmse.rename(columns={rmse.columns[0]:r})\n",
    "    rmse_test = pd.concat([rmse_test,rmse],axis=1)\n",
    "    # getting the training scores to plot alongside the learning curves\n",
    "score_test = np.log(rmse_test.sum(axis=1)/3).unstack(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89f29c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_test.cummin().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddc64b",
   "metadata": {},
   "source": [
    "For use in Karan's decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8322546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3224d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bhu_param = pd.DataFrame([0.343, 0.17, 0.262, 0.362, 0.244, 0.73, 0.035, 0.338, 0.39],\n",
    "                             ['Al', 'Ag', 'Au', 'Cu', 'Ni', 'Pb', 'Sn', 'Steel', 'Zn'],\n",
    "                        ['Recycling input rate'])\n",
    "bhu_param['Mine cost change per year'] = means['Mining production']['Mine cost change per year']\n",
    "bhu_param['Fraction of viable mines that open'] = means[\n",
    "    'Reserve development']['Fraction of viable mines that open']\n",
    "\n",
    "bhu_param['Recycling input rate'] = pd.Series([0.343, 0.17, 0.262, 0.362, 0.244, 0.73, 0.035, 0.338, 0.39],\n",
    "                             ['Al', 'Ag', 'Au', 'Cu', 'Ni', 'Pb', 'Sn', 'Steel', 'Zn'])\n",
    "bhu_param['Intensity elasticity to price'] = means['Demand response']['Intensity elasticity to price']\n",
    "bhu_param['Mean mine production growth/year'] = many_sg.historical_data.sort_index().loc[\n",
    "    idx[:,2001:2019],'Primary supply'].groupby(level=0).pct_change().groupby(level=0).mean()\n",
    "# bhu_param['CAC slope eq.'] = means['Reserve development']['Incentive ore grade elasticity to COT']\n",
    "bhu_param_color = bhu_param.copy().subtract(bhu_param.min())\n",
    "bhu_param_color = bhu_param_color.div(bhu_param_color.max())\n",
    "fig,ax=plt.subplots(figsize=(12,4))\n",
    "\n",
    "sns.heatmap(bhu_param_color.T, annot=bhu_param.T, fmt='.2f',\n",
    "            cbar_kws={'label':'Low            Med           High','ticks':[]},)\n",
    "fig.set_dpi(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95536294",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_sg.rmse_df.unstack().stack(0).fillna(0).to_csv(\n",
    "    'generalization/output_files/tuned_rmse_df_out_split_grades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcae714",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_sg.results['Mean total minesite cost']\n",
    "x = np.array([[]])\n",
    "x = np.append(x,[[8,8,76]],axis=0)\n",
    "x = np.append(x,[[8,8,76]],axis=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82e0dd",
   "metadata": {},
   "source": [
    "**For putting all historical data in better format in one file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791eadb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# writer = pd.ExcelWriter('generalization/output_files/historical_data.xlsx', engine='xlsxwriter')\n",
    "\n",
    "\n",
    "\n",
    "# ele = 'Al'\n",
    "# for ele in many_sg.results.index.get_level_values(0).unique():\n",
    "#     comm = many_sg.element_commodity_map[ele]\n",
    "#     hist_data = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx', \n",
    "#                   sheet_name=ele, index_col=0)\n",
    "#     if 'Source(s)' in hist_data.index: \n",
    "#         hist_data.drop('Source(s)',inplace=True)\n",
    "#     hist_data = hist_data[['Total demand','Primary supply','Scrap demand']].sort_index()\n",
    "\n",
    "#     hist_prices = pd.read_csv('generalization/input_files/user_defined/price adjustment results.csv',index_col=0)\n",
    "#     hist_prices = hist_prices.loc[:,[i for i in hist_prices.columns if \n",
    "#                                      comm in i and 'diff' not in i]].sort_index()\n",
    "#     hist_prices = hist_prices.rename(columns={\n",
    "#         f'log({comm})':'Refined price, inflation, oil price adjusted', \n",
    "#         f'{comm} original':'Refined price, unadjusted'}).dropna(how='all')\n",
    "#     hist_prices['Refined price, inflation, oil price adjusted, rolling mean'] = hist_prices[\n",
    "#         'Refined price, inflation, oil price adjusted'].rolling(5,min_periods=1, center=True).mean()\n",
    "#     out_data = pd.concat([hist_data,hist_prices],axis=1).sort_index().dropna(how='all')\n",
    "#     out_data.to_excel(writer, sheet_name=ele)\n",
    "\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8424848",
   "metadata": {},
   "source": [
    "# Trying to get USGS data - just ask for it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5225046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "import io\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urlpdf = 'https://pubs.usgs.gov/periodicals/mcs2021/mcs2021-copper.pdf' \n",
    "response = requests.get(urlpdf)\n",
    "with io.BytesIO(response.content) as f:\n",
    "    pdf = PdfReader(f)\n",
    "    information = pdf.metadata\n",
    "    number_of_pages = len(pdf.pages)\n",
    "    txt = f\"\"\"\n",
    "    Author: {information.author}\n",
    "    Creator: {information.creator}\n",
    "    Producer: {information.producer}\n",
    "    Subject: {information.subject}\n",
    "    Title: {information.title}\n",
    "    Number of pages: {number_of_pages}\n",
    "    \"\"\"\n",
    "    # Here the metadata of your pdf\n",
    "    # numpage for the number page\n",
    "    numpage=2\n",
    "    page = pdf.pages[numpage-1]\n",
    "    page_content = page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25c8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content.split('\\nWorld t otal (rounded)')[1].split('\\nWorld')[0]\n",
    "page_content#.split('\\nUnited  States')[0].split('\\n \\n')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4313ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "copper = 'https://www.usgs.gov/centers/national-minerals-information-center/copper-statistics-and-information'\n",
    "url=requests.get(copper)\n",
    "soup = BeautifulSoup(url.content,\"lxml\")\n",
    "\n",
    "for a in soup.find_all('a', href=True):\n",
    "    mystr= a['href']\n",
    "    if mystr[-4:]=='.pdf' and 'mcs' in mystr:\n",
    "        print(mystr)\n",
    "        response = requests.get(mystr)\n",
    "        with io.BytesIO(response.content) as f:\n",
    "            pdf = PdfReader(f)\n",
    "            information = pdf.metadata\n",
    "            number_of_pages = len(pdf.pages)\n",
    "            txt = f\"\"\"\n",
    "            Author: {information.author}\n",
    "            Creator: {information.creator}\n",
    "            Producer: {information.producer}\n",
    "            Subject: {information.subject}\n",
    "            Title: {information.title}\n",
    "            Number of pages: {number_of_pages}\n",
    "            \"\"\"\n",
    "            # Here the metadata of your pdf\n",
    "            # numpage for the number page\n",
    "            numpage=2\n",
    "            page = pdf.pages[numpage-1]\n",
    "            page_content = page.extract_text()\n",
    "            \n",
    "            if '\\nWorld total' in page_content:\n",
    "                print(page_content.split('\\nWorld total (rounded)')[1].split('\\nWorld')[0])\n",
    "            elif '\\n World total' in page_content:\n",
    "                print(page_content.split('\\n World total (rounded)')[1].split('\\nWorld')[0])\n",
    "            elif 'World t otal' in page_content:\n",
    "                print(page_content.split('\\nWorld t otal (rounded)')[1].split('\\nWorld')[0])\n",
    "            else:\n",
    "                print('no world total')\n",
    "                \n",
    "            if '\\nUnited States' in page_content:\n",
    "                cols = page_content.split('\\nUnited States')[0]\n",
    "                if '\\n \\n' in cols:\n",
    "                    cols = cols.split('\\n \\n')[1]\n",
    "                elif 'World Mine' in cols:\n",
    "                    cols = cols.split('World Mine')[1]\n",
    "                print(cols)\n",
    "            elif '\\nUnited  States' in page_content:\n",
    "                print(page_content.split('\\nUnited  States')[0].split('\\n \\n')[1])\n",
    "            else:\n",
    "                print('no cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16f53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_sg.historical_data.loc['Al']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14874c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f9ee6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "copper = 'https://www.usgs.gov/centers/national-minerals-information-center/copper-statistics-and-information'\n",
    "url=requests.get(copper)\n",
    "soup = BeautifulSoup(url.content,\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47989a0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import camelot\n",
    "links = soup.findAll('a')\n",
    "for a in links:\n",
    "    link = a.get('href')\n",
    "    title = a.get('title')\n",
    "    if link is not None and 'pdf' in link:\n",
    "        title = str(a).split('>')[-2].split('<')[0]\n",
    "        year_strings = [str(i) for i in np.arange(1950,2050)]\n",
    "        if title in year_strings:\n",
    "            print(title, link)\n",
    "            import camelot\n",
    " \n",
    "            # extract all the tables in the PDF file\n",
    "            abc = camelot.read_pdf('link')   #address of file location\n",
    "\n",
    "            # print the first table as Pandas DataFrame\n",
    "            print(abc[0].df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d654c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0d5d89",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23addda6",
   "metadata": {
    "code_folding": [
     3
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "folder_name = '2023-01-18 17_04_44_5_run_hist_all_split_grades'\n",
    "folder_path = 'generalization/output_files/Historical tuning/'+folder_name\n",
    "\n",
    "# this class is already saved elsewhere\n",
    "class LoadFolderContents:\n",
    "    def __init__(self, folder_path):\n",
    "        if not os.path.exists(folder_path):\n",
    "            potential_paths = ['generalization/output_files/Historical tuning',\n",
    "                               'generalization/output_files/Simulation',\n",
    "                               'output_files/Historical tuning',\n",
    "                               'output_files/Simulation',\n",
    "                              ]\n",
    "            for path in potential_paths:\n",
    "                if os.path.exists(path):\n",
    "                    if folder_path in os.listdir(path):\n",
    "                        self.folder_path = f'{path}/{folder_path}'\n",
    "        else:\n",
    "            self.folder_path = folder_path\n",
    "            \n",
    "        if not hasattr(self,'folder_path'):\n",
    "            raise ValueError('The folder path given has to give the relative or absolute path to your folder, '\n",
    "                             'including the folder name, or give the folder name and we will check for it '\n",
    "                             'inside the following paths:\\n'+ str(potential_paths+[os.getcwd()])\n",
    "                            )\n",
    "            \n",
    "    def load_scenario_data(self):\n",
    "        folder_path = self.folder_path\n",
    "        if os.path.exists(folder_path):\n",
    "            subfolders = [f'{self.folder_path}/{i}' for i in os.listdir(folder_path) if i != 'input_files' and '.' not in i]\n",
    "            self.subfolders = subfolders\n",
    "            commodities = [i.split('_')[-1] for i in subfolders]\n",
    "            for subfolder in subfolders:\n",
    "                commodity = subfolder.split('_')[-1]\n",
    "                for file in os.listdir(subfolder):\n",
    "                    if file[-4:] == '.csv':\n",
    "                        df_name = file.split('.csv')[0]\n",
    "                        file = f'{subfolder}/{file}'\n",
    "                        var_name = df_name+'_ph'\n",
    "                        if 'historical_data' in file:\n",
    "                            variable = pd.read_csv(file, index_col=[0])\n",
    "                        else:\n",
    "                            variable = pd.read_csv(file, index_col=[0,1])\n",
    "                            if 'rmse_df' in file:\n",
    "                                variable = variable.iloc[:,0].unstack(0)\n",
    "                        setattr(self, var_name, variable)\n",
    "                        \n",
    "                        if not hasattr(self, df_name):\n",
    "                            setattr(self, df_name, pd.DataFrame())\n",
    "                        so_far = getattr(self, df_name)\n",
    "                        to_add = pd.concat([variable], keys=[commodity])\n",
    "                        new = pd.concat([so_far, to_add])\n",
    "                        setattr(self, df_name, new)\n",
    "    \n",
    "    def get_sorted_dataframes(self):\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            if not hasattr(self,'rmse_df'):\n",
    "                return None\n",
    "            self.rmse_df_sorted = pd.DataFrame()\n",
    "            self.hyperparam_sorted = pd.DataFrame()\n",
    "            self.results_sorted = pd.DataFrame()\n",
    "            for commodity in self.rmse_df.index.get_level_values(0).unique():\n",
    "                rmse = self.rmse_df.loc[commodity].copy()\n",
    "                if 'score' not in rmse.index:\n",
    "                    return None\n",
    "                sorted_index = rmse.sort_values(by='score',axis=1).columns\n",
    "                rmse_sorted = pd.concat([rmse.loc[:,sorted_index].T.reset_index(drop=True).T], keys=[commodity])\n",
    "                self.rmse_df_sorted = pd.concat([self.rmse_df_sorted, rmse_sorted])\n",
    "                for df_name in ['hyperparam', 'results']:\n",
    "                    df = getattr(self, df_name).loc[commodity]\n",
    "                    df = df.loc[idx[sorted_index,:],:]\n",
    "                    ind = df.copy().index\n",
    "                    level_0 = df.index.get_level_values(0).unique().sort_values()\n",
    "                    level_1 = df.index.get_level_values(1).unique()\n",
    "                    df = df.rename(dict(zip(df.index.get_level_values(0).unique(), \n",
    "                                            np.arange(0,len(df.index.get_level_values(0).unique())))), level=0)\n",
    "                    previous = getattr(self, df_name+'_sorted')\n",
    "                    df = pd.concat([df],keys=[commodity])\n",
    "                    setattr(self, df_name+'_sorted', pd.concat([previous, df]))\n",
    "                    \n",
    "lod = LoadFolderContents(folder_name)\n",
    "lod.load_scenario_data()\n",
    "lod.get_sorted_dataframes()\n",
    "\n",
    "folder_name = '2023-01-18 17_50_28_6_baselines'\n",
    "lod_fut = LoadFolderContents(folder_name)\n",
    "lod_fut.load_scenario_data()\n",
    "lod_fut.get_sorted_dataframes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bdd270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7830bad",
   "metadata": {},
   "source": [
    "# Extracting Dahl Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "meds = pd.read_excel('MEDS.xlsx',skiprows=7,sheet_name='MEDS').dropna(how='all').dropna(how='all',axis=1)\n",
    "commodity_strings = [many_sg.element_commodity_map[i] for\n",
    "                     i in many_sg.results.index.get_level_values(0).unique()]+['Iron','Nichel','Bauxite','Lithium']\n",
    "meds = meds.loc[[np.any([j.lower() in str(i).lower() for j in commodity_strings]) and\n",
    "               'cobalt' not in str(i).lower() and 'platinum' not in str(i).lower()\n",
    "                 for i in meds['Product']]].replace(' Copper/','Copper/')\\\n",
    "                .replace('Non-tinplate uses','Tin_non-tinplate uses')\\\n",
    "                .replace('Tinplate uses','Tin_tinplate uses').replace('NichelΔ','NickelΔ').replace('tin','Tin')\\\n",
    "                .replace('Refined Copper','Copper-refined')\n",
    "meds['Commodity'] = [i.split(' ')[0].split('_')[0].split('/')[0].split('Δ')[0].split('-')[0]\n",
    "                     for i in meds['Product']]\n",
    "meds['Element'] = [many_sg.commodity_element_map[\n",
    "    i.replace('Iron','Steel').replace('Bauxite','Aluminum')] for i in meds['Commodity']]\n",
    "meds['Modifier'] = [' '.join(i.split(' ')[1:]) if ' ' in i else ' '.join(i.split('-')[1:]) \n",
    "                    if '-' in i else ' '.join(i.split('_')[1:]) if '_' in\n",
    "                    i else ''\n",
    "                     for i in meds['Product']]\n",
    "# meds = meds.loc[[str(i)[0]>'H' for i in meds['Ref']]]\n",
    "\n",
    "orig_columns = ['Parameter','Publication','In Dahl', 'Method',\n",
    "                                  'Steel','Al','Cu','Au','Pb','Li','Ni','Ag','Sn','Zn',\n",
    "                                  'Degrees of Freedom','Years','Notes']\n",
    "styled_df = pd.DataFrame(columns=orig_columns)\n",
    "\n",
    "# meds = meds.sort_values(by=)\n",
    "s = 0\n",
    "for i in meds.index[:]:\n",
    "    ph = meds.T[i]\n",
    "    parameter = 'Demand elasticity to price'\n",
    "    income = 'Demand elasticity to GDP'\n",
    "    if np.any([j in ph['Modifier'] for j in ['Ore','mined','Primary','Mine Capac']]):\n",
    "        parameter = 'Primary supply elasticity to price'\n",
    "        income = 'Primary supply elasticity to GDP'\n",
    "    elif np.any([j in ph['Modifier'] for j in ['secondary','Recovery']]):\n",
    "        parameter = 'Collection elasticity to scrap price'\n",
    "        income = 'Secondary elasticity to GDP'\n",
    "    elif ph['Modifier'] == 'Aluminum':\n",
    "        parameter = None\n",
    "        income = 'aluminum'\n",
    "    elif ph['S/D']=='S':\n",
    "        parameter = 'Primary supply elasticity to price'\n",
    "        income = 'Supply elasticity to GDP'\n",
    "    elif ph['S/D']=='SCap':\n",
    "        parameter = 'Primary supply elasticity to price'\n",
    "        income = 'scap'\n",
    "    \n",
    "    if parameter is not None:\n",
    "        if (ph['t(p),Pv,sg']==np.nan or type(ph['t(p),Pv,sg'])==str or \n",
    "                                  abs(ph['t(p),Pv,sg'])>1.96):#getting stat sig\n",
    "            for k,ind,abc in zip(ph[['P sr','P lr','Pstat']].dropna(),\n",
    "                          ph[['P sr','P lr','Pstat']].dropna().index, ['a','b','c']):\n",
    "                if k not in ['[+]','[-]','NI']:\n",
    "                    mod = ph['Modifier']\n",
    "                    reg = ph['Country/Region']\n",
    "                    comments = ph['Comments']\n",
    "                    styled_df.loc[s,'Notes'] = f' {mod}, Region: {reg}; Comments from Dahl: {comments}'\n",
    "                    styled_df.loc[s, 'Value'] = k\n",
    "                    styled_df.loc[s, 'Publication'] = ph['Ref']\n",
    "                    styled_df.loc[s, 'Element'] = ph['Element']\n",
    "                    styled_df.loc[s, 'Parameter'] = parameter\n",
    "                    styled_df.loc[s, 'Years'] = '-'.join((str(ph['yr1']),str(ph['yr2'])))\n",
    "                    styled_df.loc[s, 'Method'] = 'Unknown, '+('short run' if ind=='P sr' else \n",
    "                                                              'long run' if ind=='P lr' else 'static')\n",
    "                    s+=1\n",
    "    if parameter is not None:\n",
    "        if (ph['t(Y),Pv,sg']==np.nan or type(ph['t(Y),Pv,sg'])==str or \n",
    "                                  abs(ph['t(Y),Pv,sg'])>1.96):#getting stat sig\n",
    "            for k,ind,abc in zip(ph[['Y sr','Y lr','Ystat']].dropna(),\n",
    "                          ph[['Y sr','Y lr','Ystat']].dropna().index, ['a','b','c']):\n",
    "                if k not in ['[+]','[-]','NI','NR']:\n",
    "                    mod = ph['Modifier']\n",
    "                    reg = ph['Country/Region']\n",
    "                    comments = ph['Comments']\n",
    "                    styled_df.loc[s,'Notes'] = f' {mod}, Region: {reg}; Comments from Dahl: {comments}'\n",
    "                    styled_df.loc[s, 'Value'] = k\n",
    "                    styled_df.loc[s, 'Publication'] = ph['Ref']\n",
    "                    styled_df.loc[s, 'Element'] = ph['Element']\n",
    "                    styled_df.loc[s, 'Parameter'] = income\n",
    "                    styled_df.loc[s, 'Years'] = '-'.join((str(ph['yr1']),str(ph['yr2'])))\n",
    "                    styled_df.loc[s, 'Method'] = 'Unknown, '+('short run' if ind=='P sr' else \n",
    "                                                              'long run' if ind=='P lr' else 'static')\n",
    "                    s+=1\n",
    "styled_df['In Dahl'] = 'From Dahl'\n",
    "styled_df.drop(columns=['Steel','Al','Cu','Au','Pb','Li','Ni','Ag','Sn','Zn',],inplace=True)\n",
    "\n",
    "reference_rename = {'B&H00': 'Blomberg (2000)',\n",
    "    'Ban72': 'Banks (1972)',\n",
    "    'Ban74': 'Banks (1974)',\n",
    "    'Beh75qF&S18': 'Behrman (1975), in Fally (2018)',\n",
    "    'C&P75': 'Connelly (1975)',\n",
    "    'CGP79qF&S18': 'Chhabra (1979)',\n",
    "    'CGP78': 'Chhabra (1978)',\n",
    "    'Cho90': 'Choe (1990)',\n",
    "    'Con91': 'Considine (1991)',\n",
    "    'CRA70qB&H79': 'Charles River Associates (1970), in Bozdogan (1979)',\n",
    "    'CRA70qMik79': 'Charles River Associates (1970), in Bozdogan (1979)',\n",
    "    'CRA71qV&T74': 'Charles River Associates (1971), in Bozdogan (1979)',\n",
    "    'Dam79qDam11': 'Damuth (1979), in Damuth (2011)',\n",
    "    'Dam11': 'Damuth (2011)',\n",
    "    'E&L02': 'Evans (2002)',\n",
    "    'F&C81': 'Foley (1981)',\n",
    "    'F&O81': 'Fisher (1981)',\n",
    "    'FCB72': 'Fisher (1972)',\n",
    "    'Fer18': 'Fernandez (2018)',\n",
    "    'Gup79': 'Gupta (1979)',\n",
    "    'Gup82=Gup79': 'Gupta (1982)',\n",
    "    'H&S81': 'Hashimoto (1981)',\n",
    "    'H&Z80': 'Hu (1980)',\n",
    "    'Hoj81': 'Hojman (1981)',\n",
    "    'Lit76qB&H79': 'Bozdogan (1979)',\n",
    "    'M&O80': 'MacKinnon (19980)',\n",
    "    'Mar83': 'Marsh (1983)',\n",
    "    'Mcn73qMcn75&B&H79': 'McNicol (1975), in McNicol (1975) and Bozdogan (1979)',\n",
    "    'P&T99qPei96': 'Pei (1999)',\n",
    "    'Pob80qWor81': 'Pobukadee (1979)',\n",
    "    'Pri87': 'Priovolos (1987)',\n",
    "    'Raf85b': 'Rafati (1985)',\n",
    "    'She86': 'Sherman (1986)',\n",
    "    'Sig95': 'Sigman (1995)',\n",
    "    'Stu17a=Stu17b': 'Stuermer (2017, Dallas Fed)',\n",
    "    'Stu17a': 'Stuermer (2017, Journal of International Money and Finance)',\n",
    "    'T&S77qMik79': 'Tims (1977)',\n",
    "    'T&T02': 'Tcha (2002)',\n",
    "    'Tay79': 'Taylor (1979)',\n",
    "    'Wag85=Wag84': 'Wagenhals (1984)'}\n",
    "styled_df = styled_df.replace(reference_rename)\n",
    "styled_df = styled_df.replace({'0. 216 ':0.216,\n",
    "                               'l.031 ':1.031, \n",
    "                               'l.026':1.026, \n",
    "                               'l.040':1.04, \n",
    "                               '-0 . 003':-0.003, \n",
    "                               '_':np.nan, \n",
    "                               '–':np.nan, \n",
    "                               '….':np.nan})\n",
    "styled_df = styled_df.loc[abs(styled_df['Value'])<3].reset_index(drop=True)\n",
    "\n",
    "pp = styled_df.set_index(['Parameter','Publication','Method','Years','Notes','Element'])#['Value'].unstack()\n",
    "pp['ind'] = np.arange(0,pp.shape[0])\n",
    "for i in pp.index[pp.index.duplicated()].unique():\n",
    "    dup = pp.loc[i]\n",
    "    for e,i in enumerate(dup['ind']):\n",
    "        styled_df.loc[i, 'Publication'] = styled_df['Publication'][i]+', '+string.printable[10+e]\n",
    "pp = styled_df.set_index(['Parameter','Publication','Method','Years','Notes','Element','In Dahl',\n",
    "                          'Degrees of Freedom'])#['Value'].unstack()\n",
    "\n",
    "pp2 = pp['Value'].unstack('Element').reset_index()\n",
    "pp2[[i for i in orig_columns if i in pp2.columns]].to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d03764",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in styled_df['Value']:\n",
    "    try:\n",
    "        float(i)\n",
    "    except:\n",
    "        print('='+i+'=')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc66fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "styled_df['Parameter'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5222cf8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de00165f",
   "metadata": {},
   "source": [
    "# Criticality - future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14829e01",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "commodities = many_b.results.index.get_level_values(0).unique()\n",
    "commodity = 'aluminum'\n",
    "\n",
    "def plot_future_line_and_hist_one_commodity(many, commodity, parameter, restrict=0.95, \n",
    "                                            use_sns=False, plot_historical=False, color='tab:blue', \n",
    "                                            restrict_2019=None,\n",
    "                                            fig=None, ax=None, dpi=50):\n",
    "    \"\"\"\n",
    "    Plots the combined set of transparent lines for historical-future time\n",
    "    progression of the given parameter and commodity, with a histogram aligning\n",
    "    with the y axis that shows the 2040 value distribution.\n",
    "\n",
    "    --------\n",
    "    many: Many() object, must have multi_scenario_results object\n",
    "    commodity: str, lowercase commodity form\n",
    "    parameter: str, any column in many.multi_scenario_results\n",
    "    restrict: float or False, whether to restrict the y-axis limits to exclude\n",
    "        outliers, with float values corresponding to the percentile being\n",
    "        plotted (e.g. 0.95 causes the middle 95% of max/min values to be shown)\n",
    "    use_sns: bool, no functionality yet but figured it would be to have the\n",
    "        sns.lineplot shading functionality\n",
    "    plot_historical: bool, whether to show the historical values alongside the\n",
    "        simulated\n",
    "    color: str, matplotlib color for the lines and histogram\n",
    "    restrict_2019: float, if non-None it is the offset from the 2019 value permitted for\n",
    "        simulated 2019 mine production permitted (e.g. +/-20% for 0.2), which is used for\n",
    "        any parameter given\n",
    "    fig: figure on which to plot\n",
    "    ax: axes on which to plot, must be a list of length 2, as the histogram\n",
    "        plots on ax[1]\n",
    "    dpi: int, dots per square inch, controls figure resolution\n",
    "    ----------\n",
    "\n",
    "    Returns: comm (the data being plotted), fig\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig,ax=easy_subplots(2,2,width_ratios=[4,1], sharey=False, width_scale=0.5)\n",
    "    comm = many.results.loc[commodity][parameter]\n",
    "    comm = comm.unstack().T.loc[2001:]\n",
    "    hist_data_dict = dict(zip(many.objective_results_map.values(), many.objective_results_map.keys()))\n",
    "    if parameter in hist_data_dict and plot_historical:\n",
    "        hist = many.historical_data.loc[commodity][hist_data_dict[parameter]].loc[2001:2019]\n",
    "        dicty = get_unit(comm, hist, parameter)\n",
    "        comm = dicty['simulated']\n",
    "        hist = dicty['historical']\n",
    "        unit = dicty['unit']\n",
    "        if restrict_2019 is not None:\n",
    "            p = 'Mine production'\n",
    "            v19 = many.results[p].loc[idx[commodity,:,2019]]\n",
    "            hist19 = many.historical_data.loc[idx[commodity,2019],hist_data_dict[p]]\n",
    "            off = restrict_2019\n",
    "            v19 = v19[(v19>(1-off)*hist19) & (v19<(1+off)*hist19)]\n",
    "            ind = v19.index\n",
    "            comm = comm.loc[:,ind]\n",
    "            print(len(ind))\n",
    "    else:\n",
    "        dicty = get_unit(comm, comm, parameter)\n",
    "        comm = dicty['simulated']\n",
    "        unit = dicty['unit']\n",
    "\n",
    "    comm.plot(legend=False, alpha=0.05, linewidth=1, color=color,\n",
    "              ylabel=f'{parameter} ({unit})',\n",
    "              xlabel='Year',\n",
    "              ax=ax[0],\n",
    "             ).grid(axis='x')\n",
    "\n",
    "    if parameter in hist_data_dict and plot_historical:\n",
    "        hist.plot(\n",
    "                color='k',\n",
    "                alpha=0.9,\n",
    "                ax=ax[0],\n",
    "                zorder=5,\n",
    "            ).grid(axis='x')\n",
    "    if restrict:\n",
    "        restrict = 1-(1-restrict)/2\n",
    "        booly = (comm.min()>-comm.min().quantile(1-restrict)) &\\\n",
    "                (comm.max()<comm.max().quantile(restrict))\n",
    "        restricted = booly[booly].index\n",
    "    else: restricted = comm.columns\n",
    "    comm.loc[2040][restricted].plot.hist(orientation='horizontal', ax=ax[1],\n",
    "                             color=color\n",
    "                            ).grid(axis='x')\n",
    "    if restrict:\n",
    "        ax[0].set(ylim=(-comm.min().quantile(1-restrict), comm.max().quantile(restrict)))\n",
    "        ax[1].set(ylim=(-comm.min().quantile(1-restrict), comm.max().quantile(restrict)))\n",
    "\n",
    "    ax[1].set(xlabel='2040', yticklabels=[])\n",
    "    xx = 0.55 if parameter=='Mine production' else 0.58 if parameter=='Refined price' else 0.56\n",
    "    fig.suptitle(f'{parameter}, {commodity.capitalize()}',y=0.9, x=xx, weight='bold')\n",
    "    fig.tight_layout(rect=(0,0,1,1))\n",
    "    fig.set_dpi(dpi)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return comm, fig\n",
    "\n",
    "\n",
    "parameter = 'Mine production'\n",
    "# parameter = 'Refined price'\n",
    "# parameter = 'Total demand'\n",
    "\n",
    "# for commodity in commodities:\n",
    "#     comm, fig = plot_future_line_and_hist_one_commodity(many_b, commodity, parameter, restrict=0.95,\n",
    "#                                                        plot_historical=True)\n",
    "comm, fig = plot_future_line_and_hist_one_commodity(many_b, 'Cu', parameter, restrict=0.8,color='tab:blue',\n",
    "                                                       plot_historical=True, dpi=200, restrict_2019=0.4)\n",
    "lab = parameter.split(' ')[1]\n",
    "fig.savefig(f'/Users/johnryter/Pictures/cu-fig-{lab}.png', transparent=False, dpi=200)\n",
    "# comm, fig = plot_future_line_and_hist_one_commodity(many_b, 'silver', parameter, restrict=0.95,color='seagreen',\n",
    "#                                                        plot_historical=True,dpi=50)\n",
    "\n",
    "# rscale=[4,1]\n",
    "# fig,ax = easy_subplots(len(commodities)*2, 6, \n",
    "#                        width_ratios = list(np.tile(rscale,3)), width_scale=0.6,\n",
    "#                       )\n",
    "# for e,commodity in enumerate(commodities):\n",
    "#     a = ax[2*e:2*e+2]\n",
    "#     plot_future_line_and_hist_one_commodity(many_b, commodity, parameter, fig=fig, ax=a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97970cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a46a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "v19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5433f8c2",
   "metadata": {},
   "source": [
    "# Byproduct trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2b1eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "[4.5,9,18,5, 7,10,23.8,3.9,4]\n",
    "# artisanal =14.5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d64c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mine = miningModel(byproduct=True)\n",
    "\n",
    "update_hyperparam = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx',\n",
    "                                   index_col=0)['Co'].dropna()\n",
    "for i in np.intersect1d(list(mine.hyperparam.keys()), update_hyperparam.index):\n",
    "    mine.hyperparam[i] = update_hyperparam[i]\n",
    "mine.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed5639",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.historical_data\n",
    "    warnings.simplefilter('error')\n",
    "    m = miningModel(simulation_time=np.arange(1970,2051), byproduct=True,verbosity=4)\n",
    "    m.hyperparam\n",
    "    m.hyperparam.loc['minesite_cost_response_to_grade_price','Value']=True\n",
    "    m.hyperparam.loc['primary_commodity_price','Value'] = 4000\n",
    "    m.hyperparam.loc['primary_ore_grade_mean','Value'] = 0.1\n",
    "    m.hyperparam.loc['calibrate_incentive_opening_method','Value'] = False\n",
    "    m.hyperparam.loc['incentive_opening_method','Value'] = 'unconstrained'\n",
    "    m.hyperparam.loc['tcrc_sd_elas','Value'] = 0.2\n",
    "    m.hyperparam.loc['demand_series_pct_change',:] = 1\n",
    "    m.hyperparam.loc['incentive_tune_tcrc'] = True\n",
    "    m.hyperparam.loc['simulate_opening','Value']=False\n",
    "    m.hyperparam.loc['annual_reserves_ratio_with_initial_production','Value'] = 1.1\n",
    "    m.hyperparam.loc['discount_rate','Value'] = 0.1\n",
    "    m.hyperparam.loc['ramp_up_cu','Value'] = 0.7\n",
    "    m.hyperparam.loc['incentive_discount_rate','Value'] = 0.15\n",
    "    m.hyperparam.loc['incentive_opening_probability','Value'] = 0\n",
    "    m.hyperparam.loc['use_ml_to_accelerate','Value'] = True\n",
    "    m.hyperparam.loc['ml_accelerate_initialize_years'] = 15\n",
    "    m.hyperparam.loc['ml_accelerate_every_n_years'] = 1001\n",
    "    m.incentive_tuning_option = 'pid-2000' # pid or elas, or pid-int, where int is the year it switches to elas\n",
    "    \n",
    "    m.simulate_history_bool = False\n",
    "    m.op_initialize_prices()\n",
    "    for i in m.simulation_time:\n",
    "        m.i = i\n",
    "        print(i)\n",
    "        m.simulate_mine_life_one_year()\n",
    "        print('-'*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ce23d",
   "metadata": {
    "code_folding": [
     12,
     14,
     48,
     103,
     280,
     402,
     421,
     481,
     597,
     631,
     639,
     709,
     723,
     746,
     777,
     888,
     920,
     938,
     965,
     991,
     1002,
     1201
    ]
   },
   "outputs": [],
   "source": [
    "from modules.mining_class import *\n",
    "from modules.demand_class import *\n",
    "from modules.refining_class import *\n",
    "from modules.scenario_parser import *\n",
    "import numpy as np\n",
    "from warnings import warn\n",
    "import warnings\n",
    "\n",
    "\n",
    "# warnings.filterwarnings('error')\n",
    "# np.seterr(all='raise')\n",
    "\n",
    "class Integration():\n",
    "    '''\n",
    "    scenario_name takes the form 00_11_22_33_44\n",
    "        where:\n",
    "        00: ss, sd, bo (scrap supply, scrap demand, both).\n",
    "         Can also be sd-alt, which uses an alternative\n",
    "         implementation of the scrap demand increase (see below)\n",
    "        11: pr or no (price response included or no)\n",
    "        22: Xyr, where X is any integer and represents\n",
    "         the number of years the increase occurs\n",
    "        33: X%tot, where X is any float/int and is the\n",
    "         increase/decrease in scrap supply or demand\n",
    "         relative to the initial year total demand\n",
    "        44: X%inc, where X is any float/int and is the\n",
    "         increase/decrease in the %tot value per year\n",
    "\n",
    "        e.g. ss_pr_1yr_1%tot_0%inc\n",
    "\n",
    "        for 22-44, an additional X should be placed at\n",
    "         the end when 00==both, describing the sd values\n",
    "         e.g. ss_pr_1yr1_1%tot1_0%inc0\n",
    "\n",
    "        Can also have 11 as nono, prno, nopr, or prpr to\n",
    "         control the ss and sd price response individually\n",
    "         (in that order)\n",
    "\n",
    "        for sd-alt, the default (no alt) formulation is that an\n",
    "        increase in scrap demand to 5% of demand over 2\n",
    "        years would be\n",
    "        [0, 2.5%, 2.5%, 0%, 0%, ..., 0%]\n",
    "        while for alt it would be\n",
    "        [0, 2.5%, 5%,   5%, 5%, ..., 5%]\n",
    "        Can ctrl+F `direct_melt_duration` or `secondary_refined_duration`\n",
    "        to see the actual methods\n",
    "    '''\n",
    "\n",
    "    def __init__(self, static_data_folder=None, user_data_folder=None, simulation_time=np.arange(2019, 2041), verbosity=0, byproduct=False,\n",
    "                 input_hyperparam=0, scenario_name='', commodity=None, price_to_use=None,\n",
    "                 historical_price_rolling_window=1, force_integration_historical_price=False,\n",
    "                 use_historical_price_for_mine_initialization=True):\n",
    "        self.version = '1.0'  # str(datetime.now())[:19]\n",
    "\n",
    "        self.price_to_use = 'log' if price_to_use == None else price_to_use\n",
    "        self.element_commodity_map = {'Al': 'Aluminum', 'Au': 'Gold', 'Cu': 'Copper', 'Steel': 'Steel', 'Co': 'Cobalt',\n",
    "                                      'REEs': 'REEs', 'W': 'Tungsten', 'Sn': 'Tin', 'Ta': 'Tantalum', 'Ni': 'Nickel',\n",
    "                                      'Ag': 'Silver', 'Zn': 'Zinc', 'Pb': 'Lead', 'Mo': 'Molybdenum', 'Pt': 'Platinum',\n",
    "                                      'Te': 'Telllurium', 'Li': 'Lithium'}\n",
    "        self.i = simulation_time[0]\n",
    "        self.commodity = commodity\n",
    "        self.user_data_folder = 'input_files/user_defined' if user_data_folder is None else user_data_folder\n",
    "        self.static_data_folder = 'input_files/static' if user_data_folder is None else static_data_folder\n",
    "        self.simulation_time = simulation_time\n",
    "        self.verbosity = verbosity\n",
    "        self.byproduct = byproduct\n",
    "        self.input_hyperparam = input_hyperparam\n",
    "        self.scenario_name = scenario_name\n",
    "        self.historical_price_rolling_window = historical_price_rolling_window\n",
    "        self.force_integration_historical_price = force_integration_historical_price\n",
    "        self.use_historical_price_for_mine_initialization = use_historical_price_for_mine_initialization\n",
    "\n",
    "        self.demand = demandModel(user_data_folder=self.user_data_folder, static_data_folder=self.static_data_folder,\n",
    "                                  simulation_time=simulation_time, verbosity=verbosity)\n",
    "        self.refine = refiningModel(simulation_time=simulation_time, verbosity=verbosity)\n",
    "        self.initialize_hyperparam()\n",
    "        self.hyperparam.loc['simulation_time', :] = np.array(\n",
    "            [simulation_time, 'simulation time from model initialization'], dtype=object)\n",
    "        self.update_hyperparam()\n",
    "\n",
    "        price_simulation_time = np.arange(simulation_time[0] - 15, simulation_time[-1] + 1)\n",
    "        self.concentrate_supply = pd.Series(np.nan, price_simulation_time)\n",
    "        self.sxew_supply = pd.Series(np.nan, price_simulation_time)\n",
    "        self.concentrate_demand = pd.DataFrame(np.nan, price_simulation_time, ['Global', 'China', 'RoW'])\n",
    "        self.refined_supply = self.concentrate_demand.copy()\n",
    "        self.refined_demand = self.concentrate_demand.copy()\n",
    "        self.scrap_supply = self.concentrate_demand.copy()\n",
    "        self.scrap_demand = self.concentrate_demand.copy()\n",
    "        self.direct_melt_demand = self.concentrate_demand.copy()\n",
    "        self.direct_melt_fraction = self.concentrate_demand.copy()\n",
    "        self.total_demand = self.concentrate_demand.copy()\n",
    "        self.primary_supply = self.concentrate_demand.copy()\n",
    "        self.primary_demand = self.concentrate_demand.copy()\n",
    "        self.secondary_supply = self.concentrate_demand.copy()\n",
    "        self.secondary_demand = self.concentrate_demand.copy()\n",
    "\n",
    "        self.scrap_spread = self.concentrate_demand.copy()\n",
    "        self.primary_commodity_price = self.concentrate_supply.copy()\n",
    "        self.tcrc = self.concentrate_supply.copy()\n",
    "\n",
    "        self.scrap_shock_start = 2019\n",
    "        self.decode_scenario_name()\n",
    "\n",
    "    def load_historical_data__(self):\n",
    "        if not hasattr(self, 'historical_data'):\n",
    "            if self.commodity != None and type(self.commodity) == str:\n",
    "                self.case_study_data_file_path = f'{self.user_data_folder}/case study data.xlsx'\n",
    "                history_file = pd.read_excel(self.case_study_data_file_path, index_col=0, sheet_name=self.commodity)\n",
    "                self.historical_data = history_file.iloc[1:][\n",
    "                    'Primary supply' if 'Primary supply' in history_file.columns else 'Primary production'].astype(\n",
    "                    float)\n",
    "                self.historical_data = self.historical_data.dropna()\n",
    "                self.historical_data.index = self.historical_data.index.astype(int)\n",
    "                self.price_adjustment_results_file_path = f'{self.user_data_folder}/price adjustment results.csv'\n",
    "                if self.price_to_use != 'case study data' or 'Primary commodity price' not in self.historical_data.dropna().columns:\n",
    "                    self.historical_price_data = pd.read_csv(self.price_adjustment_results_file_path, index_col=0)\n",
    "                    cap_mat = self.element_commodity_map[self.commodity]\n",
    "                    price_map = {'log': 'log(' + cap_mat + ')', 'diff': 'diff(' + cap_mat+')',\n",
    "                                 'original': cap_mat + ' original'}\n",
    "                    self.historical_price_data = self.historical_price_data[price_map[self.price_to_use]].astype(\n",
    "                        float).dropna().sort_index()\n",
    "                    self.historical_price_data.name = 'Primary commodity price'\n",
    "                    if hasattr(self.historical_data,\n",
    "                               'columns') and 'Primary commodity price' in self.historical_data.columns:\n",
    "                        self.historical_data.drop('Primary commodity price', axis=1, inplace=True)\n",
    "                    self.historical_data = pd.concat([self.historical_data, self.historical_price_data], axis=1)\n",
    "                if 'Primary commodity price' in self.historical_data.columns:\n",
    "                    self.historical_data.loc[self.historical_data.index, 'Primary commodity price'] = \\\n",
    "                    self.historical_data['Primary commodity price'].rolling(self.historical_price_rolling_window,\n",
    "                                                                            min_periods=1, center=True).mean()\n",
    "\n",
    "    def load_historical_data(self):\n",
    "        \"\"\"\n",
    "        This function is called within the __init__() method and updates the hyperparameters\n",
    "        using default changes (setting incentive_require_tune_years, presimulate_n_years,\n",
    "        and end_calibrate_years hyperparameter values to 10, and start_calibrate_years to 5.\n",
    "        This means that for all scenarios we run, we have it run on an additional 10 years of\n",
    "        data before the simulation_time variable start, where it is required to tune the\n",
    "        incentive pool such that concentrate supply=demand in each of those years. The\n",
    "        start and end calibrate years give the time after the additional early simulation\n",
    "        start that we take the mean opening probability and use it for the case where\n",
    "        incentive_opening_probability==0.\n",
    "\n",
    "        Otherwise, this function uses the value of hyperparam from\n",
    "        initialization to update the self.hyperparam variable. The input\n",
    "        to initialization can be a string or pandas series of hyperparameters and their values.\n",
    "        If it is a string, the function gets the values from the case study data excel file that\n",
    "        match the string.\n",
    "        \"\"\"\n",
    "        simulation_time = self.simulation_time\n",
    "        if type(self.commodity) == str and not hasattr(self, 'historical_data'):\n",
    "            self.case_study_data_file_path = f'{self.user_data_folder}/case study data.xlsx'\n",
    "            input_file = pd.read_excel(self.case_study_data_file_path, index_col=0)\n",
    "            commodity_inputs = input_file[self.commodity]\n",
    "            n_years_tune = 20\n",
    "            commodity_inputs.loc['incentive_require_tune_years'] = n_years_tune\n",
    "            commodity_inputs.loc['presimulate_n_years'] = n_years_tune\n",
    "            commodity_inputs.loc['end_calibrate_years'] = n_years_tune\n",
    "            commodity_inputs.loc['start_calibrate_years'] = 5\n",
    "            commodity_inputs.loc['close_price_method'] = 'max'\n",
    "            commodity_inputs = commodity_inputs.dropna()\n",
    "\n",
    "            history_file = pd.read_excel(self.case_study_data_file_path, index_col=0, sheet_name=self.commodity)\n",
    "            historical_data = history_file.loc[[i for i in history_file.index if i != 'Source(s)']].dropna(axis=1,\n",
    "                                                                                                           how='all').astype(\n",
    "                float)\n",
    "            historical_data.index = historical_data.index.astype(int)\n",
    "            if simulation_time[0] in historical_data.index and simulation_time[0] != 2019:\n",
    "                historical_data = history_file.loc[[i for i in simulation_time if i in history_file.index]]\n",
    "                if self.price_to_use != 'case study data':\n",
    "                    self.price_adjustment_results_file_path = f'{self.user_data_folder}/price adjustment results.csv'\n",
    "                    price_update_file = pd.read_csv(self.price_adjustment_results_file_path, index_col=0)\n",
    "                    cap_mat = self.element_commodity_map[self.commodity]\n",
    "                    price_map = {'log': 'log(' + cap_mat + ')', 'diff': 'diff(' + cap_mat+')',\n",
    "                                 'original': cap_mat + ' original'}\n",
    "                    historical_price = price_update_file[price_map[self.price_to_use]].astype(float)\n",
    "                    if not self.use_historical_price_for_mine_initialization:\n",
    "                        historical_price = historical_price.loc[\n",
    "                            [i for i in historical_price.index if i in self.simulation_time]]\n",
    "                    historical_price.name = 'Primary commodity price'\n",
    "                    historical_price.index = historical_price.index.astype(int)\n",
    "                    if 'Primary commodity price' in historical_data.columns:\n",
    "                        historical_data = pd.concat(\n",
    "                            [historical_data.drop('Primary commodity price', axis=1), historical_price], axis=1)\n",
    "                        historical_data.index = historical_data.index.astype(int)\n",
    "                        historical_data = historical_data.sort_index().dropna(how='all')\n",
    "                    else:\n",
    "                        historical_data = pd.concat([historical_data, historical_price], axis=1).sort_index().dropna(\n",
    "                            how='all')\n",
    "                if 'Primary commodity price' in historical_data.columns:\n",
    "                    historical_data.loc[historical_data.index, 'Primary commodity price'] = historical_data[\n",
    "                        'Primary commodity price'].rolling(self.historical_price_rolling_window, min_periods=1,\n",
    "                                                           center=True).mean()\n",
    "                original_demand = commodity_inputs['initial_demand']\n",
    "                original_production = commodity_inputs['Total production, Global']\n",
    "                original_primary_production = commodity_inputs['primary_production']\n",
    "                if 'Total demand' in historical_data.columns:\n",
    "                    commodity_inputs.loc['initial_demand'] = historical_data['Total demand'][simulation_time[0]]\n",
    "                elif 'Primary production' in historical_data.columns:\n",
    "                    commodity_inputs.loc['initial_demand'] = historical_data['Primary production'][simulation_time[\n",
    "                        0]] * original_demand / original_primary_production\n",
    "                    historical_data.loc[:, 'Total demand'] = historical_data['Primary production'] * original_demand / \\\n",
    "                                                             historical_data['Primary production'][simulation_time[-1]]\n",
    "                elif 'Primary supply' in historical_data.columns:\n",
    "                    commodity_inputs.loc['initial_demand'] = historical_data['Primary supply'][simulation_time[\n",
    "                        0]] * original_demand / original_primary_production\n",
    "                    historical_data.loc[:, 'Total demand'] = historical_data['Primary supply'] * original_demand / \\\n",
    "                                                             historical_data['Primary supply'][simulation_time[-1]]\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        'Need either [Total demand] or [Primary production] in historical data columns (ignore the brackets, but case sensitive)')\n",
    "\n",
    "                if 'Scrap demand' in historical_data.columns:\n",
    "                    commodity_inputs.loc['Recycling input rate, Global'] = historical_data['Scrap demand'][\n",
    "                                                                               simulation_time[0]] / \\\n",
    "                                                                           historical_data['Total demand'][\n",
    "                                                                               simulation_time[0]]\n",
    "                    commodity_inputs.loc['Recycling input rate, China'] = historical_data['Scrap demand'][\n",
    "                                                                              simulation_time[0]] / \\\n",
    "                                                                          historical_data['Total demand'][\n",
    "                                                                              simulation_time[0]]\n",
    "\n",
    "                if 'Primary production' in historical_data.columns:\n",
    "                    commodity_inputs.loc['primary_production'] = historical_data['Primary production'][\n",
    "                        simulation_time[0]]\n",
    "                elif 'Primary supply' in historical_data.columns:\n",
    "                    commodity_inputs.loc['primary_production'] = historical_data['Primary supply'][simulation_time[0]]\n",
    "                else:\n",
    "                    commodity_inputs.loc['primary_production'] *= commodity_inputs['initial_demand'] / original_demand\n",
    "\n",
    "                if 'Primary commodity price' in historical_data.columns:\n",
    "                    commodity_inputs.loc['primary_commodity_price'] = historical_data['Primary commodity price'][\n",
    "                        simulation_time[0]]\n",
    "\n",
    "                if 'Total production' in historical_data.columns:\n",
    "                    commodity_inputs.loc['Total production, Global'] = historical_data['Total production'][\n",
    "                        simulation_time[0]]\n",
    "                elif 'Scrap demand' in historical_data.columns and 'Primary production' in historical_data.columns:\n",
    "                    commodity_inputs.loc['Total production, Global'] = historical_data['Primary production'][\n",
    "                                                                           simulation_time[0]] + \\\n",
    "                                                                       historical_data['Scrap demand'][\n",
    "                                                                           simulation_time[0]]\n",
    "                elif 'Scrap demand' in historical_data.columns and 'Primary supply' in historical_data.columns:\n",
    "                    commodity_inputs.loc['Total production, Global'] = historical_data['Primary supply'][\n",
    "                                                                           simulation_time[0]] + \\\n",
    "                                                                       historical_data['Scrap demand'][\n",
    "                                                                           simulation_time[0]]\n",
    "                elif 'Primary production' in historical_data.columns:\n",
    "                    commodity_inputs.loc['Total production, Global'] = historical_data['Primary production'][\n",
    "                                                                           simulation_time[\n",
    "                                                                               0]] * original_production / original_primary_production\n",
    "                elif 'Primary supply' in historical_data.columns:\n",
    "                    commodity_inputs.loc['Total production, Global'] = historical_data['Primary supply'][\n",
    "                                                                           simulation_time[\n",
    "                                                                               0]] * original_production / original_primary_production\n",
    "                else:\n",
    "                    commodity_inputs.loc['Total production, Global'] = commodity_inputs[\n",
    "                                                                           'initial_demand'] * original_production / \\\n",
    "                                                                       commodity_inputs['Total production, Global']\n",
    "                # if self.material=='Al': commodity_inputs.loc['Total production, Global'] = 36000\n",
    "            self.historical_data = historical_data.copy()\n",
    "            self.commodity_inputs = commodity_inputs.copy()\n",
    "            for q in commodity_inputs.index:\n",
    "                self.hyperparam.loc[q, 'Value'] = commodity_inputs[q]\n",
    "            self.changing_base_parameters_series = commodity_inputs.copy()\n",
    "        self.hyperparam.loc['commodity', 'Value'] = self.commodity\n",
    "        if 'SX-EW fraction of production, Global' in self.hyperparam.dropna(how='all').index:\n",
    "            self.hyperparam.loc['primary_sxew_fraction', 'Value'] = self.hyperparam['Value'][\n",
    "                'SX-EW fraction of production, Global']\n",
    "        else:\n",
    "            self.hyperparam.loc['primary_sxew_fraction', 'Value'] = 0\n",
    "\n",
    "        if hasattr(self, 'historical_data') and 'Primary commodity price' in self.historical_data.columns:\n",
    "            self.primary_commodity_price = self.historical_data['Primary commodity price'].dropna()\n",
    "            self.primary_commodity_price = pd.concat([pd.Series(self.primary_commodity_price.iloc[0],\n",
    "                                                                np.arange(1900,\n",
    "                                                                          self.primary_commodity_price.dropna().index[\n",
    "                                                                              0])),\n",
    "                                                      self.primary_commodity_price]).sort_index()\n",
    "\n",
    "    def initialize_hyperparam(self):\n",
    "        hyperparameters = pd.DataFrame(np.nan, index=[], columns=['Value', 'Notes'])\n",
    "        hyperparameters.loc['random_state', :] = 20220208, 'random state/seed used for mine value generation'\n",
    "\n",
    "        # refining and demand\n",
    "        hyperparameters.loc['refining and demand', :] = np.nan\n",
    "        hyperparameters.loc['initial_demand', :] = 1, 'initial overall demand'\n",
    "        hyperparameters.loc['Recycling input rate, Global', ['Value',\n",
    "                                                             'Notes']] = 0.4, 'float, fraction of demand in the initial year satisfied by recycled inputs; includes refined and direct melt'\n",
    "        hyperparameters.loc['Recycling input rate, China', ['Value',\n",
    "                                                            'Notes']] = 0.4, 'float, fraction of demand in the initial year satisfied by recycled inputs; includes refined and direct melt'\n",
    "        hyperparameters.loc['Secondary refinery fraction of recycled content, Global',\n",
    "        :] = 0.6, 'float, fraction of recycled content demanded by refineries, remainder is direct melt'\n",
    "        hyperparameters.loc['china_fraction_demand', ['Value',\n",
    "                                                      'Notes']] = 0.7, 'China fraction of demand, was 0.52645 for copper in 2019'\n",
    "        hyperparameters.loc['scrap_to_cathode_eff', :] = 0.99, 'Efficiency of remelting and refining scrap'\n",
    "\n",
    "        # refining only\n",
    "        hyperparameters.loc['refining only', :] = np.nan\n",
    "        hyperparameters.loc['pri CU TCRC elas', :] = 0.1, 'Capacity utilization at primary-only refineries'\n",
    "        hyperparameters.loc['sec CU TCRC elas', :] = 0.1, 'Capacity utilization at secondary-consuming refineries'\n",
    "        hyperparameters.loc['pri CU price elas',\n",
    "        :] = 0.01, 'primary refinery capacity uitlization elasticity to primary commodity price'\n",
    "        hyperparameters.loc['sec CU price elas',\n",
    "        :] = 0.01, 'secondary refinery capacity uitlization elasticity to primary commodity price'\n",
    "        hyperparameters.loc['sec ratio TCRC elas', :] = -0.4, 'Secondary ratio elasticity to TCRC'\n",
    "        hyperparameters.loc['sec ratio scrap spread elas', :] = 0.8, 'Secondary ratio elasticity to scrap spread'\n",
    "        hyperparameters.loc[\n",
    "            'Use regions'] = False, 'True makes it so global refining is determined from combo of China and RoW; False means we ignore the region level'\n",
    "        hyperparameters.loc[\n",
    "            'refinery_capacity_growth_lag'] = 1, 'capacity growth lag, can be 1 or 0. 1 means that we use the year_i-1 and year_i-2 years to calculated the growth associated with mining or demand for calculating capacity growth, while 0 means we use year_i and year_i-1.'\n",
    "\n",
    "        # price\n",
    "        hyperparameters.loc['price', :] = np.nan\n",
    "        hyperparameters.loc['primary_commodity_price', :] = 6000, 'primary commodity price'\n",
    "        hyperparameters.loc['initial_scrap_spread', :] = 500, 'scrap spread at simulation start'\n",
    "\n",
    "        # price elasticities\n",
    "        hyperparameters.loc['price elasticities', :] = np.nan\n",
    "        hyperparameters.loc['primary_commodity_price_elas_sd',\n",
    "        :] = -0.4, 'primary commodity price elasticity to supply-demand imbalance (here using S/D ratio)'\n",
    "        hyperparameters.loc['tcrc_elas_sd',\n",
    "        :] = 0.7, 'TCRC elasticity to supply-demand imbalance (here using S/D ratio)'\n",
    "        hyperparameters.loc['tcrc_elas_price', :] = 0.7, 'TCRC elasticity to primary commodity price change'\n",
    "        hyperparameters.loc['scrap_spread_elas_sd',\n",
    "        :] = 0.4, 'scrap spread elasticity to supply-demand imbalance (here using S/D ratio)'\n",
    "        hyperparameters.loc['scrap_spread_elas_primary_commodity_price',\n",
    "        :] = 0.4, 'scrap spread elasticity to primary commodity price, using ratio year-over-year'\n",
    "        hyperparameters.loc['direct_melt_elas_scrap_spread', :] = 0.4, 'direct melt demand elasticity to scrap spread'\n",
    "        hyperparameters.loc['collection_elas_scrap_price',\n",
    "        :] = 0.6, '% increase in collection corresponding with a 1% increase in scrap price'\n",
    "\n",
    "        # determining model structure\n",
    "        hyperparameters.loc['determining model structure', :] = np.nan\n",
    "        hyperparameters.loc['scrap_trade_simplistic',\n",
    "        :] = True, 'if True, sets scrap supply in each region equal to the demand scaled by the global supply/demand ratio. If False, we should set up a price-informed trade algorithm, but have not done that yet so this should stay True for now'\n",
    "        hyperparameters.loc['presimulate_mining',\n",
    "        :] = True, 'True means we simulate mining production for presimulate_n_years before the simulation starts, theoretically to force alignment with simulation start time mine production'\n",
    "        hyperparameters.loc['presimulate_n_years',\n",
    "        :] = 20, 'int, number of years in the past to simulate mining to establish baseline'\n",
    "        hyperparameters.loc['collection_rate_price_response',\n",
    "        :] = True, 'bool, whether or not this parameter responds to price'\n",
    "        hyperparameters.loc['direct_melt_price_response',\n",
    "        :] = True, 'bool, whether or not this parameter responds to price'\n",
    "        hyperparameters.loc[\n",
    "            'refinery_capacity_fraction_increase_mining'] = 0.001, '0 means refinery capacity growth is determined by demand growth alone; 1 means refinery capacity growth is determined by mining production growth alone. Values in between allow for a weighted mixture.'\n",
    "\n",
    "        # mining only\n",
    "        hyperparameters.loc['mining only', :] = np.nan\n",
    "        hyperparameters.loc['primary_ore_grade_mean', :] = 0.1, 'Ore grade mean for lognormal distribution'\n",
    "        hyperparameters.loc['use_ml_to_accelerate', 'Value'] = False\n",
    "        hyperparameters.loc['ml_accelerate_initialize_years', 'Value'] = 20\n",
    "        hyperparameters.loc['ml_accelerate_every_n_years', 'Value'] = 5\n",
    "        hyperparameters.loc['incentive_tuning_option', 'Value'] = 'pid-2019'\n",
    "        hyperparameters.loc['internal_price_formation', 'Value'] = False\n",
    "        hyperparameters.loc['primary_production_mean', 'Value'] = 0.001\n",
    "        hyperparameters.loc['primary_production_var', 'Value'] = 0.5\n",
    "        hyperparameters.loc['primary_ore_grade_var', 'Value'] = 1\n",
    "        hyperparameters.loc['incentive_opening_probability', 'Value'] = 0.05\n",
    "        hyperparameters.loc['incentive_require_tune_years',\n",
    "        :] = 20, 'requires incentive tuning for however many years such that supply=demand, with no requirements on incentive_opening_probability and allowing the given incentive_opening_probability to be used'\n",
    "        hyperparameters.loc['demand_series_method', 'Value'] = 'none'\n",
    "        hyperparameters.loc['end_calibrate_years', 'Value'] = 20\n",
    "        hyperparameters.loc['start_calibrate_years', 'Value'] = 5\n",
    "        hyperparameters.loc['ramp_up_cu', 'Value'] = 0.4\n",
    "        hyperparameters.loc['ml_accelerate_initialize_years', 'Value'] = max(\n",
    "            hyperparameters['Value'][['ml_accelerate_initialize_years', 'end_calibrate_years']])\n",
    "        hyperparameters.loc['mine_cu_margin_elas', 'Value'] = 0.8\n",
    "        hyperparameters.loc['mine_cost_price_elas', 'Value'] = 0\n",
    "        hyperparameters.loc['mine_cost_og_elas', 'Value'] = -0.113\n",
    "        hyperparameters.loc['mine_cost_change_per_year', 'Value'] = 0.05\n",
    "        hyperparameters.loc['incentive_mine_cost_change_per_year', 'Value'] = 0.05\n",
    "        hyperparameters.loc['primary_price_resources_contained_elas', 'Value'] = 0.5\n",
    "        hyperparameters.loc['close_price_method', 'Value'] = 'max'\n",
    "        hyperparameters.loc['close_years_back', 'Value'] = 3\n",
    "        hyperparameters.loc['close_probability_split_max', 'Value'] = 0.3\n",
    "        hyperparameters.loc['close_probability_split_mean', 'Value'] = 0.5\n",
    "        hyperparameters.loc['close_probability_split_min', 'Value'] = 0.2\n",
    "        hyperparameters.loc['primary_oge_scale', 'Value'] = 0.399365\n",
    "        hyperparameters.loc[\n",
    "            'initial_ore_grade_decline', 'Value'] = -0.05  # 'Initial ore grade for new mines, elasticity to cumulative ore treated'\n",
    "        hyperparameters.loc['annual_reserves_ratio_with_initial_production_const', 'Value'] = 1.1\n",
    "        hyperparameters.loc['primary_overhead_const', 'Value'] = 0\n",
    "        hyperparameters.loc['ramp_up_fraction', ['Value', 'Notes']] = np.array([0.02,\n",
    "                                                                                'fraction of mines in the initial mine generation step that are in any of the ramp up stages (e.g. if ramp_up_year is 3 and ramp_up_fraction is 0.1, then 10% of the mines will have ramp up flag=1, 10% ramp up flag=2, etc.). Value is currently 0.02 based on an initial guess.'],\n",
    "                                                                               dtype='object')\n",
    "        hyperparameters.loc['demand_series_method', ['Value', 'Notes']] = np.array(['',\n",
    "                                                                                    'This is for setting up the so-called demand series, which is what the mining module tries to tune mine production to match when historical presimulation is done. This value is normally either yoy or target, and setting it to anything else allows us to ensure the demand_series used for mine tuning is the one from our historical data file.'],\n",
    "                                                                                   dtype='object')\n",
    "        hyperparameters.loc['reserves_ratio_price_lag',\n",
    "        :] = 5, 'lag on price change price(t-lag)/price(t-lag-1) used for informing incentive pool size change, paired with resources_contained_elas_primary_price (and byproduct if byproduct==True)'\n",
    "\n",
    "        # demand\n",
    "        hyperparameters.loc['demand only', :] = np.nan\n",
    "        hyperparameters.loc['commodity'] = 'notAu'\n",
    "        hyperparameters.loc['sector_specific_dematerialization_tech_growth', 'Value'] = -0.03\n",
    "        hyperparameters.loc['sector_specific_price_response', 'Value'] = -0.06\n",
    "        hyperparameters.loc['region_specific_price_response', 'Value'] = -0.1\n",
    "        hyperparameters.loc['intensity_response_to_gdp', 'Value'] = 0.69\n",
    "\n",
    "        self.hyperparam = hyperparameters.copy()\n",
    "\n",
    "    def update_hyperparam(self):\n",
    "        update = 0\n",
    "        if type(self.input_hyperparam) == pd.core.frame.DataFrame:\n",
    "            if 'Value' in self.input_hyperparam.columns:\n",
    "                update = self.input_hyperparam.copy()['Value']\n",
    "            elif self.input_hyperparam.shape[1] == 1:\n",
    "                update = self.input_hyperparam.copy().iloc[:, 0]\n",
    "        elif type(self.input_hyperparam) == pd.core.series.Series:\n",
    "            update = self.input_hyperparam.copy()\n",
    "        if type(update) != int:\n",
    "            for ind in update.index:\n",
    "                if type(update[ind]) == pd.core.series.Series:\n",
    "                    con = pd.DataFrame([[update[ind], '']],\n",
    "                                       [ind], ['Value', 'Notes'])\n",
    "                    if ind in self.hyperparam.index: self.hyperparam.drop(ind, inplace=True)\n",
    "                    self.hyperparam = pd.concat([self.hyperparam, con])\n",
    "                else:\n",
    "                    self.hyperparam.loc[ind, 'Value'] = update[ind]\n",
    "\n",
    "    def hyperparam_agreement(self):\n",
    "        dem = self.demand\n",
    "        dem.collection_rate_price_response = self.collection_rate_price_response\n",
    "        dem.scenario_type = self.scenario_type\n",
    "        dem.scrap_shock_start = self.scrap_shock_start\n",
    "        dem.collection_rate_duration = self.collection_rate_duration\n",
    "        dem.collection_rate_pct_change_tot = self.collection_rate_pct_change_tot\n",
    "        dem.collection_rate_pct_change_inc = self.collection_rate_pct_change_inc\n",
    "\n",
    "        ref = self.refine\n",
    "        ref.scenario_type = self.scenario_type\n",
    "        ref.secondary_refined_price_response = self.secondary_refined_price_response\n",
    "        ref.secondary_refined_duration = self.secondary_refined_duration\n",
    "        ref.secondary_refined_pct_change_tot = self.secondary_refined_pct_change_tot\n",
    "        ref.secondary_refined_pct_change_inc = self.secondary_refined_pct_change_inc\n",
    "        h = self.hyperparam['Value']\n",
    "\n",
    "        dem.hyperparam.loc['initial_demand', 'Value'] = h['initial_demand']\n",
    "        ref.hyperparam.loc['Total production', 'Global'] = h['initial_demand']  # will need updated later\n",
    "\n",
    "        dem.hyperparam.loc['china_fraction_demand', 'Value'] = h['china_fraction_demand']\n",
    "        ref.hyperparam.loc['Regional production fraction of total production', 'China'] = h['china_fraction_demand']\n",
    "\n",
    "        ref.hyperparam.loc['Recycling input rate', 'Global'] = h['Recycling input rate, Global']\n",
    "        ref.hyperparam.loc['Recycling input rate', 'China'] = h['Recycling input rate, China']\n",
    "        dem.hyperparam.loc['recycling_input_rate_china', 'Value'] = h['Recycling input rate, China']\n",
    "        dem.hyperparam.loc['recycling_input_rate_row', 'Value'] = (h['Recycling input rate, Global'] - h[\n",
    "            'Recycling input rate, China'] * h['china_fraction_demand']) / (1 - h['china_fraction_demand'])\n",
    "\n",
    "        dem.hyperparam.loc['scrap_to_cathode_eff', 'Value'] = h['scrap_to_cathode_eff']\n",
    "        ref.hyperparam.loc['scrap_to_cathode_eff', :] = h['scrap_to_cathode_eff']\n",
    "\n",
    "        if self.verbosity > 1: print('Refinery parameters updated:')\n",
    "        for param in np.intersect1d(ref.hyperparam.index, h.index):\n",
    "            ref.hyperparam.loc[param, :] = h[param]\n",
    "            if self.verbosity > 1:\n",
    "                print('   ref', param, 'now', h[param])\n",
    "\n",
    "        h_index_split = [j for j in h.index if ', ' in j]\n",
    "        h_index_ind = np.unique([j.split(', ')[0] for j in h_index_split])\n",
    "        for param in np.intersect1d(ref.hyperparam.index, h_index_ind):\n",
    "            regions = np.unique([j.split(', ')[1] for j in h_index_split if param in j])\n",
    "            for reg in regions:\n",
    "                ref.hyperparam.loc[param, reg] = h[', '.join([param, reg])]\n",
    "                if self.verbosity > 1:\n",
    "                    print('   ref', param, reg, 'now', h[', '.join([param, reg])])\n",
    "\n",
    "        for param in np.intersect1d(ref.ref_hyper_param.index, h.index):\n",
    "            ref.ref_hyper_param.loc[param, :] = h[param]\n",
    "            if self.verbosity > 1:\n",
    "                print('   rhp', param, h[param])\n",
    "\n",
    "        for param in np.intersect1d(dem.hyperparam.index, h.index):\n",
    "            dem.hyperparam.loc[param, 'Value'] = h[param]\n",
    "            if self.verbosity > 1:\n",
    "                print('   demand', param, h[param])\n",
    "\n",
    "        self.demand = dem\n",
    "        self.refine = ref\n",
    "\n",
    "    def initialize_integration(self):\n",
    "        '''\n",
    "        Sets up the integration, also sets up the scenario corresponding with scenario_name\n",
    "        '''\n",
    "        self.load_historical_data()\n",
    "\n",
    "        i = self.i\n",
    "        self.conc_to_cathode_eff = self.refine.ref_param['Global']['conc to cathode eff']\n",
    "        self.scrap_to_cathode_eff = self.refine.ref_param['Global']['scrap to cathode eff']\n",
    "\n",
    "        # initializing demand\n",
    "        self.hyperparam_agreement()\n",
    "        self.demand.direct_melt_alt = self.direct_melt_alt\n",
    "        self.demand.run()\n",
    "\n",
    "        # initializing collection_rate (scenario modification is done in the demand class)\n",
    "        self.collection_rate = self.demand.collection_rate.copy()\n",
    "        self.additional_scrap = self.demand.additional_scrap.copy()\n",
    "\n",
    "        # initializing refining\n",
    "        self.refine.run()\n",
    "\n",
    "        # total_demand\n",
    "        self.total_demand = pd.concat([self.demand.demand.sum(axis=1),\n",
    "                                       self.demand.demand['China'].sum(axis=1),\n",
    "                                       self.demand.demand['RoW'].sum(axis=1)],\n",
    "                                      axis=1, keys=['Global', 'China', 'RoW'])\n",
    "        demand_ph = self.total_demand.copy()\n",
    "        self.total_demand.loc[self.i + 1:] = np.nan\n",
    "\n",
    "        # refined_demand, direct_melt_demand\n",
    "        self.direct_melt_demand = self.total_demand.apply(\n",
    "            lambda x: x * self.refine.hyperparam.loc['Direct melt fraction of production'], axis=1)\n",
    "        self.direct_melt_fraction = self.direct_melt_demand / self.total_demand\n",
    "        self.direct_melt_fraction = self.direct_melt_fraction.apply(lambda x: self.direct_melt_fraction.loc[i], axis=1)\n",
    "        #         self.direct_melt_demand.loc[i+1:] = np.nan\n",
    "        self.refined_demand = self.total_demand - self.direct_melt_demand\n",
    "        self.direct_melt_demand = self.direct_melt_demand.apply(lambda x: x / self.scrap_to_cathode_eff, axis=1)\n",
    "        self.additional_direct_melt = self.direct_melt_demand.copy()\n",
    "        self.additional_direct_melt.loc[:] = 0\n",
    "        if self.scenario_type in ['scrap demand', 'scrap demand-alt', 'both', 'both-alt']:\n",
    "            shock_start = self.scrap_shock_start  # shock start is the year the scenario would have started originally (first change in 2020 for 2019 shock_start)\n",
    "            if not self.direct_melt_alt:  # trying an alternative method, seems like adding more each year is not quite in line with how the market would work. Instead, it should be that once someone increases demand, their new demand is implicit within the rest of the market so we do not need to keep adding it each year\n",
    "                multiplier_array = np.append(np.repeat(1, shock_start - self.simulation_time[0] + 1), np.append(\n",
    "                    np.repeat(1 + (self.direct_melt_pct_change_tot - 1) / self.direct_melt_duration,\n",
    "                              self.direct_melt_duration),\n",
    "                    [self.direct_melt_pct_change_inc for j in\n",
    "                     np.arange(1, np.sum(self.simulation_time >= shock_start) - self.direct_melt_duration)]))\n",
    "            else:\n",
    "                multiplier_array = np.append(np.repeat(1, shock_start - self.simulation_time[0]), np.append(\n",
    "                    np.linspace(1, self.direct_melt_pct_change_tot, self.direct_melt_duration + 1),\n",
    "                    [self.direct_melt_pct_change_tot * self.direct_melt_pct_change_inc ** j for j in\n",
    "                     np.arange(1, np.sum(self.simulation_time >= shock_start) - self.direct_melt_duration)]))\n",
    "            if self.direct_melt_price_response == False:\n",
    "                for yr, mul in zip(self.simulation_time, multiplier_array):\n",
    "                    self.direct_melt_fraction.loc[yr, :] *= mul\n",
    "            else:\n",
    "                multiplier_array -= 1\n",
    "                #                 ph2 = self.demand.scrap_collected.stack().unstack(1).unstack()\n",
    "                self.additional_direct_melt = demand_ph.loc[self.simulation_time[0]:].apply(\n",
    "                    lambda x: demand_ph.loc[shock_start] * multiplier_array[x.name - self.simulation_time[0]], axis=1)\n",
    "                self.additional_direct_melt.loc[self.simulation_time[0] - 1, :] = 0\n",
    "                ind = (self.additional_direct_melt > demand_ph.loc[self.additional_direct_melt.index] * 0.9 *\n",
    "                       self.direct_melt_fraction.loc[self.additional_direct_melt.index]).any(axis=1)\n",
    "                if ind.sum() > 0:\n",
    "                    ind = ind[ind].index\n",
    "                    self.additional_direct_melt.loc[ind] = demand_ph.loc[ind] * 0.9 * self.direct_melt_fraction.loc[ind]\n",
    "                    # print(330,'\\n',demand_ph.loc[ind]*0.9*self.direct_melt_fraction.loc[ind])\n",
    "                self.additional_direct_melt = self.additional_direct_melt.sort_index()\n",
    "                # print(333,'\\n',self.additional_direct_melt, '\\n', demand_ph.loc[self.additional_direct_melt.index]*0.9*self.direct_melt_fraction.loc[self.additional_direct_melt.index])\n",
    "\n",
    "        # concentrate_demand, secondary_refined_demand, refined_supply\n",
    "        self.concentrate_demand = self.refine.ref_stats.loc[:, idx[:, 'Primary production']].droplevel(1, axis=1)\n",
    "        self.concentrate_demand.loc[i + 1:] = np.nan\n",
    "        self.secondary_refined_demand = self.refine.ref_stats.loc[:, idx[:, 'Secondary production']].droplevel(1,\n",
    "                                                                                                               axis=1)\n",
    "        self.secondary_refined_demand.loc[i + 1:] = np.nan\n",
    "        self.secondary_ratio = self.refine.ref_stats.loc[:, idx[:, 'Secondary ratio']].droplevel(1, axis=1)\n",
    "        self.refined_supply = self.concentrate_demand + self.secondary_refined_demand\n",
    "        # SX-EW production is added to refined supply after mining is initialized below\n",
    "        self.primary_supply = self.concentrate_demand / self.refined_supply * self.refined_demand\n",
    "        self.primary_demand = self.refined_demand * self.secondary_refined_demand / self.refined_supply\n",
    "\n",
    "        self.concentrate_demand = self.concentrate_demand.apply(lambda x: x / self.conc_to_cathode_eff, axis=1)\n",
    "        self.secondary_refined_demand = self.secondary_refined_demand.apply(lambda x: x / self.scrap_to_cathode_eff,\n",
    "                                                                            axis=1)\n",
    "        self.additional_secondary_refined = self.direct_melt_demand.copy()\n",
    "        self.additional_secondary_refined.loc[:] = 0\n",
    "        if self.scenario_type in ['scrap demand', 'scrap demand-alt', 'both', 'both-alt']:\n",
    "            shock_start = self.scrap_shock_start  # shock start is the year the scenario would have started originally (first change in 2020 for 2019 shock_start)\n",
    "            if not self.secondary_refined_alt:\n",
    "                multiplier_array = np.append(np.repeat(1, shock_start - self.simulation_time[0] + 1), np.append(\n",
    "                    np.repeat(1 + (self.secondary_refined_pct_change_tot - 1) / self.secondary_refined_duration,\n",
    "                              self.secondary_refined_duration),\n",
    "                    [self.secondary_refined_pct_change_inc for j in\n",
    "                     np.arange(1, np.sum(self.simulation_time >= shock_start) - self.secondary_refined_duration)]))\n",
    "            else:\n",
    "                multiplier_array = np.append(np.repeat(1, shock_start - self.simulation_time[0]), np.append(\n",
    "                    np.linspace(1, self.secondary_refined_pct_change_tot, self.secondary_refined_duration + 1),\n",
    "                    [self.secondary_refined_pct_change_tot * self.secondary_refined_pct_change_inc ** j for j in\n",
    "                     np.arange(1, np.sum(self.simulation_time >= shock_start) - self.secondary_refined_duration)]))\n",
    "            if self.secondary_refined_price_response == False:\n",
    "                for yr, mul in zip(self.simulation_time, multiplier_array):\n",
    "                    self.secondary_ratio.loc[yr, :] *= mul\n",
    "                self.refine.secondary_ratio = self.secondary_ratio.copy()\n",
    "            else:\n",
    "                multiplier_array -= 1\n",
    "                #                 ph2 = self.demand.scrap_collected.stack().unstack(1).unstack()\n",
    "                #                 self.additional_secondary_refined = self.demand.demand.loc[self.simulation_time[0]:].apply(lambda x: ph2.loc[self.simulation_time[0]]*multiplier_array[x.name-self.simulation_time[0]],axis=1)\n",
    "                self.additional_secondary_refined = demand_ph.loc[self.simulation_time[0]:].apply(\n",
    "                    lambda x: demand_ph.loc[shock_start] * multiplier_array[x.name - self.simulation_time[0]], axis=1)\n",
    "                self.additional_secondary_refined.loc[self.simulation_time[0] - 1, :] = 0\n",
    "                self.additional_secondary_refined = self.additional_secondary_refined.sort_index()\n",
    "                self.refine.additional_secondary_refined = self.additional_secondary_refined.copy()\n",
    "        self.initialize_mining()\n",
    "\n",
    "    def complete_initialization(self):\n",
    "        h = self.hyperparam['Value']\n",
    "\n",
    "        # concentrate_supply,    initializing mining\n",
    "        if h['presimulate_mining']:\n",
    "            self.presimulate_mining()\n",
    "        else:\n",
    "            self.mining.run()\n",
    "\n",
    "        self.concentrate_supply = self.mining.concentrate_supply_series.copy()\n",
    "        self.sxew_supply = self.mining.sxew_supply_series.copy()\n",
    "        if self.refine.hyperparam['Global']['SX-EW fraction of production']!=0:\n",
    "            self.row_fraction_sxew = self.refine.hyperparam['RoW'][[\n",
    "                'SX-EW fraction of production', 'Total production']].product() / self.refine.hyperparam['Global'][[\n",
    "                'SX-EW fraction of production', 'Total production']].product()\n",
    "            self.china_fraction_sxew = self.refine.hyperparam['China'][[\n",
    "                'SX-EW fraction of production', 'Total production']].product() / self.refine.hyperparam['Global'][[\n",
    "                'SX-EW fraction of production', 'Total production']].product()\n",
    "        else:\n",
    "            self.row_fraction_sxew = 0\n",
    "            self.china_fraction_sxew = 0\n",
    "        # ^ we assume that the distribution of SX-EW production between China and RoW remains constant throughout\n",
    "        self.sxew_supply_regional = pd.concat([\n",
    "            self.sxew_supply,\n",
    "            self.sxew_supply * self.china_fraction_sxew,\n",
    "            self.sxew_supply * self.row_fraction_sxew\n",
    "        ], axis=1, keys=['Global', 'China', 'RoW'])\n",
    "        self.refined_supply = (self.refined_supply + self.sxew_supply_regional).loc[self.simulation_time[0]:]\n",
    "        self.mine_production = self.concentrate_supply + self.sxew_supply\n",
    "\n",
    "        # scrap_supply, scrap_demand\n",
    "        self.scrap_supply = self.demand.scrap_supply.copy()\n",
    "        self.scrap_demand = self.secondary_refined_demand + self.direct_melt_demand\n",
    "\n",
    "    def initialize_price(self):\n",
    "        i = self.i\n",
    "        h = self.hyperparam['Value']\n",
    "        self.scrap_spread.loc[:i] = h['initial_scrap_spread']\n",
    "        if self.primary_commodity_price.isna().all():\n",
    "            self.primary_commodity_price.loc[:i] = h['primary_commodity_price']\n",
    "        self.tcrc.loc[:i] = self.mining.primary_tcrc_series[i]\n",
    "\n",
    "    def price_evolution(self):\n",
    "        i = self.i\n",
    "        h = self.hyperparam['Value']\n",
    "        # refined\n",
    "        self.refined_supply = self.refined_supply.where(self.refined_supply > 1e-9).fillna(1e-9)\n",
    "        self.refined_supply = self.refined_supply.where(self.refined_supply < 1e12).fillna(1e12)\n",
    "        self.refined_demand = self.refined_demand.where(self.refined_demand > 1e-9).fillna(1e-9)\n",
    "        self.refined_demand = self.refined_demand.where(self.refined_demand < 1e12).fillna(1e12)\n",
    "        self.primary_commodity_price.loc[i] = self.primary_commodity_price.loc[i - 1] * (\n",
    "                    self.refined_supply['Global'][i - 1] / self.refined_demand['Global'][i - 1]) ** h[\n",
    "                                                  'primary_commodity_price_elas_sd']\n",
    "        self.primary_commodity_price = self.primary_commodity_price.where(self.primary_commodity_price > 1e-9).fillna(\n",
    "            1e-9)\n",
    "        self.primary_commodity_price = self.primary_commodity_price.where(self.primary_commodity_price < 1e20).fillna(\n",
    "            1e20)\n",
    "        if self.force_integration_historical_price:\n",
    "            self.primary_commodity_price.loc[i] = self.historical_data['Primary commodity price'][i]\n",
    "        # if hasattr(self,'historical_data'):\n",
    "        #     self.primary_commodity_price.loc[i] = self.historical_data['Primary commodity price'][i]\n",
    "\n",
    "        # tcrc\n",
    "        self.concentrate_supply = self.concentrate_supply.where(self.concentrate_supply > 1e-9).fillna(1e-9)\n",
    "        self.concentrate_supply = self.concentrate_supply.where(self.concentrate_supply < 1e12).fillna(1e12)\n",
    "        self.concentrate_demand = self.concentrate_demand.where(self.concentrate_demand > 1e-9).fillna(1e-9)\n",
    "        self.concentrate_demand = self.concentrate_demand.where(self.concentrate_demand < 1e12).fillna(1e12)\n",
    "        self.tcrc.loc[i] = self.tcrc.loc[i - 1] * (\n",
    "                    self.concentrate_supply[i - 1] / self.concentrate_demand['Global'][i - 1]) ** h['tcrc_elas_sd'] \\\n",
    "                           * (self.primary_commodity_price.loc[i] / self.primary_commodity_price.loc[i - 1]) ** h[\n",
    "                               'tcrc_elas_price']\n",
    "        self.tcrc = self.tcrc.where(self.tcrc > 1e-9).fillna(1e-9)\n",
    "        self.tcrc = self.tcrc.where(self.tcrc < 1e12).fillna(1e12)\n",
    "\n",
    "        # scrap trade\n",
    "        self.scrap_supply = self.scrap_supply.where(self.scrap_supply > 1e-9).fillna(1e-9)\n",
    "        self.scrap_supply = self.scrap_supply.where(self.scrap_supply < 1e12).fillna(1e12)\n",
    "        self.scrap_demand = self.scrap_demand.where(self.scrap_demand > 1e-9).fillna(1e-9)\n",
    "        self.scrap_demand = self.scrap_demand.where(self.scrap_demand < 1e12).fillna(1e12)\n",
    "        self.scrap_spread = self.scrap_spread.where(self.scrap_spread < 1e20).fillna(1e20)\n",
    "        self.scrap_spread = self.scrap_spread.where(self.scrap_spread > 1e-9).fillna(1e-9)\n",
    "        if h['scrap_trade_simplistic'] and i > self.simulation_time[2]:\n",
    "            self.scrap_supply.loc[i - 1, 'China'] = self.scrap_supply['China'][i - 2] * self.scrap_supply['Global'][\n",
    "                i - 1] / self.scrap_supply['Global'][i - 2]\n",
    "            self.scrap_supply.loc[i - 1, 'RoW'] = self.scrap_supply['RoW'][i - 2] * self.scrap_supply['Global'][i - 1] / \\\n",
    "                                                  self.scrap_supply['Global'][i - 2]\n",
    "        elif h['scrap_trade_simplistic']:\n",
    "            self.scrap_supply.loc[i - 1, 'China'] = self.scrap_supply['Global'][i - 1] * self.scrap_demand['China'][\n",
    "                i - 1] / self.scrap_demand['Global'][i - 1]\n",
    "            self.scrap_supply.loc[i - 1, 'RoW'] = self.scrap_supply['Global'][i - 1] * self.scrap_demand['RoW'][i - 1] / \\\n",
    "                                                  self.scrap_demand['Global'][i - 1]\n",
    "\n",
    "        # scrap spread\n",
    "        self.scrap_spread.loc[i, 'China'] = self.scrap_spread['China'][i - 1] * (\n",
    "                    self.scrap_supply['China'][i - 1] / self.scrap_demand['China'][i - 1]) ** h['scrap_spread_elas_sd'] \\\n",
    "                                            * (self.primary_commodity_price[i] / self.primary_commodity_price[i - 1]) ** \\\n",
    "                                            h['scrap_spread_elas_primary_commodity_price']\n",
    "\n",
    "        # print(self.scrap_spread['RoW'][i-1], self.scrap_supply['RoW'][i-1], self.scrap_demand['RoW'][i-1],\n",
    "        #     self.primary_commodity_price[i], self.primary_commodity_price[i-1])\n",
    "\n",
    "        self.scrap_spread.loc[i, 'RoW'] = self.scrap_spread['RoW'][i - 1] * (\n",
    "                    self.scrap_supply['RoW'][i - 1] / self.scrap_demand['RoW'][i - 1]) ** h['scrap_spread_elas_sd'] \\\n",
    "                                          * (self.primary_commodity_price[i] / self.primary_commodity_price[i - 1]) ** \\\n",
    "                                          h['scrap_spread_elas_primary_commodity_price']\n",
    "        self.scrap_spread.loc[i, 'Global'] = self.scrap_spread['Global'][i - 1] * (\n",
    "                    self.scrap_supply['Global'][i - 1] / self.scrap_demand['Global'][i - 1]) ** h[\n",
    "                                                 'scrap_spread_elas_sd'] \\\n",
    "                                             * (self.primary_commodity_price[i] / self.primary_commodity_price[\n",
    "            i - 1]) ** h['scrap_spread_elas_primary_commodity_price']\n",
    "        self.scrap_spread = self.scrap_spread.where(self.scrap_spread > 1e-9).fillna(1e-9)\n",
    "\n",
    "    def run_demand(self):\n",
    "        self.demand.i = self.i\n",
    "        i = self.i\n",
    "        h = self.hyperparam['Value'].copy()\n",
    "        if self.i - 2 not in self.primary_commodity_price.index:\n",
    "            self.primary_commodity_price.loc[self.i - 2] = self.primary_commodity_price[self.i - 1]\n",
    "        if self.i - 2 not in self.scrap_spread.index:\n",
    "            self.scrap_spread.loc[i - 2] = self.scrap_spread.loc[i - 1]\n",
    "        self.demand.commodity_price_series = self.primary_commodity_price.copy()\n",
    "        self.demand.collection_rate = self.collection_rate.copy()\n",
    "        self.demand.run()\n",
    "        self.additional_scrap = self.demand.additional_scrap.copy()\n",
    "        # print(self.additional_scrap.loc[2018:])\n",
    "\n",
    "    def run_refine(self):\n",
    "        h = self.hyperparam['Value']\n",
    "        self.refine.i = self.i\n",
    "        self.refine.tcrc_series = self.tcrc.copy()\n",
    "        self.refine.scrap_spread_series = self.scrap_spread.copy()\n",
    "        self.refine.refined_price_series = self.primary_commodity_price.copy()\n",
    "        if self.i - 2 not in self.concentrate_supply.index:\n",
    "            self.concentrate_supply.loc[self.i - 2] = self.concentrate_supply[self.i - 1] * \\\n",
    "                                                      self.refined_demand['Global'][self.i - 2] / \\\n",
    "                                                      self.refined_demand['Global'][self.i - 1]\n",
    "        if self.i - 2 not in self.scrap_supply.index:\n",
    "            self.scrap_supply.loc[self.i - 2] = self.scrap_supply.loc[self.i - 1] * self.refined_demand.loc[\n",
    "                self.i - 2] / self.refined_demand.loc[self.i - 1]\n",
    "\n",
    "        conc_supply = self.concentrate_supply.copy() / self.concentrate_supply[self.simulation_time[0]]\n",
    "        ref_demand = self.refined_demand['Global'].copy() / self.refined_demand['Global'][self.simulation_time[0]]\n",
    "        pri_growth_series = conc_supply * h['refinery_capacity_fraction_increase_mining'] + ref_demand * (\n",
    "                    1 - h['refinery_capacity_fraction_increase_mining'])\n",
    "        self.refine.pri_cap_growth_series = pri_growth_series\n",
    "\n",
    "        self.refine.sec_cap_growth_series = self.scrap_supply.copy()\n",
    "        self.refine.run()\n",
    "\n",
    "    def run_mining(self):\n",
    "        self.mining.i = self.i\n",
    "        self.mining.primary_price_series.loc[self.i] = self.primary_commodity_price[self.i]\n",
    "        self.mining.primary_tcrc_series.loc[self.i] = self.tcrc[self.i]\n",
    "        if self.concentrate_demand['Global'][self.i - 1] == np.nan and self.mining.demand_series[self.i - 1] != np.nan:\n",
    "            self.concentrate_demand.loc[self.i - 1, 'Global'] = self.mining.demand_series.loc[self.i - 1]\n",
    "            print(433, 'did this one')\n",
    "        self.mining.demand_series.loc[self.i - 1] = self.concentrate_demand['Global'][\n",
    "                                                        self.i-1]+self.sxew_supply[self.i-1]\n",
    "        # try:\n",
    "        self.mining.run()\n",
    "        # except Exception as e:\n",
    "        #     import pickle\n",
    "        #     file = open('self.pkl', 'wb')\n",
    "        #     pickle.dump(self, file)\n",
    "        #     file.close()\n",
    "        #     raise e\n",
    "        # concentrate_supply\n",
    "        self.concentrate_supply.loc[self.i] = self.mining.concentrate_supply_series[self.i]\n",
    "        self.sxew_supply.loc[self.i] = self.mining.sxew_supply_series[self.i]\n",
    "        self.sxew_supply_regional = pd.concat([\n",
    "            self.sxew_supply,\n",
    "            self.sxew_supply * self.china_fraction_sxew,\n",
    "            self.sxew_supply * self.row_fraction_sxew\n",
    "        ], axis=1, keys=['Global', 'China', 'RoW'])\n",
    "        # since mining runs first, update refined supply after refining portion runs\n",
    "        self.mine_production.loc[self.i] = self.concentrate_supply.loc[self.i] + self.sxew_supply.loc[self.i]\n",
    "\n",
    "        self.primary_supply.loc[self.i, 'Global'] += self.sxew_supply.loc[self.i]\n",
    "        self.primary_supply.loc[self.i, 'RoW'] += self.sxew_supply.loc[self.i]\n",
    "\n",
    "    def initialize_mining(self):\n",
    "        h = self.hyperparam['Value'].copy()\n",
    "        end_yr = self.simulation_time[0]\n",
    "        if h['presimulate_mining']:\n",
    "            mine_simulation_time = np.arange(end_yr - h['presimulate_n_years'], self.simulation_time[-1] + 1)\n",
    "        else:\n",
    "            mine_simulation_time = self.simulation_time\n",
    "        self.mining = miningModel(simulation_time=mine_simulation_time, byproduct=self.byproduct,\n",
    "                                  verbosity=self.verbosity)\n",
    "        self.mining.primary_price_series = self.primary_commodity_price.copy()\n",
    "        if hasattr(self,\n",
    "                   'byproduct_price_series'): self.mining.byproduct_price_series = self.byproduct_commodity_price.copy()\n",
    "        m = self.mining\n",
    "        if self.verbosity > 1: print('Mining parameters updated:')\n",
    "        for param in [j for j in h.index if j in m.hyperparam]:\n",
    "            m.hyperparam[param] = h[param]\n",
    "            if self.verbosity > 1:\n",
    "                print('  ', param, 'now', h[param])\n",
    "\n",
    "        #         primary_production\n",
    "        if hasattr(self, 'historical_data'):\n",
    "            self.historical_mining = self.historical_data[\n",
    "                'Primary supply' if 'Primary supply' in self.historical_data.columns else 'Primary production'].astype(\n",
    "                float)\n",
    "            self.historical_mining = self.historical_mining.dropna()\n",
    "            self.historical_mining.index = self.historical_mining.index.astype(int)\n",
    "            start_hist_year = self.historical_mining.sort_index().index[0]\n",
    "            m.demand_series = self.demand.alt_demand[\n",
    "                [j for j in self.demand.alt_demand.columns if j != 'China Fraction']].sum(axis=1).rolling(5).mean()\n",
    "            m.demand_series *= self.historical_mining[start_hist_year] / m.demand_series[start_hist_year]\n",
    "            m.demand_series = pd.concat([m.demand_series.loc[~m.demand_series.index.isin(self.historical_mining.index)],\n",
    "                                         self.historical_mining]).sort_index()\n",
    "\n",
    "            # m.primary_price_series = self.historical_data['Primary commodity price'].copy().dropna()\n",
    "            # m.primary_price_series.name = 'Primary commodity price'\n",
    "            # m.primary_price_series = pd.concat([pd.Series(m.primary_price_series.iloc[0],[i for i in mine_simulation_time if i not in m.primary_price_series.index]),\n",
    "            #     m.primary_price_series]).sort_index()\n",
    "            # self.primary_price_series = m.primary_price_series.copy()\n",
    "            m.hyperparam['primary_commodity_price'] = m.primary_price_series.iloc[0]\n",
    "        else:\n",
    "            m.demand_series = self.demand.alt_demand[\n",
    "                [j for j in self.demand.alt_demand.columns if j != 'China Fraction']].sum(axis=1).rolling(5).mean()\n",
    "            m.demand_series *= self.concentrate_demand['Global'][end_yr] / m.demand_series[end_yr]\n",
    "            warn(\n",
    "                'if simulating a real commodity, the Integration class initialization should take a str input for its commodity variable, which should correspond with a sheet name in case study data.xlsx')\n",
    "        self.mining = m\n",
    "\n",
    "    def presimulate_mining(self):\n",
    "        h = self.hyperparam['Value']\n",
    "        end_yr = self.simulation_time[0]\n",
    "        hist_simulation_time = np.arange(end_yr - h['presimulate_n_years'], end_yr + 1)\n",
    "        m = self.mining\n",
    "\n",
    "        initial_ore_grade_decline = m.hyperparam['initial_ore_grade_decline']\n",
    "        incentive_mine_cost_change_per_year = m.hyperparam['incentive_mine_cost_change_per_year']\n",
    "        mine_cost_change_per_year = m.hyperparam['mine_cost_change_per_year']\n",
    "        annual_reserves_ratio_with_initial_production_slope = m.hyperparam[\n",
    "            'annual_reserves_ratio_with_initial_production_slope']\n",
    "        m.hyperparam['internal_price_formation'] = False\n",
    "        m.hyperparam['initial_ore_grade_decline'] = 0\n",
    "        m.hyperparam['incentive_mine_cost_change_per_year'] = 0\n",
    "        m.hyperparam['mine_cost_change_per_year'] = 0\n",
    "        m.hyperparam['annual_reserves_ratio_with_initial_production_slope'] = 0\n",
    "\n",
    "        m.hyperparam['primary_production'] = m.demand_series[m.simulation_time[0]]\n",
    "        primary_production_mean_series = m.demand_series * h['primary_production_mean'] / m.demand_series[end_yr]\n",
    "        self.primary_production_mean_series = primary_production_mean_series.copy()\n",
    "        #         display(m.demand_series)\n",
    "        for year in hist_simulation_time:\n",
    "            self.update_hyperparam_scenario_input(year)\n",
    "            m.i = year\n",
    "            m.in_history = year > hist_simulation_time[1]\n",
    "            if self.verbosity > 1:\n",
    "                print('sim mine history year:', year)\n",
    "            m.hyperparam['primary_production_mean'] = primary_production_mean_series[year]\n",
    "            # TODO Decide if changing mean mine size every year during presimulation makes sense\n",
    "            m.run()\n",
    "\n",
    "            if self.verbosity > 4:\n",
    "                fig, ax = easy_subplots(3)\n",
    "                ax[0].plot(m.supply_series, label='Supply', marker='o')\n",
    "                ax[0].plot(m.demand_series, label='Demand', marker='v', zorder=0)\n",
    "                ax[0].legend()\n",
    "\n",
    "                ax[1].plot(m.supply_series / m.demand_series, marker='o')\n",
    "\n",
    "                ax[2].plot(m.primary_tcrc_series, marker='o')\n",
    "\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "\n",
    "        m.hyperparam['initial_ore_grade_decline'] = initial_ore_grade_decline\n",
    "        m.hyperparam['incentive_mine_cost_change_per_year'] = incentive_mine_cost_change_per_year\n",
    "        m.hyperparam['mine_cost_change_per_year'] = mine_cost_change_per_year\n",
    "        m.hyperparam[\n",
    "            'annual_reserves_ratio_with_initial_production_slope'] = annual_reserves_ratio_with_initial_production_slope\n",
    "        m.hyperparam['internal_price_formation'] = False\n",
    "        m.initial_hyperparam = m.hyperparam.copy()\n",
    "        self.concentrate_supply = m.concentrate_supply_series.copy()\n",
    "        self.sxew_supply = m.sxew_supply_series.copy()\n",
    "        self.mine_production = self.concentrate_supply + self.sxew_supply\n",
    "        self.tcrc = m.primary_tcrc_series.copy()\n",
    "        self.tcrc = pd.concat([self.tcrc.dropna(), pd.Series(self.tcrc.dropna().sort_index().iloc[-1],\n",
    "                                                             [i for i in self.simulation_time if\n",
    "                                                              i not in self.tcrc.dropna().index])]).sort_index()\n",
    "        self.primary_commodity_price = m.primary_price_series.copy()\n",
    "        self.concentrate_demand.loc[:, 'Global'] = m.demand_series.copy()\n",
    "\n",
    "        m.in_history = False\n",
    "        self.mining = m\n",
    "\n",
    "    def calculate_direct_melt_demand(self):\n",
    "        i = self.i\n",
    "        h = self.hyperparam['Value']\n",
    "        #         self.direct_melt_fraction.loc[i-1] = self.direct_melt_demand.loc[i-1]/self.total_demand.loc[i-1]\n",
    "        if False and self.direct_melt_price_response:\n",
    "            self.direct_melt_demand.loc[i] = self.direct_melt_demand.loc[i - 1] * self.scrap_supply.loc[i] / \\\n",
    "                                             self.scrap_supply.loc[i - 1] \\\n",
    "                                             * (self.scrap_spread.loc[i] / self.scrap_spread.loc[i - 1]) ** h[\n",
    "                                                 'direct_melt_elas_scrap_spread']\n",
    "        elif False and self.direct_melt_price_response:\n",
    "            direct_melt_fraction = self.refine.hyperparam.loc['Direct melt fraction of production'] \\\n",
    "                                   * (self.scrap_spread.loc[i] / self.scrap_spread.loc[i - 1]) ** h[\n",
    "                                       'direct_melt_elas_scrap_spread']\n",
    "        elif self.direct_melt_price_response:\n",
    "            temp_direct_melt_demand = self.direct_melt_demand.copy()\n",
    "            temp_direct_melt_demand.loc[:i, :] -= self.additional_direct_melt.loc[:i, :]\n",
    "            # print(581,'\\n',temp_direct_melt_demand.dropna())\n",
    "            # print(582,'\\n',self.additional_direct_melt.dropna())\n",
    "            temp_direct_melt_fraction = temp_direct_melt_demand / self.total_demand\n",
    "            self.direct_melt_demand.loc[i - 1, :] -= self.additional_direct_melt.loc[i - 1, :]\n",
    "            direct_melt_fraction = temp_direct_melt_fraction.loc[i - 1] \\\n",
    "                                   * (self.scrap_spread.loc[i] / self.scrap_spread.loc[i - 1]) ** h[\n",
    "                                       'direct_melt_elas_scrap_spread']\n",
    "            if (direct_melt_fraction > 1).any(): direct_melt_fraction[direct_melt_fraction > 1] = 1\n",
    "            if (direct_melt_fraction < 0).any(): direct_melt_fraction[direct_melt_fraction < 0] = 0\n",
    "            self.direct_melt_demand.loc[i] = self.total_demand.loc[i] * direct_melt_fraction\n",
    "            self.direct_melt_demand.loc[i - 1:i, :] += self.additional_direct_melt.loc[i - 1:i, :]\n",
    "            self.direct_melt_fraction.loc[i] = self.direct_melt_demand.loc[i] / self.total_demand.loc[i]\n",
    "            self.temp_direct_melt_fraction = temp_direct_melt_fraction.copy()\n",
    "        else:\n",
    "            self.direct_melt_demand.loc[i] = self.total_demand.loc[i] * self.direct_melt_fraction.loc[i]\n",
    "\n",
    "    def update_integration_variables_post_demand(self):\n",
    "        i = self.i\n",
    "\n",
    "        # scrap supply\n",
    "        self.scrap_supply = self.demand.scrap_supply.copy()\n",
    "\n",
    "        # total_demand\n",
    "        self.total_demand.loc[i, 'Global'] = self.demand.demand.sum(axis=1)[i]\n",
    "        self.total_demand.loc[i, 'China'] = self.demand.demand['China'].sum(axis=1)[i]\n",
    "        self.total_demand.loc[i, 'RoW'] = self.demand.demand['RoW'].sum(axis=1)[i]\n",
    "        self.total_demand[self.total_demand < 0] = 0\n",
    "\n",
    "        # refined_demand, direct_melt_demand\n",
    "        self.calculate_direct_melt_demand()\n",
    "        self.refined_demand.loc[i] = self.total_demand.loc[i] - self.direct_melt_demand.loc[i]\n",
    "        self.refined_demand[self.refined_demand < 0] = 0\n",
    "        self.direct_melt_demand.loc[i] /= self.scrap_to_cathode_eff\n",
    "\n",
    "    def update_integration_variables_post_refine(self):\n",
    "        i = self.i\n",
    "\n",
    "        # concentrate_demand, secondary_refined_demand, refined_supply\n",
    "        self.concentrate_demand.loc[i] = self.refine.ref_stats.loc[i, idx[:, 'Primary production']].droplevel(1)\n",
    "        self.concentrate_demand[self.concentrate_demand < 0] = 0\n",
    "        self.secondary_refined_demand.loc[i] = self.refine.ref_stats.loc[i, idx[:, 'Secondary production']].droplevel(1)\n",
    "        self.secondary_refined_demand[self.secondary_refined_demand < 0] = 0\n",
    "        self.refined_supply.loc[i] = self.concentrate_demand.loc[i] + self.secondary_refined_demand.loc[\n",
    "            i] + self.sxew_supply_regional.loc[i]\n",
    "\n",
    "        # primary supply and demand, assuming all sxew is done in RoW (this update is done in the mining module, and here we do primary supply as primary refined supply)\n",
    "        self.primary_supply.loc[i] = self.concentrate_demand.copy().loc[i] / self.refined_supply.loc[\n",
    "            i] * self.refined_demand.loc[i]\n",
    "        self.primary_demand.loc[i] = self.refined_demand.loc[i] * self.secondary_refined_demand.loc[i] / \\\n",
    "                                     self.refined_supply.loc[i]\n",
    "\n",
    "        # correcting concentrate demand and secondary refined demand for efficiency loss\n",
    "        self.concentrate_demand.loc[i] /= self.conc_to_cathode_eff\n",
    "        self.secondary_refined_demand.loc[i] /= self.scrap_to_cathode_eff\n",
    "\n",
    "        # scrap_demand\n",
    "        self.scrap_demand.loc[i] = self.secondary_refined_demand.loc[i] + self.direct_melt_demand.loc[i]\n",
    "\n",
    "        self.mining.demand_series.loc[self.i] = self.concentrate_demand['Global'][\n",
    "                                                    self.i]+self.sxew_supply_regional['Global'][self.i]\n",
    "\n",
    "    def run(self):\n",
    "        for year in self.simulation_time:\n",
    "            if self.verbosity > 0: print(year)\n",
    "            self.i = year\n",
    "            if self.i == self.simulation_time[0]:\n",
    "                self.decode_scenario_name()\n",
    "                self.update_hyperparam_scenario_input()\n",
    "                self.initialize_integration()\n",
    "                # self.update_hyperparam_scenario_input()\n",
    "                self.complete_initialization()\n",
    "                self.initialize_price()\n",
    "            else:\n",
    "                self.update_hyperparam_scenario_input()\n",
    "                self.price_evolution()\n",
    "                self.run_demand()\n",
    "                self.update_integration_variables_post_demand()\n",
    "                self.run_mining()\n",
    "\n",
    "                self.update_integration_variables_post_demand()\n",
    "                self.run_refine()\n",
    "                self.update_integration_variables_post_refine()\n",
    "        self.concentrate_demand = pd.concat([self.concentrate_demand[['China','RoW']].sum(axis=1),\n",
    "                                             self.concentrate_demand['China'],\n",
    "                                             self.concentrate_demand['RoW']],\n",
    "                                            axis=1, keys=['Global', 'China', 'RoW'])\n",
    "\n",
    "    def run_mining_only(self):\n",
    "        for year in self.simulation_time:\n",
    "            if self.verbosity > 0: print(year)\n",
    "            self.i = year\n",
    "            if self.i == self.simulation_time[0]:\n",
    "                self.initialize_integration()\n",
    "            else:\n",
    "                self.primary_commodity_price.loc[year] = \\\n",
    "                self.historical_data['Primary commodity price'].copy().dropna().loc[year]\n",
    "                self.run_mining()\n",
    "\n",
    "    def decode_scenario_name(self):\n",
    "        '''\n",
    "        scenario_name takes the form 00_11_22_33_44\n",
    "        where:\n",
    "        00: ss, sd, bo (scrap supply, scrap demand, both).\n",
    "         Can also be sd-alt, which uses an alternative\n",
    "         implementation of the scrap demand increase (see below)\n",
    "        11: pr or no (price response included or no)\n",
    "        22: Xyr, where X is any integer and represents\n",
    "         the number of years the increase occurs\n",
    "        33: X%tot, where X is any float/int and is the\n",
    "         increase/decrease in scrap supply or demand\n",
    "         relative to the initial year total demand\n",
    "        44: X%inc, where X is any float/int and is the\n",
    "         increase/decrease in the %tot value per year\n",
    "\n",
    "        e.g. ss_pr_1yr_1%tot_0%inc\n",
    "\n",
    "        for 22-44, an additional X should be placed at\n",
    "         the end when 00==both, describing the sd values\n",
    "         e.g. ss_pr_1yr1_1%tot1_0%inc0\n",
    "\n",
    "        Can also have 11 as nono, prno, nopr, or prpr to\n",
    "         control the ss and sd price response individually\n",
    "         (in that order)\n",
    "\n",
    "        for sd-alt, the default (no alt) formulation is that an\n",
    "        increase in scrap demand to 5% of demand over 2\n",
    "        years would be\n",
    "        [0, 2.5%, 2.5%, 0%, 0%, ..., 0%]\n",
    "        while for alt it would be\n",
    "        [0, 2.5%, 5%,   5%, 5%, ..., 5%]\n",
    "        Can ctrl+F `direct_melt_duration` or `secondary_refined_duration`\n",
    "        to see the actual methods\n",
    "        '''\n",
    "        scenario_name = self.scenario_name\n",
    "        if type(scenario_name) != str:\n",
    "            scenario_name_str = scenario_name.index.names[0]\n",
    "        else:\n",
    "            scenario_name_str = scenario_name\n",
    "        error_string = 'improper format for scenario_name. Takes the form of 00_11_22_33_44 where:' + \\\n",
    "                       '\\n\\t00: ss, sd, bo (scrap supply, scrap demand, both)' + \\\n",
    "                       '\\n\\t11: pr or no (price response included or no)' + \\\n",
    "                       '\\n\\t22: Xyr, where X is any integer and represents the number of years the increase occurs' + \\\n",
    "                       '\\n\\t33: X%tot, where X is any float/int and is the increase/decrease in scrap supply or demand relative\\n\\t\\tto the initial year total demand' + \\\n",
    "                       '\\n\\t44: X%inc, where X is any float/int and is the increase/decrease in the %tot value per year' + \\\n",
    "                       '\\n\\n\\tfor 22-44, an additional X should be placed at the end when 00==bo, describing the sd values' + \\\n",
    "                       '\\n\\n\\tscenario_name: ' + scenario_name_str + \\\n",
    "                       '\\n\\tproper format: 00_11_Xyr_X%tot_X%inc or 00_11_XyrX_X%totX_X%incX if 00=bo'\n",
    "        collection_rate_price_response = self.hyperparam['Value']['collection_rate_price_response']\n",
    "        direct_melt_price_response = self.hyperparam['Value']['direct_melt_price_response']\n",
    "        collection_rate_duration = 0\n",
    "        collection_rate_pct_change_tot = 0\n",
    "        collection_rate_pct_change_inc = 0\n",
    "        direct_melt_duration = 0\n",
    "        direct_melt_pct_change_tot = 0\n",
    "        direct_melt_pct_change_inc = 0\n",
    "        secondary_refined_duration = 0\n",
    "        secondary_refined_pct_change_tot = 0\n",
    "        secondary_refined_pct_change_inc = 0\n",
    "        self.secondary_refined_alt = False\n",
    "        self.direct_melt_alt = False\n",
    "\n",
    "        if type(scenario_name) != str:\n",
    "            self.scenario_update_df = scenario_name.copy()\n",
    "            scenario_update_scrap_handling(self)\n",
    "        elif scenario_name == '':\n",
    "            scenario_type = scenario_name\n",
    "        elif '++' in scenario_name:\n",
    "            self.scenario_frame = get_scenario_dataframe(\n",
    "                file_path_for_scenario_setup=scenario_name.split('++')[0],\n",
    "                default_year=2019)\n",
    "            scenario_name_alt = scenario_name.split('++')[1]\n",
    "            self.scenario_update_df = self.scenario_frame.loc[scenario_name_alt]\n",
    "            scenario_update_scrap_handling(self)\n",
    "\n",
    "        else:\n",
    "            if '_' in scenario_name:\n",
    "                name = scenario_name.split('_')\n",
    "            else:\n",
    "                raise ValueError(error_string)\n",
    "            if np.any([j not in scenario_name for j in ['yr', '%tot', '%inc']]) \\\n",
    "                    or len(name) != 5 \\\n",
    "                    or not np.any([j in name[0] for j in ['ss', 'sd', 'bo']]) \\\n",
    "                    or not np.any([j in name[1] for j in ['pr', 'no']]) \\\n",
    "                    or 'yr' not in name[2] or '%tot' not in name[3] or '%inc' not in name[4]:\n",
    "                raise ValueError(error_string)\n",
    "            scenario_type = name[0].replace('ss', 'scrap supply').replace('sd', 'scrap demand').replace('bo', 'both')\n",
    "\n",
    "            if scenario_type in ['scrap supply', 'both', 'both-alt']:\n",
    "                collection_rate_duration = int(name[2].split('yr')[0])\n",
    "                collection_rate_pct_change_tot = float(name[3].split('%tot')[0])\n",
    "                collection_rate_pct_change_inc = float(name[4].split('%inc')[0])\n",
    "                if name[1] == 'pr':\n",
    "                    collection_rate_price_response = True\n",
    "                elif name[1] == 'no':\n",
    "                    collection_rate_price_response = False\n",
    "\n",
    "            if scenario_type in ['scrap demand', 'both', 'scrap demand-alt', 'both-alt']:\n",
    "                if scenario_type == 'both' or scenario_type == 'both-alt':\n",
    "                    integ = 1\n",
    "                    if name[2].split('yr')[1] == '' or name[3].split('%tot')[1] == '' or name[4].split('%inc')[1] == '':\n",
    "                        if self.verbosity>-1:\n",
    "                            print(\n",
    "                            'WARNING: scenario name does not fit BOTH format, using SCRAP SUPPLY value for SCRAP DEMAND')\n",
    "                        integ = 0\n",
    "                else:\n",
    "                    integ = 0\n",
    "                direct_melt_duration = int(name[2].split('yr')[integ])\n",
    "                direct_melt_pct_change_tot = float(name[3].split('%tot')[integ])\n",
    "                direct_melt_pct_change_inc = float(name[4].split('%inc')[integ])\n",
    "                if name[1] == 'pr':\n",
    "                    direct_melt_price_response = True\n",
    "                elif name[1] == 'no':\n",
    "                    direct_melt_price_response = False\n",
    "                self.direct_melt_alt = '-alt' in scenario_type\n",
    "                self.secondary_refined_alt = self.direct_melt_alt\n",
    "\n",
    "            if len(name[1]) > 2:\n",
    "                if name[1] == 'nono':\n",
    "                    collection_rate_price_response = False\n",
    "                    direct_melt_price_response = False\n",
    "                elif name[1] == 'prno':\n",
    "                    collection_rate_price_response = True\n",
    "                    direct_melt_price_response = False\n",
    "                elif name[1] == 'nopr':\n",
    "                    collection_rate_price_response = False\n",
    "                    direct_melt_price_response = True\n",
    "                elif name[1] == 'prpr':\n",
    "                    collection_rate_price_response = True\n",
    "                    direct_melt_price_response = True\n",
    "                else:\n",
    "                    print(\n",
    "                        f'WARNING, scenario name does not fit price response format, using default value from hyperparam input which is:\\n\\tcollection_rate_price_response={collection_rate_price_response}\\n\\tdirect_melt_price_response={direct_melt_price_response}')\n",
    "            secondary_refined_duration = direct_melt_duration\n",
    "            secondary_refined_pct_change_tot = 1 + direct_melt_pct_change_tot * self.hyperparam['Value'][\n",
    "                'Secondary refinery fraction of recycled content, Global'] / 100\n",
    "            secondary_refined_pct_change_inc = 1 + direct_melt_pct_change_inc / 100\n",
    "            direct_melt_duration = direct_melt_duration\n",
    "            direct_melt_pct_change_tot = 1 + direct_melt_pct_change_tot * (\n",
    "                    1 - self.hyperparam['Value']['Secondary refinery fraction of recycled content, Global']) / 100\n",
    "            direct_melt_pct_change_inc = 1 + direct_melt_pct_change_inc / 100\n",
    "\n",
    "        if type(scenario_name)==str and '++' not in scenario_name:\n",
    "            self.scenario_type = scenario_type\n",
    "            self.collection_rate_price_response = collection_rate_price_response\n",
    "            self.direct_melt_price_response = direct_melt_price_response\n",
    "            self.secondary_refined_price_response = direct_melt_price_response\n",
    "            self.collection_rate_duration = collection_rate_duration\n",
    "            self.collection_rate_pct_change_tot = 1 + collection_rate_pct_change_tot / 100\n",
    "            self.collection_rate_pct_change_inc = 1 + collection_rate_pct_change_inc / 100\n",
    "            self.direct_melt_duration = direct_melt_duration\n",
    "            self.direct_melt_pct_change_tot = direct_melt_pct_change_tot\n",
    "            self.direct_melt_pct_change_inc = direct_melt_pct_change_inc\n",
    "            self.secondary_refined_duration = secondary_refined_duration\n",
    "            self.secondary_refined_pct_change_tot = secondary_refined_pct_change_tot\n",
    "            self.secondary_refined_pct_change_inc = secondary_refined_pct_change_inc\n",
    "\n",
    "        self.hyperparam.loc['scenario_type', 'Value'] = self.scenario_type\n",
    "        self.hyperparam.loc['scenario_type', 'Notes'] = 'empty string, scrap supply, scrap demand, or both'\n",
    "        self.hyperparam.loc['collection_rate_price_response', 'Value'] = self.collection_rate_price_response\n",
    "        self.hyperparam.loc[\n",
    "            'collection_rate_price_response', 'Notes'] = 'whether or not there should be a price response for collection rate'\n",
    "        self.hyperparam.loc['direct_melt_price_response', 'Value'] = self.direct_melt_price_response\n",
    "        self.hyperparam.loc[\n",
    "            'direct_melt_price_response', 'Notes'] = 'whether there should be a price response for direct melt fraction'\n",
    "        self.hyperparam.loc['secondary_refined_price_response', 'Value'] = self.direct_melt_price_response\n",
    "        self.hyperparam.loc[\n",
    "            'secondary_refined_price_response', 'Notes'] = 'whether there should be a price response for direct melt fraction'\n",
    "        self.hyperparam.loc['collection_rate_duration', 'Value'] = self.collection_rate_duration\n",
    "        self.hyperparam.loc[\n",
    "            'collection_rate_duration', 'Notes'] = 'length of the increase in collection rate described by collection_rate_pct_change_tot'\n",
    "        self.hyperparam.loc['collection_rate_pct_change_tot', 'Value'] = self.collection_rate_pct_change_tot\n",
    "        self.hyperparam.loc[\n",
    "            'collection_rate_pct_change_tot', 'Notes'] = 'without price response, describes the percent increase in collection rate attained at the end of the linear ramp with duration collection_rate_duration. Given as 1+%change/100'\n",
    "        self.hyperparam.loc['collection_rate_pct_change_inc', 'Value'] = self.collection_rate_pct_change_inc\n",
    "        self.hyperparam.loc[\n",
    "            'collection_rate_pct_change_inc', 'Notes'] = 'once the collection_rate_pct_change_tot is reached, the collection rate will then increase by this value per year. Given as 1+%change/100'\n",
    "        self.hyperparam.loc['direct_melt_duration', 'Value'] = self.direct_melt_duration\n",
    "        self.hyperparam.loc[\n",
    "            'direct_melt_duration', 'Notes'] = 'length of the increase in direct melt fraction described by direct_melt_pct_change_tot'\n",
    "        self.hyperparam.loc['direct_melt_alt', 'Value'] = self.direct_melt_alt\n",
    "        self.hyperparam.loc['direct_melt_pct_change_tot', 'Value'] = self.direct_melt_pct_change_tot\n",
    "        self.hyperparam.loc[\n",
    "            'direct_melt_pct_change_tot', 'Notes'] = 'without price response, describes the percent increase in collection rate attained at the end of the linear ramp with duration direct_melt_duration. Given as 1+%change/100'\n",
    "        self.hyperparam.loc['direct_melt_pct_change_inc', 'Value'] = self.direct_melt_pct_change_inc\n",
    "        self.hyperparam.loc[\n",
    "            'direct_melt_pct_change_inc', 'Notes'] = 'once the direct_melt_pct_change_tot is reached, the direct melt fraction will then increase by this value per year. Given as 1+%change/100'\n",
    "        self.hyperparam.loc['secondary_refined_duration', 'Value'] = self.secondary_refined_duration\n",
    "        self.hyperparam.loc[\n",
    "            'secondary_refined_duration', 'Notes'] = 'length of the increase in direct melt fraction described by direct_melt_pct_change_tot'\n",
    "        self.hyperparam.loc['secondary_refined_alt', 'Value'] = self.secondary_refined_alt\n",
    "        self.hyperparam.loc['secondary_refined_pct_change_tot', 'Value'] = self.secondary_refined_pct_change_tot\n",
    "        self.hyperparam.loc[\n",
    "            'secondary_refined_pct_change_tot', 'Notes'] = 'without price response, describes the percent increase in collection rate attained at the end of the linear ramp with duration direct_melt_duration. Given as 1+%change/100'\n",
    "        self.hyperparam.loc['secondary_refined_pct_change_inc', 'Value'] = self.secondary_refined_pct_change_inc\n",
    "        self.hyperparam.loc[\n",
    "            'secondary_refined_pct_change_inc', 'Notes'] = 'once the direct_melt_pct_change_tot is reached, the direct melt fraction will then increase by this value per year. Given as 1+%change/100'\n",
    "\n",
    "    def update_hyperparam_scenario_input(self, i=None):\n",
    "        \"\"\"\n",
    "        Run within any year to update hyperparameters from the scenario input excel file\n",
    "        \"\"\"\n",
    "        h = self.hyperparam['Value']\n",
    "        if not hasattr(self,'scenario_update_df'):\n",
    "            return None\n",
    "        if i is None: i = self.i\n",
    "        intersect = np.intersect1d(i, self.scenario_update_df.index.get_level_values(1).unique())\n",
    "        if len(intersect) > 0:\n",
    "            update_this_year = self.scenario_update_df.loc[idx[:, i]]\n",
    "            if update_this_year.index.duplicated().any():\n",
    "                ind = update_this_year.loc[update_this_year.index.duplicated()].index\n",
    "                update_this_year = update_this_year.loc[~update_this_year.index.duplicated()]\n",
    "                warnings.warn(\n",
    "                    f'\\nMultiple entries for the same year: {list(ind)}, {i}  |  Only the first row will be implemented.')\n",
    "            for v in update_this_year.index.get_level_values(0).unique():\n",
    "                value = update_this_year.loc[v] if update_this_year.index.nlevels == 1 \\\n",
    "                    else update_this_year.loc[v].loc[i]\n",
    "                iterator = [self.hyperparam]+[getattr(self, at).hyperparam for at in ['mining', 'refine', 'demand']\n",
    "                                              if hasattr(self, at)]\n",
    "                for it, q in enumerate(iterator):\n",
    "                    if type(q) == dict:\n",
    "                        if v in q:\n",
    "                            q[v] = value\n",
    "                    else:\n",
    "                        if v in q.index or it==0:\n",
    "                            q.loc[v, 'Value'] = value\n",
    "\n",
    "                if self.verbosity>-10:\n",
    "                    print(i, v, value) # TODO remove this after confirmed working\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10119cf7",
   "metadata": {},
   "source": [
    "# ISIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead00583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ed244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b25393cf",
   "metadata": {},
   "source": [
    "## Round 1, scrap supply only shocks, no collection response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_q = Many()\n",
    "# many_q.load_data('2023-05-30 14_08_31_4_displacement')\n",
    "many_q.load_data('2023-06-11 21_36_39_6_displacement')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef31420",
   "metadata": {},
   "outputs": [],
   "source": [
    "many_q.hyperparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca500ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = many_q.results[[i for i in many_q.results.columns if 'collection' in i.lower()]].loc['Cu']\n",
    "# x.loc[2021]/x.loc[2018]\n",
    "x.loc[0]['Collection rate transport']-x.loc[1]['Collection rate transport']\n",
    "# x.loc[0]['Old scrap collection']-x.loc[1]['Old scrap collection']\n",
    "# x.loc[0]\n",
    "x.loc[0].pct_change()-x.loc[1].pct_change()\n",
    "# x.loc[1].pct_change().loc[:,:'Collection rate transport'].plot()\n",
    "x.loc[549]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573ade23",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99245f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "offset = 0.4\n",
    "ref_yr = [2019]\n",
    "groups_rates = many_q.hyperparam.loc[idx[:,:,\n",
    "                          ['hyperparam_set_group', 'hyperparam_set_number','collection_rate_pct_change_inc']],\n",
    "                      'Value'].unstack()\n",
    "new_index = groups_rates\n",
    "#     .set_index(['hyperparam_set_group','hyperparam_set_number','collection_rate_pct_change_inc'],append=True)\n",
    "res = many_q.results.copy().unstack()\n",
    "def new_index_fn(i):\n",
    "    vals = new_index.loc[i]\n",
    "    return (i[0], i[1], vals.iloc[0], vals.iloc[1], vals.iloc[2])\n",
    "res.index = pd.MultiIndex.from_tuples([new_index_fn(i) for i in res.index])\n",
    "res = res.stack()\n",
    "closeish_init = res.loc[idx[:,:,:,:,'0',2001:2019],'Mine production'].unstack(0).unstack().T\n",
    "hist = many_q.historical_data.loc[idx[:,:],'Primary supply']\n",
    "\n",
    "# Gets all where scenarios value at ref_yr is within offset*100% of historical value for that year\n",
    "closeish = closeish_init.apply(lambda x: (x>hist.loc[x.index]*(1-offset)) & (x<hist.loc[x.index]*(1+offset)), axis=0)\n",
    "closeish_st = closeish.unstack().stack(0).stack(0).stack(0).stack(0)[ref_yr]\n",
    "if len(closeish_st.shape)>1:\n",
    "    close_ind = closeish_st.loc[closeish_st.all(axis=1)].index\n",
    "else:\n",
    "    close_ind = closeish_st[closeish_st].index\n",
    "\n",
    "# Getting changes, cumulative changes from baseline, calculating displacement\n",
    "close_res = res.copy().droplevel(1)\n",
    "close_res_c = close_res.stack().unstack(-3)\n",
    "close_pct_change = close_res_c.apply(lambda x: (x-close_res_c['0'])/close_res_c['0'], axis=0).unstack().stack(0)*100\n",
    "close_change = close_res_c.apply(lambda x: x-close_res_c['0'], axis=0).unstack().stack(0)\n",
    "close_cumu_change = close_change.groupby(level=[0,1,2,4]).cumsum()\n",
    "close_res_c2 = close_res.stack().unstack(-2)\n",
    "close_change_2019 = close_res_c2.div(close_res_c2[2019],\n",
    "                                     axis=0).unstack(-2).stack(0).stack().unstack(-3) #ratio w/ 2019\n",
    "displacement = -close_cumu_change['Mine production']/close_cumu_change['Scrap supply']\n",
    "displacement = displacement.copy()\n",
    "\n",
    "disp_price = pd.concat([\n",
    "    close_change['Refined price'],\n",
    "    close_pct_change['Refined price'],\n",
    "    close_change_2019['Refined price'],\n",
    "    close_change_2019['Mine production'],\n",
    "    displacement.copy(),\n",
    "],axis=1,keys=['Price change','Price change %', 'Price ratio', 'Mine production ratio', 'Displacement'])\\\n",
    "    .drop('1.0',level=4)\n",
    "\n",
    "# removing times where displacement is zero or NaN\n",
    "disp_price = pd.concat([disp_price.loc[idx[i[0], i[2], i[3], :, :],:] for i in close_ind])\n",
    "disp_price = disp_price.loc[(disp_price['Displacement']!=0)&(np.isfinite(disp_price['Displacement']))]\n",
    "disp_price['log(Mine production ratio)'] = np.log10(disp_price['Mine production ratio'])\n",
    "disp_price['log(Price ratio)'] = np.log10(disp_price['Price ratio'])\n",
    "\n",
    "# getting displacement back in results format for concatenation\n",
    "alt_index = new_index.reset_index().set_index(\n",
    "    ['level_0','hyperparam_set_group','hyperparam_set_number',\n",
    "     'collection_rate_pct_change_inc'],drop=False).sort_index()\n",
    "renamed_displacement = displacement.copy()\n",
    "def alt_index_fn(i):\n",
    "    vals = alt_index.loc[(i[0],i[1],i[2],i[4])]\n",
    "    return (vals.iloc[0], vals.iloc[1], i[3])\n",
    "renamed_displacement.index = pd.MultiIndex.from_tuples([alt_index_fn(i) for i in renamed_displacement.index])\n",
    "renamed_displacement = renamed_displacement.sort_index()\n",
    "renamed_displacement.name = 'Displacement'\n",
    "many_q.results['Displacement'] = renamed_displacement\n",
    "\n",
    "displacement.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cd6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = many_q.hyperparam.copy()\n",
    "def hyp_index_fn(i):\n",
    "    vals = new_index.loc[i[:2]]\n",
    "    return (i[0], i[1], vals.iloc[0], vals.iloc[1], vals.iloc[2], i[2])\n",
    "hyp.index = pd.MultiIndex.from_tuples([hyp_index_fn(i) for i in hyp.index])\n",
    "hyp = hyp['Value'].unstack()\n",
    "many_q.hyp = hyp.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe17d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.unstack().loc[close_ind].stack()['Mine production'].unstack().T.plot(legend=False)\n",
    "plt.hlines(hist.loc['Cu'].loc[2019],2001,2019, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded4f513",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# res['Mine production'].unstack().T.plot(legend=False)\n",
    "\n",
    "sns.lineplot(res['Mine production'].reset_index(), y='Mine production', x='level_5')\n",
    "sns.lineplot(res.unstack().loc[close_ind].stack()['Mine production'].reset_index(),\n",
    "             y='Mine production', x='level_5')\n",
    "plt.hlines(hist.loc['Cu'].loc[2019],2001,2019, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51540d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "closeish_init[close_ind.droplevel(0)].droplevel(0).plot(legend=False)\n",
    "plt.hlines(hist.loc['Cu'].loc[2019],2001,2019, 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e16aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disp_price_r = disp_price.sort_index().loc[idx[:,:,:, 2040:, :],:].reset_index().rename(columns={\n",
    "    'level_0':'Commodity', 'level_1':'hyperparam group', 'level_2':'hyperparam scenario', \n",
    "    'level_3':'Year', 'level_4':'Scrap scenario'\n",
    "}).astype({'hyperparam group':float, 'hyperparam scenario':float})\\\n",
    "    .astype({'hyperparam group':int, 'hyperparam scenario':int})\n",
    "disp_price_r['hyperparam ind'] = \\\n",
    "    disp_price_r['hyperparam group']*disp_price_r['hyperparam scenario'].max()+disp_price_r['hyperparam scenario']\n",
    "\n",
    "# culling the weird ones\n",
    "disp_price_r = disp_price_r.loc[\n",
    "    (disp_price_r['Displacement']<2)&\n",
    "    (disp_price_r['Displacement']>-2)&\n",
    "    (disp_price_r['log(Price ratio)']>0)\n",
    "]\n",
    "disp_price_d = pd.get_dummies(disp_price_r, columns=['Year', 'hyperparam ind'])\n",
    "\n",
    "m = sm.GLS(disp_price_r['Price ratio'], \n",
    "           (disp_price_d[[i for i in disp_price_d.columns if i=='Displacement'\n",
    "                                         or 'hyperparam ind' in i or 'Year' in i]])\n",
    "          ).fit(cov_type='HC3')\n",
    "# m = sm.GLS(disp_price_r['log(Price ratio)'], \n",
    "#            sm.add_constant(disp_price_d[[i for i in disp_price_d.columns if i=='log(Mine production ratio)'\n",
    "#                                          or 'hyperparam ind' in i or 'Year' in i]])\n",
    "#           ).fit(cov_type='HC3')\n",
    "mod = m\n",
    "m.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61afe8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cmap='Dark2'\n",
    "# ph = disp_price_r.loc[[i in [5,16, 19] for i in disp_price_r['hyperparam ind']]].replace({\n",
    "#     5:0, 16:1, 38:3, 19:2\n",
    "# })\n",
    "# [114, 5, 121,  34, 77, 102, 140]\n",
    "ph = disp_price_r.loc[[i in [114, 140, 100, 82] for i in disp_price_r['hyperparam ind']]].replace({\n",
    "#     5:0, 3:1, 10:3, 91:2\n",
    "})\n",
    "ks = np.arange(0,2*len(ph['hyperparam ind'].unique())+2)\n",
    "\n",
    "fig,ax=plt.subplots(figsize=[6,5])\n",
    "ax.set(xlim=[-1.15,2.15], ylim=(0.7908330074583488, 2.6767509198164466), \n",
    "           xlabel=None, ylabel=None)\n",
    "fig.set_dpi(150)\n",
    "\n",
    "for k in ks:\n",
    "    fig, ax = plt.subplots(figsize=[6,5])\n",
    "    colors = mpl.color_sequences['Dark2'][:len(ph['hyperparam ind'].unique())]\n",
    "    colors = mpl.color_sequences['Dark2'][:int(np.ceil((k+1)))]\n",
    "    # Add regression lines to each of these\n",
    "    # colors = [mpl.colormaps[cmap](i) for i in np.linspace(0,1,len(ph['hyperparam ind'].unique()))]\n",
    "    for e,i,c in zip(np.arange(0,100),ph['hyperparam ind'].unique(), colors):\n",
    "        ph2 = ph.loc[(ph['hyperparam ind']==i)&(ph['Displacement']<2)&(ph['Displacement']>-1)]\n",
    "        ph2.plot.scatter(y='Price ratio', x='Displacement', color=c, s=200, ax=ax).grid(axis='x')\n",
    "        m = sm.GLS(ph2['Price ratio'], sm.add_constant(ph2['Displacement'])).fit(cov_type='HC3')\n",
    "        xmin = ph2['Displacement'].min()\n",
    "        xmax = ph2['Displacement'].max()\n",
    "        x = np.linspace(xmin,xmax)\n",
    "        y = m.params['const']+m.params['Displacement']*x\n",
    "        if k-e>len(ph['hyperparam ind'].unique()):\n",
    "            ax.plot(x,y, color=c, label=i)\n",
    "    if k==ks[-1]:\n",
    "        x = np.array([-1.5,3])\n",
    "        y = mod.params['Year_2040'] + x*mod.params['Displacement']\n",
    "        ax.plot(x,y,'k')\n",
    "    ax.set(xlim=[-1.15,2.15], ylim=(0.7908330074583488, 2.6767509198164466), \n",
    "           xlabel=None, ylabel=None)\n",
    "    if k==60:\n",
    "        fig.set_dpi(200)\n",
    "    else:\n",
    "        fig.set_dpi(150)\n",
    "#     fig.patch.set_alpha(0)\n",
    "#     ax.legend()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "p19 = many_q.historical_data['Primary commodity price'].loc['Cu'].loc[2019]\n",
    "x = np.linspace(0,1,51)\n",
    "p_ratio = mod.params['Year_2040']+mod.params['Displacement']*x\n",
    "price = p_ratio*p19\n",
    "price = pd.Series(price, x)\n",
    "(price[1]-price[0])/p19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72bce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_part = 8\n",
    "full_list = disp_price_r['hyperparam ind'].unique()\n",
    "for j in range(int(np.ceil(len(full_list)/n_per_part))):\n",
    "    sub_list = np.arange(j*n_per_part, (j+1)*n_per_part)\n",
    "    if len(full_list[sub_list[0]:])<len(sub_list):\n",
    "        sub_list = sub_list[:len(full_list[sub_list[0]:])]\n",
    "    \n",
    "    fig, ax = easy_subplots(sub_list)\n",
    "    for k,a in zip(sub_list,ax):\n",
    "        ph = disp_price_r.loc[[i in [k] for i in disp_price_r['hyperparam ind']]]\n",
    "        ph.plot.scatter(y='Price ratio', x='Displacement', ax=a, s=200)\n",
    "    #     a.set(title=k, xlim=[-1.15,2.15], ylim=(0.7908330074583488, 2.6767509198164466))\n",
    "    fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdd3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = displacement.unstack().loc[idx[:,:,:,2040],:].index\n",
    "fig,ax=easy_subplots(ind, use_subplots=True, sharey=True)\n",
    "for i,a in zip(ind, ax):\n",
    "    close_res['Mine production'].loc[idx[:,i[1],i[2],:]].unstack().T.plot(ax=a,legend=False)\n",
    "    a.set_title(i)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038c8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = displacement.unstack().loc[idx[:,:,:,2040],:].index\n",
    "len(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1c5700",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa8a2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "addl_params = many.rmse_df.copy()\n",
    "addl_params = addl_params.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n",
    "addl_params = addl_params.loc[(addl_params.apply(lambda x: x-x.mean(), axis=1)>1e-7).any(axis=1)]\n",
    "# addl_params = addl_params.loc[[i for i in addl_params.index if i[1]!='primary_commodity_price' and \n",
    "#                               'duration' not in i[1] and 'pct_change' not in i[1] and 'hyperparam' not in i[1]]]\n",
    "# addl_params = pd.concat([addl_params, \n",
    "#                          addl_params.idx[:,'hyperparam_set_group'],:]*addl_params.idx[:,'hyperparam_set_number'],:].max(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c361169",
   "metadata": {
    "code_folding": [
     3,
     191
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from skopt import Optimizer\n",
    "\n",
    "include_hyper_id = False\n",
    "\n",
    "def get_case_study_data(many):\n",
    "    case_study = pd.read_excel('generalization/input_files/user_defined/case study data.xlsx', index_col=0)\n",
    "    precious = case_study.copy()[['Au','Ag']]\n",
    "    for i,j in zip(['industrial','transport','construction'],\n",
    "                   ['bar and coin','jewelry','industrial']):\n",
    "        cols = [k for k in precious.index if i in k]\n",
    "        rename_dict = dict(zip(cols, [k.replace(i,j) for k in cols]))\n",
    "        precious = precious.rename(rename_dict)\n",
    "    case_study = case_study.loc[:,~case_study.columns.isin(precious.columns)]\n",
    "    case_study = pd.concat([case_study, precious],axis=1)\n",
    "    case_study = case_study[many.results.index.get_level_values(0).unique()]\n",
    "    case_vars = ['historical_growth_rate','china_fraction_demand','Recycling input rate, Global',\n",
    "                 'Secondary refinery fraction of recycled content, Global','primary_production_mean',\n",
    "                 'primary_ore_grade_mean','primary_commodity_price'\n",
    "                ] + [i for i in case_study.index if 'sector_dist' in i]\n",
    "    case_study = case_study.loc[case_vars].fillna(0)\n",
    "    return case_study\n",
    "\n",
    "def tune_ml_model(many, Regressor=None, objective='Refined price', plot=False):\n",
    "    \"\"\"\n",
    "    Takes in Many object, and creates attributes:\n",
    "    - best_model: best-performing ML model\n",
    "    - fi_data: StandardScaler correction of feature importance data\n",
    "    - fi_data_orig: uncorrected feature importance data\n",
    "    - X_train_df: dataframe corresponding to training data\n",
    "    - X_validate_df: dataframe corresponding to validation data\n",
    "    - X_test_df: dataframe corresponding to testing data\n",
    "    \n",
    "    Returns best_model\n",
    "    \"\"\"\n",
    "    # getting data from fruity_plots function\n",
    "    comms = many.results.index.get_level_values(0).unique()\n",
    "    many.objective = objective\n",
    "\n",
    "    # getting additional parameter data\n",
    "    addl_params = many.rmse_df.copy()\n",
    "    addl_params = addl_params.apply(lambda x: pd.to_numeric(x, errors='coerce')).dropna()\n",
    "    addl_params = addl_params.loc[(addl_params.apply(lambda x: x-x.mean(), axis=1)>1e-7).any(axis=1)]\n",
    "    if include_hyper_id:\n",
    "        hyper_id = addl_params.loc[idx[:,'hyperparam_set_group'],:].rename({'hyperparam_set_group':'hyper_id'},level=1)*\\\n",
    "            addl_params.loc[idx[:,'hyperparam_set_number'],:].max(axis=1).max()+\\\n",
    "            addl_params.loc[idx[:,'hyperparam_set_number'],:].rename({'hyperparam_set_number':'hyper_id'},level=1)\n",
    "        addl_params = pd.concat([addl_params, hyper_id]).sort_index()\n",
    "    addl_params = addl_params.loc[[i for i in addl_params.index if i[1]!='primary_commodity_price' and \n",
    "                                  'duration' not in i[1] and 'pct_change' not in i[1] and 'hyperparam' not in i[1]]]\n",
    "    addl_params = addl_params.stack().unstack(1)\n",
    "    if include_hyper_id:\n",
    "        addl_params = pd.get_dummies(addl_params, columns=['hyper_id'])\n",
    "    \n",
    "    # getting y data\n",
    "    y_df = many.results[objective].copy()\n",
    "    y_df = y_df.loc[idx[:,:,2040]]\n",
    "    ph = many.objective_results_map\n",
    "    results_objective_map = dict(zip(ph.values(),ph.keys()))\n",
    "    if objective!='Displacement':\n",
    "        y_df = y_df.unstack()\n",
    "        string = results_objective_map[objective]\n",
    "        y_df = y_df.divide(\n",
    "            many.historical_data[string].loc[idx[:,2018]],\n",
    "            axis=0).stack()\n",
    "    if type(y_df)==pd.core.series.Series:\n",
    "        y_df = pd.DataFrame(y_df)\n",
    "    y_df = y_df.rename(columns={y_df.columns[0]:objective})\n",
    "    y_df = y_df.sort_index()\n",
    "    if objective!='Displacement':\n",
    "        y_df = np.log10(y_df)\n",
    "    else:\n",
    "        y_df = y_df[(y_df>-1)&(y_df<2)]\n",
    "\n",
    "    # getting case study data\n",
    "    case_study = get_case_study_data(many)\n",
    "    many.case_study_data = case_study.copy()\n",
    "    case_study = case_study.T.stack()\n",
    "    case_study = pd.concat([case_study for _ in many.rmse_df.columns], axis=1)\n",
    "    case_study.columns = many.rmse_df.columns\n",
    "    case_study = case_study.stack().unstack(1)\n",
    "    \n",
    "    # concatenating\n",
    "    ind = np.intersect1d(addl_params.index, y_df.index)\n",
    "    ind = np.intersect1d(ind, case_study.index)\n",
    "#     fi_data = pd.concat([y_df, addl_params.loc[ind], case_study.loc[ind]],axis=1).fillna(0)\n",
    "    fi_data = pd.concat([y_df, addl_params.loc[ind]],axis=1).fillna(0)\n",
    "    fi_data = fi_data.dropna()\n",
    "    \n",
    "    # log columns:\n",
    "    cols = ['primary_commodity_price','primary_production_mean','primary_ore_grade_mean']\n",
    "    for q in cols:\n",
    "        if q in fi_data.columns:\n",
    "            fi_data.loc[:,q] = np.log10(fi_data[q])\n",
    "    \n",
    "    # removing outliers\n",
    "#     fi_data = fi_data.loc[abs(fi_data['Mean change normed'])<80]\n",
    "#     fi_data = fi_data.loc[(y_df<y_df.quantile(0.95)).iloc[:,0]]\n",
    "    many.fi_data_orig = fi_data.copy()\n",
    "\n",
    "    # applying standard scaler\n",
    "    scaler = StandardScaler()\n",
    "    fi_std = scaler.fit_transform(fi_data.values)\n",
    "    fi_data = pd.DataFrame(fi_std,fi_data.index,fi_data.columns)\n",
    "    if 'region_specific_price_response' in fi_data.columns:\n",
    "        fi_data.drop(columns='region_specific_price_response',inplace=True)\n",
    "    many.fi_data = fi_data.copy()\n",
    "\n",
    "    # reformatting for split\n",
    "    X_df = fi_data[[i for i in fi_data.columns if i not in \n",
    "                    ['Mean change normed','Mean change','Scrap collected','RIR',objective]]]\n",
    "    y_df = fi_data[objective]\n",
    "    X, y = X_df.values, y_df.values\n",
    "\n",
    "    # train/test/validate split\n",
    "    many.X_train, many.X_test, many.y_train, many.y_test, many.ind_train, many.ind_test = train_test_split(\n",
    "        X, y, y_df.index, test_size=0.333,random_state=42)\n",
    "    many.X_validate, many.X_test, many.y_validate, many.y_test, many.ind_validate, many.ind_test = train_test_split(\n",
    "        many.X_test, many.y_test, many.ind_test,\n",
    "        test_size=0.5,\n",
    "        random_state=43)\n",
    "    many.X_train_df = pd.DataFrame(many.X_train, many.ind_train, X_df.columns)\n",
    "    many.X_test_df = pd.DataFrame(many.X_test, many.ind_test, X_df.columns)\n",
    "    many.X_validate_df = pd.DataFrame(many.X_validate, many.ind_validate, X_df.columns)\n",
    "    many.y_train_df = pd.Series(many.y_train, many.ind_train)\n",
    "    many.y_test_df = pd.Series(many.y_test, many.ind_test)\n",
    "    many.y_validate_df = pd.Series(many.y_validate, many.ind_validate)\n",
    "\n",
    "#     return many\n",
    "\n",
    "    # Tuning regression hyperparameters:\n",
    "    Regressor_dict = dict(zip(\n",
    "            ['RandomForestRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor'],\n",
    "            [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor]))\n",
    "    if Regressor is None:\n",
    "        Regressor = [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor][0]\n",
    "    elif type(Regressor)==int:\n",
    "        Regressor = [RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor][Regressor]\n",
    "    elif type(Regressor)==str and Regressor != 'all':\n",
    "        Regressor = Regressor_dict[Regressor]\n",
    "        \n",
    "    if type(Regressor)!=str:\n",
    "        get_best_mod(many, Regressor)\n",
    "        if plot:\n",
    "            fig,ax=plt.subplots()\n",
    "            ax.scatter(many.y_train_df.values, many.best_mod.predict(many.X_train_df.values), label='Train')\n",
    "            ax.scatter(many.y_validate_df.values, many.best_mod.predict(many.X_validate_df.values), label='Validate')\n",
    "            ax.scatter(many.y_test_df.values, many.best_mod.predict(many.X_test_df.values), label='Test')\n",
    "            ax.legend()\n",
    "            ax.set(xlabel='Actual', ylabel='Predicted')\n",
    "        return many.best_mod\n",
    "    elif Regressor == 'all':\n",
    "        many.best_mods = {}\n",
    "        for regr in Regressor_dict:\n",
    "            Regressor = Regressor_dict[regr]\n",
    "            best_mod = get_best_mod(many, Regressor)\n",
    "            many.best_mods[regr] = best_mod\n",
    "        if plot:\n",
    "            fig,ax=easy_subplots(list(many.best_mods.keys()))\n",
    "            mins = np.inf\n",
    "            maxs = -np.inf\n",
    "            for regr,a in zip(many.best_mods,ax):\n",
    "                a.scatter(many.y_train_df.values, \n",
    "                           many.best_mods[regr].predict(many.X_train_df.values), label='Train')\n",
    "                a.scatter(many.y_validate_df.values, \n",
    "                           many.best_mods[regr].predict(many.X_validate_df.values), label='Validate')\n",
    "                a.scatter(many.y_test_df.values, \n",
    "                           many.best_mods[regr].predict(many.X_test_df.values), label='Test')\n",
    "                mins1 = min([min(many.y_train_df.values), min(many.y_validate_df.values), \n",
    "                             min(many.y_test_df.values)])\n",
    "                maxs1 = max([max(many.y_train_df.values), max(many.y_validate_df.values), \n",
    "                             max(many.y_test_df.values)])\n",
    "                mins2 = min([min(many.best_mods[regr].predict(many.X_train_df.values)),\n",
    "                         min(many.best_mods[regr].predict(many.X_validate_df.values)),\n",
    "                         min(many.best_mods[regr].predict(many.X_test_df.values))])\n",
    "                maxs2 = max([max(many.best_mods[regr].predict(many.X_train_df.values)),\n",
    "                         max(many.best_mods[regr].predict(many.X_validate_df.values)),\n",
    "                         max(many.best_mods[regr].predict(many.X_test_df.values))])\n",
    "                mins3 = min([mins1, mins2])\n",
    "                maxs3 = max([maxs1, maxs2])\n",
    "                if mins3<mins: mins = mins3\n",
    "                if maxs3>maxs: maxs = maxs3\n",
    "                a.legend()\n",
    "                r2_test = r2_score(many.y_test, many.best_mods[regr].predict(many.X_test))\n",
    "                r2_test = round(r2_test,3)\n",
    "                a.set(xlabel='Actual', ylabel='Predicted', title=f'{regr}\\nTest R2: {r2_test}')\n",
    "            mins -= (maxs-mins)/15\n",
    "            maxs += (maxs-mins)/15\n",
    "            for a in ax:\n",
    "                a.set(xlim=(mins,maxs),ylim=(mins,maxs))\n",
    "        return many.best_mods\n",
    "\n",
    "def get_best_mod(many, Regressor):\n",
    "    variables = {'n_estimators':(5,800), 'min_samples_leaf': (0.00000001,0.9999)}\n",
    "    opt = Optimizer([variables[i] for i in variables.keys()])\n",
    "\n",
    "    rmse_min = 1e4\n",
    "    best_params = None\n",
    "    best_mod = None\n",
    "    for i in range(20):\n",
    "        if i==0:\n",
    "            suggested = [800,1e-5]\n",
    "        else:\n",
    "            suggested = opt.ask()\n",
    "        regr = Regressor(random_state=0, n_estimators=suggested[0], min_samples_leaf=suggested[1])\n",
    "        regr.fit(many.X_train, many.y_train)\n",
    "        mse = mean_squared_error(regr.predict(many.X_validate), many.y_validate)**0.5\n",
    "        train_mse = mean_squared_error(regr.predict(many.X_train), many.y_train)**0.5\n",
    "        opt.tell(suggested, mse)\n",
    "        if mse<rmse_min:\n",
    "            rmse_min = mse\n",
    "            best_params = suggested\n",
    "            best_mod = regr\n",
    "        print('iteration:', i, suggested,'Train MSE:',train_mse, 'Validation MSE:',mse)\n",
    "    print('Best params:', best_params)\n",
    "    print('Train R2:', r2_score(many.y_train, best_mod.predict(many.X_train)))\n",
    "    print('Validation R2:', r2_score(many.y_validate, best_mod.predict(many.X_validate)))\n",
    "    print('Test R2:', r2_score(many.y_test, best_mod.predict(many.X_test)))\n",
    "    many.best_mod = best_mod\n",
    "    return best_mod\n",
    "\n",
    "def from_fruity_feature_importance(many):\n",
    "    case_study_only = False\n",
    "    n_params = 15\n",
    "\n",
    "    importances = pd.Series(many.best_mod.feature_importances_, many.X_test_df.columns)\n",
    "    many.importances = importances.copy()\n",
    "    impor_heat = importances.copy()#.drop('Scrap demand')\n",
    "    actual_sum = impor_heat.sum()\n",
    "    ind = np.intersect1d(impor_heat.index, many.case_study_data.index)\n",
    "    if case_study_only:\n",
    "        impor_heat = impor_heat.loc[ind]\n",
    "#     impor_heat = impor_heat.div(actual_sum,axis=1)\n",
    "    impor_heat /= actual_sum\n",
    "    impor_heat = impor_heat.sort_values(ascending=False).head(n_params)\n",
    "    impor_heat = impor_heat.rename(index=make_parameter_names_nice(impor_heat.index))\n",
    "#     impor_heat = impor_heat.stack().reset_index()\n",
    "    impor_heat = impor_heat.reset_index()\n",
    "    impor_heat = impor_heat.rename(columns={'index':'Parameter',0:'Feature importance'})\n",
    "    # sns.heatmap(impor_heat, ax=ax,\n",
    "    #             yticklabels=True,\n",
    "    #            )\n",
    "    orient = 'v'\n",
    "    figsize = [8,5+impor_heat['Parameter'].unique().shape[0]*11/40]\n",
    "    if orient=='h':\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        kwargs = {'y':'Parameter', 'x':'Feature importance', 'orient':orient}\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=figsize[::-1])\n",
    "        kwargs = {'x':'Parameter', 'y':'Feature importance', 'orient':orient}\n",
    "    sns.barplot(impor_heat, ax=ax, **kwargs)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90);\n",
    "    ax.legend(title='Scrap scenario', alignment='left')\n",
    "\n",
    "tune_ml_model(many_q, objective = 'Refined price', Regressor='all', plot=True)\n",
    "# tune_ml_model(many_q, objective = 'Displacement', Regressor='all', plot=True)\n",
    "from_fruity_feature_importance(many_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1464ca01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df96852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "many = many_q\n",
    "# fig,ax=plt.subplots()\n",
    "# ax.scatter(many.y_train_df.values, many.best_mod.predict(many.X_train_df.values), label='Train')\n",
    "# ax.scatter(many.y_validate_df.values, many.best_mod.predict(many.X_validate_df.values), label='Validate')\n",
    "# ax.scatter(many.y_test_df.values, many.best_mod.predict(many.X_test_df.values), label='Test')\n",
    "# ax.legend()\n",
    "# ax.set(xlabel='Actual', ylabel='Predicted')\n",
    "\n",
    "fig,ax=easy_subplots(list(many.best_mods.keys()),use_subplots=True,sharey=True)\n",
    "mins = np.inf\n",
    "maxs = -np.inf\n",
    "for regr,a in zip(many.best_mods,ax):\n",
    "    a.scatter(many.y_train_df.values, \n",
    "               many.best_mods[regr].predict(many.X_train_df.values), label='Train')\n",
    "    a.scatter(many.y_validate_df.values, \n",
    "               many.best_mods[regr].predict(many.X_validate_df.values), label='Validate')\n",
    "    a.scatter(many.y_test_df.values, \n",
    "               many.best_mods[regr].predict(many.X_test_df.values), label='Test')\n",
    "    mins1 = min([min(many.y_train_df.values), min(many.y_validate_df.values), \n",
    "                 min(many.y_test_df.values)])\n",
    "    maxs1 = max([max(many.y_train_df.values), max(many.y_validate_df.values), \n",
    "                 max(many.y_test_df.values)])\n",
    "    mins2 = min([min(many.best_mods[regr].predict(many.X_train_df.values)),\n",
    "             min(many.best_mods[regr].predict(many.X_validate_df.values)),\n",
    "             min(many.best_mods[regr].predict(many.X_test_df.values))])\n",
    "    maxs2 = max([max(many.best_mods[regr].predict(many.X_train_df.values)),\n",
    "             max(many.best_mods[regr].predict(many.X_validate_df.values)),\n",
    "             max(many.best_mods[regr].predict(many.X_test_df.values))])\n",
    "    mins3 = min([mins1, mins2])\n",
    "    maxs3 = max([maxs1, maxs2])\n",
    "    if mins3<mins: mins = mins3\n",
    "    if maxs3>maxs: maxs = maxs3\n",
    "    a.legend()\n",
    "    r2_test = r2_score(many.y_test_df.values, \n",
    "               many.best_mods[regr].predict(many.X_test_df.values))\n",
    "    r2_test = round(r2_test,3)\n",
    "    a.set(xlabel='Actual', ylabel='Predicted', title=f'{regr}\\nTest R2: {r2_test}')\n",
    "mins -= (maxs-mins)/15\n",
    "maxs += (maxs-mins)/15\n",
    "\n",
    "for a in ax:\n",
    "    a.set(xlim=(mins,maxs),ylim=(mins,maxs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = ExtraTreesRegressor(random_state=0, n_estimators=800, min_samples_leaf=1e-8)\n",
    "regr.fit(many_b.X_train, many_b.y_train)\n",
    "print('Train R2:', r2_score(many_b.y_train, regr.predict(many_b.X_train)))\n",
    "print('Validation R2:', r2_score(many_b.y_validate, regr.predict(many_b.X_validate)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb155b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fcff3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "import fasttreeshap as shap # works exactly the same for the TreeExplainer function, should be fasterjust_use_et = True\n",
    "just_use_et=False\n",
    "for many in [many_q]:\n",
    "    if just_use_et:\n",
    "        regr = regr = ExtraTreesRegressor(random_state=0, n_estimators=800, min_samples_leaf=1e-8)\n",
    "        regr.fit(many.X_train, many.y_train)\n",
    "    else:\n",
    "        regr = many.best_mods['ExtraTreesRegressor']\n",
    "    explainer = shap.TreeExplainer(regr)\n",
    "    shap_df = many.X_test_df.copy()\n",
    "    to_add = many.fi_data.loc[idx[:,:25],:].index\n",
    "    to_add = to_add[~to_add.isin(many.X_test_df.index)]\n",
    "    shap_df = pd.concat([shap_df,many.fi_data.loc[to_add]])\n",
    "    shap_values_object = explainer(shap_df.values)\n",
    "    shap_values = shap_values_object.values\n",
    "    shap_values_df = pd.DataFrame(shap_values, shap_df.index, shap_df.columns)\n",
    "    many.shap_values_df = shap_values_df.copy()\n",
    "    setattr(many, 'shap_values_df_'+many.objective.replace(' ','_').lower(), shap_values_df.copy())\n",
    "#     explainer_copy = shap.TreeExplainer(regr)\n",
    "#     shap_df = many.X_test_df.copy()\n",
    "#     to_add = many.fi_data.loc[idx[:,:25],:].index\n",
    "#     to_add = to_add[~to_add.isin(many.X_test_df.index)]\n",
    "#     shap_df = pd.concat([shap_df,many.fi_data.loc[to_add]])\n",
    "#     shap_interaction_values = explainer_copy.shap_interaction_values(shap_df.values)\n",
    "#     x = shap_interaction_values\n",
    "#     if many.commodity!=None:\n",
    "#         many.shap_interaction_df = pd.DataFrame(x.reshape(x.shape[0]*x.shape[1],x.shape[2]), shap_df.index, shap_df.columns)\n",
    "#     else:\n",
    "#         many.shap_interaction_df = pd.DataFrame(x.reshape(x.shape[0]*x.shape[1],x.shape[2]), shap_df.index, shap_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef32894b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee81d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2981e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "use_displacement=False\n",
    "\n",
    "many = many_q\n",
    "if use_displacement:\n",
    "    many.shap_values_df = many.shap_values_df_displacement.copy()\n",
    "    many.objective = 'Displacement'\n",
    "else:\n",
    "    many.shap_values_df = many.shap_values_df_refined_price.copy()\n",
    "    many.objective = 'Refined price'\n",
    "    \n",
    "many.shap_values_df\n",
    "X_test_df_orig = many.fi_data_orig.loc[many.shap_values_df.index]\n",
    "X_test_df = many.fi_data.loc[many.shap_values_df.index]\n",
    "nice_dict = make_parameter_names_nice(many.fi_data.columns)\n",
    "slopes = pd.Series(np.nan, many.shap_values_df.columns)\n",
    "n_col = int(len(many.shap_values_df.columns)/2)\n",
    "cols_sub = [many.shap_values_df.columns[:n_col], many.shap_values_df.columns[n_col:]]\n",
    "\n",
    "color = mpl.color_sequences['Dark2'][0 if many.objective=='Refined price' else 1]\n",
    "for cols in cols_sub:\n",
    "    fig,ax = easy_subplots(cols)\n",
    "    for col,a in zip(cols,ax):\n",
    "        if col in X_test_df_orig.columns:\n",
    "            a.scatter(x=X_test_df_orig[col], y=many.shap_values_df[col], color=color)\n",
    "            title = nice_dict[col]\n",
    "            if len(title)>30:\n",
    "                title = title.split(' ')\n",
    "                title = '\\n'.join([\n",
    "                    ' '.join(title[:int(len(title)/2)]),\n",
    "                    ' '.join(title[int(len(title)/2):])\n",
    "                ])\n",
    "            a.set(xlabel='Parameter values', ylabel='SHAP values', title=title)\n",
    "            if col in ['intensity_response_to_gdp','sector_specific_price_response']:\n",
    "                a.set(ylim=(-.2,.2))\n",
    "            m = sm.GLS(many.shap_values_df[col], sm.add_constant(X_test_df[col])).fit(cov_type='HC3')\n",
    "            slopes.loc[col] = m.params[col]\n",
    "    fig.tight_layout()\n",
    "    fig.set_dpi(200)\n",
    "    plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268998a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5722dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter    \n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "many = many_q\n",
    "params = ['mine_cu_margin_elas','collection_elas_scrap_price','mine_cost_og_elas', 'tcrc_elas_sd']\n",
    "param = params[-1]\n",
    "\n",
    "mine_cost_shap_m = many.shap_values_df_displacement.loc[:,param]\n",
    "mine_cost_shap_p = many.shap_values_df_refined_price.loc[:,param]\n",
    "# mine_cost_shap.groupby(level=0).mean()\n",
    "# mine_cost = many.case_study_data.loc['Secondary refinery fraction of recycled content, Global']\n",
    "# plt.scatter(mine_cost, mine_cost_shap.groupby(level=0).median())\n",
    "mine_cost = many.fi_data_orig.loc[mine_cost_shap_m.index,param]\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[6.75,5.7])\n",
    "# ax.scatter(mine_cost, mine_cost_shap_p, color=plt.color_sequences['Dark2'][1])\n",
    "\n",
    "width, poly, uni_size, middle = 100, 3, 100, 0.8\n",
    "\n",
    "mine_shap = pd.concat([mine_cost, mine_cost_shap_m],axis=1,keys=['data','shap']).set_index('data')\n",
    "mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).mean().plot(ax=ax, color='#c33086')\\\n",
    "    .grid(axis='x')\n",
    "# ax.scatter(mine_cost, mine_cost_shap_m)\n",
    "lb = mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile((1-middle)/2)\n",
    "ub = mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile(1-(1-middle)/2)\n",
    "ax.fill_between(lb.index, lb, ub, color='#c33086', alpha=0.4)\n",
    "# mine_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).mean().plot(ax=ax)\n",
    "# mine_filt = mine_shap[['shap']].sort_index().apply(savgol_filter, window_length=width, polyorder=poly)\n",
    "# mine_filt.plot(ax=ax,color='k')\n",
    "# mine_filt = mine_shap[['shap']].sort_index().apply(uniform_filter1d, size=uni_size)\n",
    "# mine_filt.plot(ax=ax,color='k')\n",
    "ax.set(ylim=(-0.14996965362916803, 0.0966476306711664),\n",
    "      xlabel=None)\n",
    "fig.set_dpi(50)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[6.75,5.7])\n",
    "price_shap = pd.concat([mine_cost, mine_cost_shap_p],axis=1,keys=['data','shap']).set_index('data')\n",
    "# ax.scatter(mine_cost, mine_cost_shap_p)\n",
    "price_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).mean().plot(ax=ax, color='#484671')\\\n",
    "    .grid(axis='x')\n",
    "lb = price_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile((1-middle)/2)\n",
    "ub = price_shap['shap'].sort_index().rolling(width,center=True, min_periods=1).quantile(1-(1-middle)/2)\n",
    "ax.fill_between(lb.index, lb, ub, color='#484671', alpha=0.4)\n",
    "# price_filt = price_shap[['shap']].sort_index().apply(savgol_filter, window_length=width, polyorder=poly)\n",
    "# price_filt.plot(ax=ax,color='k')\n",
    "# price_filt = price_shap[['shap']].sort_index().apply(uniform_filter1d, size=uni_size)\n",
    "# price_filt.plot(ax=ax,color='k')\n",
    "# ax.plot(x.index, v.const+v.x*x.x+v.x2*x.x2, color='k')\n",
    "# mine_cost_g = mine_cost.groupby(level=0).median()\n",
    "# mine_cost_shap_g = mine_cost_shap.groupby(level=0).median()\n",
    "# ax.set(xlim=(-11.690742879493992, 11.800456526405199), ylim=(-0.36736509335182116, 0.18857329350258228),\n",
    "#       xlabel=None)\n",
    "# if want same as above:\n",
    "ax.set(ylim=(-0.14996965362916803, 0.0966476306711664),\n",
    "      xlabel=None)\n",
    "fig.set_dpi(50)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e876be35",
   "metadata": {},
   "source": [
    "# Checking copper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f718751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "data = many_sg.results[['Scrap supply','Scrap demand','Spread']].loc['Cu'].reset_index().rename(\n",
    "    columns={'level_0':'Scenario','level_1':'Year'})\n",
    "data = data[data<1e8]\n",
    "data['Spread'] = data['Spread'][data['Spread']<1e4]\n",
    "# data = data.dropna()\n",
    "data['S-D'] = data['Scrap supply']-data['Scrap demand']\n",
    "sns.lineplot(data, x='Year', y='Scrap supply')\n",
    "sns.lineplot(data, x='Year', y='Scrap demand')\n",
    "plt.figure()\n",
    "sns.lineplot(data, x='Year', y='S-D')\n",
    "plt.figure()\n",
    "sns.lineplot(data, x='Year', y='Spread')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468120d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd14749c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063b2af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9da52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a078009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc30710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55429c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "generalization",
   "language": "python",
   "name": "generalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "615.883px",
    "left": "26px",
    "top": "565.783px",
    "width": "286.988px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "30",
    "lenType": "10",
    "lenVar": "41"
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "543.85px",
    "left": "1405px",
    "right": "20px",
    "top": "100px",
    "width": "435px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
